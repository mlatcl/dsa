---
title: "Gaussian Processes"
venue: "Virtual Data Science Nigeria"
abstract: "<p>Classical machine learning and statistical approaches to
learning, such as neural networks and linear regression, assume a
parametric form for functions. Gaussian process models are an
alternative approach that assumes a probabilistic prior over functions.
This brings benefits, in that uncertainty of function estimation is
sustained throughout inference, and some challenges: algorithms for
fitting Gaussian processes tend to be more complex than parametric
models. In this sessions I will introduce Gaussian processes and explain
why sustaining uncertainty is important.</p>"
edit_url: https://github.com/mlatcl/dsa/edit/gh-pages/_lamd/gaussian-processes.md
date: 2020-11-13
published: 2020-11-13
time: "15:00 (West Africa Standard Time)"
session: 4
reveal: 04-gaussian-processes.slides.html
transition: None
ipynb: 04-gaussian-processes.ipynb
pptx: 04-gaussian-processes.pptx
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="setup">Setup</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_notebooks/includes/notebook-setup.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_notebooks/includes/notebook-setup.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
<h2 id="notutils">notutils</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This small package is a helper package for various notebook utilities
used below.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install notutils</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/notutils"
class="uri">https://github.com/lawrennd/notutils</a></p>
<p>Once <code>notutils</code> is installed, it can be imported in the
usual manner.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> notutils</span></code></pre></div>
<h2 id="pods">pods</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In Sheffield we created a suite of software tools for ‘Open Data
Science’. Open data science is an approach to sharing code, models and
data that should make it easier for companies, health professionals and
scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/ods"
class="uri">https://github.com/lawrennd/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<h2 id="mlai">mlai</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The <code>mlai</code> software is a suite of helper functions for
teaching and demonstrating machine learning algorithms. It was first
used in the Machine Learning and Adaptive Intelligence course in
Sheffield in 2013.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install mlai</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/mlai"
class="uri">https://github.com/lawrennd/mlai</a></p>
<p>Once <code>mlai</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="figure">
<div id="gaussian-processes-for-machine-learning-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/rasmussen-williams-book.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gaussian-processes-for-machine-learning-magnify"
class="magnify"
onclick="magnifyFigure(&#39;gaussian-processes-for-machine-learning&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-processes-for-machine-learning-caption"
class="caption-frame">
<p>Figure: A key reference for Gaussian process models remains the
excellent book “Gaussian Processes for Machine Learning” (<span
class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams
(2006)</span>). The book is also
<a href="http://www.gaussianprocess.org/gpml/" target="_blank">freely
available online</a>.</p>
</div>
</div>
<p><span class="citation" data-cites="Rasmussen:book06">Rasmussen and
Williams (2006)</span> is still one of the most important references on
Gaussian process models. It is <a
href="http://www.gaussianprocess.org/gpml/">available freely
online</a>.</p>
<h2 id="a-first-course-in-machine-learning">A First Course in Machine
Learning</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/first-course-book.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/first-course-book.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="a-first-course-in-machine-learning-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//mlai/a-first-course-in-machine-learning.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="a-first-course-in-machine-learning-magnify" class="magnify"
onclick="magnifyFigure(&#39;a-first-course-in-machine-learning&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="a-first-course-in-machine-learning-caption"
class="caption-frame">
<p>Figure: The main course text is “A First Course in Machine Learning”
by <span class="citation" data-cites="Rogers:book11">Rogers and Girolami
(2011)</span>.</p>
</div>
</div>
<!--include{_gp/includes/what-is-a-gp.md}-->
<h2 id="example-prediction-of-malaria-incidence-in-uganda">Example:
Prediction of Malaria Incidence in Uganda</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip0)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacecho
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip1)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip2)"/>
</svg>
</div>
<p>As an example of using Gaussian process models within the full
pipeline from data to decsion, we’ll consider the prediction of Malaria
incidence in Uganda. For the purposes of this study malaria reports come
in two forms, HMIS reports from health centres and Sentinel data, which
is curated by the WHO. There are limited sentinel sites and many HMIS
sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in
collaboration with John Quinn and Martin Mubangizi <span
class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>. John and Martin were
initally from the AI-DEV group from the University of Makerere in
Kampala and more latterly they were based at UN Global Pulse in Kampala.
You can see the work summarized on the UN Global Pulse <a
href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">disease
outbreaks project site here</a>.</p>
<ul>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
<p>Malaria data is spatial data. Uganda is split into districts, and
health reports can be found for each district. This suggests that models
such as conditional random fields could be used for spatial modelling,
but there are two complexities with this. First of all, occasionally
districts split into two. Secondly, sentinel sites are a specific
location within a district, such as Nagongera which is a sentinel site
based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify"
onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
<p>The common standard for collecting health data on the African
continent is from the Health management information systems (HMIS).
However, this data suffers from missing values <span class="citation"
data-cites="Gething:hmis06">(Gething et al., 2006)</span> and diagnosis
of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is
located.</p>
</div>
</div>
<p><a
href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World
Health Organization Sentinel Surveillance systems</a> are set up “when
high-quality data are needed about a particular disease that cannot be
obtained through a passive system”. Several sentinel sites give accurate
assessment of malaria disease levels in Uganda, including a site in
Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify"
onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature
for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to
investigate whether Gaussian process models could be used to assimilate
information from these two different sources of disease informaton.
Further, we were interested in whether local information on rainfall and
temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside
rainfall and temperature, to improve predictions from HMIS data of
levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify"
onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school
held at Makerere in Kampala in 2013. The school led, in turn, to the
Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole
district over time. Estimate is constructed with a Gaussian process with
an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have
fitted the reports with a Gaussian process with an additive covariance
function. It has two components, one is a long time scale component (in
red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the
blue line (the short term report signal) above the red (which represents
the long term trend? If so we have higher than expected reports. If this
is the case <em>and</em> the gradient is still positive (i.e. reports
are going up) we encode this with a <em>red</em> color. If it is the
case and the gradient of the blue line is negative (i.e. reports are
going down) we encode this with an <em>amber</em> color. Conversely, if
the blue line is below the red <em>and</em> decreasing, we color
<em>green</em>. On the other hand if it is below red but increasing, we
color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad
situation getting worse, amber is bad, but improving. Green is good and
getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of
the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria
situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the
districts to give an immediate impression of the current status of the
disease across the country.</p>
<h1 id="what-is-machine-learning">What is Machine Learning?</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>What is machine learning? At its most basic level machine learning is
a combination of</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>where <em>data</em> is our observations. They can be actively or
passively acquired (meta-data). The <em>model</em> contains our
assumptions, based on previous experience. That experience can be other
data, it can come from transfer learning, or it can merely be our
beliefs about the regularities of the universe. In humans our models
include our inductive biases. The <em>prediction</em> is an action to be
taken or a categorization or a quality score. The reason that machine
learning has become a mainstay of artificial intelligence is the
importance of predictions in artificial intelligence. The data and the
model are combined through computation.</p>
<p>In practice we normally perform machine learning using two functions.
To combine data with a model we typically make use of:</p>
<p><strong>a prediction function</strong> it is used to make the
predictions. It includes our beliefs about the regularities of the
universe, our assumptions about how the world works, e.g., smoothness,
spatial similarities, temporal similarities.</p>
<p><strong>an objective function</strong> it defines the ‘cost’ of
misprediction. Typically, it includes knowledge about the world’s
generating processes (probabilistic objectives) or the costs we pay for
mispredictions (empirical risk minimization).</p>
<p>The combination of data and model through the prediction function and
the objective function leads to a <em>learning algorithm</em>. The class
of prediction functions and objective functions we can make use of is
restricted by the algorithms they lead to. If the prediction function or
the objective function are too complex, then it can be difficult to find
an appropriate learning algorithm. Much of the academic field of machine
learning is the quest for new learning algorithms that allow us to bring
different types of models and data together.</p>
<p>A useful reference for state of the art in machine learning is the UK
Royal Society Report, <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a>.</p>
<p>You can also check my post blog post on <a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a>.</p>
<h2 id="overdetermined-system">Overdetermined System</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/overdetermined-system.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/overdetermined-system.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The challenge with a linear model is that it has two unknowns, <span
class="math inline">\(m\)</span>, and <span
class="math inline">\(c\)</span>. Observing data allows us to write down
a system of simultaneous linear equations. So, for example if we observe
two data points, the first with the input value, <span
class="math inline">\(x_1 = 1\)</span> and the output value, <span
class="math inline">\(y_1 =3\)</span> and a second data point, <span
class="math inline">\(x= 3\)</span>, <span
class="math inline">\(y=1\)</span>, then we can write two simultaneous
linear equations of the form.</p>
<p>point 1: <span class="math inline">\(x= 1\)</span>, <span
class="math inline">\(y=3\)</span> <span class="math display">\[
3 = m + c
\]</span> point 2: <span class="math inline">\(x= 3\)</span>, <span
class="math inline">\(y=1\)</span> <span class="math display">\[
1 = 3m + c
\]</span></p>
<p>The solution to these two simultaneous equations can be represented
graphically as</p>
<div class="figure">
<div id="over-determined-system-3-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/over_determined_system003.svg" width="40%" style=" ">
</object>
</div>
<div id="over-determined-system-3-magnify" class="magnify"
onclick="magnifyFigure(&#39;over-determined-system-3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="over-determined-system-3-caption" class="caption-frame">
<p>Figure: The solution of two linear equations represented as the fit
of a straight line through two data</p>
</div>
</div>
<p>The challenge comes when a third data point is observed, and it
doesn’t fit on the straight line.</p>
<p>point 3: <span class="math inline">\(x= 2\)</span>, <span
class="math inline">\(y=2.5\)</span> <span class="math display">\[
2.5 = 2m + c
\]</span></p>
<div class="figure">
<div id="over-determined-system-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/over_determined_system004.svg" width="40%" style=" ">
</object>
</div>
<div id="over-determined-system-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;over-determined-system-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="over-determined-system-4-caption" class="caption-frame">
<p>Figure: A third observation of data is inconsistent with the solution
dictated by the first two observations</p>
</div>
</div>
<p>Now there are three candidate lines, each consistent with our
data.</p>
<div class="figure">
<div id="over-determined-system-7-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/over_determined_system007.svg" width="40%" style=" ">
</object>
</div>
<div id="over-determined-system-7-magnify" class="magnify"
onclick="magnifyFigure(&#39;over-determined-system-7&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="over-determined-system-7-caption" class="caption-frame">
<p>Figure: Three solutions to the problem, each consistent with two
points of the three observations</p>
</div>
</div>
<p>This is known as an <em>overdetermined</em> system because there are
more data than we need to determine our parameters. The problem arises
because the model is a simplification of the real world, and the data we
observe is therefore inconsistent with our model.</p>
<h2 id="pierre-simon-laplace">Pierre-Simon Laplace</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/overdetermined-laplace-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/overdetermined-laplace-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The solution was proposed by Pierre-Simon Laplace. His idea was to
accept that the model was an incomplete representation of the real
world, and the way it was incomplete is <em>unknown</em>. His idea was
that such unknowns could be dealt with through probability.</p>
<h3 id="pierre-simon-laplace-1">Pierre-Simon Laplace</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplace-portrait.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplace-portrait.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="pierre-simon-laplace-image-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/Pierre-Simon_Laplace.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="pierre-simon-laplace-image-magnify" class="magnify"
onclick="magnifyFigure(&#39;pierre-simon-laplace-image&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pierre-simon-laplace-image-caption" class="caption-frame">
<p>Figure: Pierre-Simon Laplace 1749-1827.</p>
</div>
</div>
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2&amp;output=embed" width="700" height="500">
</iframe>
<p>Famously, Laplace considered the idea of a deterministic Universe,
one in which the model is <em>known</em>, or as the below translation
refers to it, “an intelligence which could comprehend all the forces by
which nature is animated”. He speculates on an “intelligence” that can
submit this vast data to analysis and propsoses that such an entity
would be able to predict the future.</p>
<blockquote>
<p>Given for one instant an intelligence which could comprehend all the
forces by which nature is animated and the respective situation of the
beings who compose it—an intelligence sufficiently vast to submit these
data to analysis—it would embrace in the same formulate the movements of
the greatest bodies of the universe and those of the lightest atom; for
it, nothing would be uncertain and the future, as the past, would be
present in its eyes.</p>
</blockquote>
<p>This notion is known as <em>Laplace’s demon</em> or <em>Laplace’s
superman</em>.</p>
<div class="figure">
<div id="laplaces-determinism-english-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//physics/laplacesDeterminismEnglish.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="laplaces-determinism-english-magnify" class="magnify"
onclick="magnifyFigure(&#39;laplaces-determinism-english&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="laplaces-determinism-english-caption" class="caption-frame">
<p>Figure: Laplace’s determinsim in English translation.</p>
</div>
</div>
<h2 id="laplaces-gremlin">Laplace’s Gremlin</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplaces-determinism.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplaces-determinism.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Unfortunately, most analyses of his ideas stop at that point, whereas
his real point is that such a notion is unreachable. Not so much
<em>superman</em> as <em>strawman</em>. Just three pages later in the
“Philosophical Essay on Probabilities” <span class="citation"
data-cites="Laplace:essai14">(Laplace, 1814)</span>, Laplace goes on to
observe:</p>
<blockquote>
<p>The curve described by a simple molecule of air or vapor is regulated
in a manner just as certain as the planetary orbits; the only difference
between them is that which comes from our ignorance.</p>
<p>Probability is relative, in part to this ignorance, in part to our
knowledge.</p>
</blockquote>
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=1YQPAAAAQAAJ&amp;pg=PR17-IA4&amp;output=embed" width="700" height="500">
</iframe>
<div class="figure">
<div id="probability-relative-in-part-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//physics/philosophicaless00lapliala.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="probability-relative-in-part-magnify" class="magnify"
onclick="magnifyFigure(&#39;probability-relative-in-part&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="probability-relative-in-part-caption" class="caption-frame">
<p>Figure: To Laplace, determinism is a strawman. Ignorance of mechanism
and data leads to uncertainty which should be dealt with through
probability.</p>
</div>
</div>
<p>In other words, we can never make use of the idealistic deterministic
Universe due to our ignorance about the world, Laplace’s suggestion, and
focus in this essay is that we turn to probability to deal with this
uncertainty. This is also our inspiration for using probability in
machine learning. This is the true message of Laplace’s essay, not
determinism, but the gremlin of uncertainty that emerges from our
ignorance.</p>
<p>The “forces by which nature is animated” is our <em>model</em>, the
“situation of beings that compose it” is our <em>data</em> and the
“intelligence sufficiently vast enough to submit these data to analysis”
is our compute. The fly in the ointment is our <em>ignorance</em> about
these aspects. And <em>probability</em> is the tool we use to
incorporate this ignorance leading to uncertainty or <em>doubt</em> in
our predictions.</p>
<h2 id="latent-variables">Latent Variables</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/laplace-latent-variable-solution.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/laplace-latent-variable-solution.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Laplace’s concept was that the reason that the data doesn’t match up
to the model is because of unconsidered factors, and that these might be
well represented through probability densities. He tackles the challenge
of the unknown factors by adding a variable, <span
class="math inline">\(\epsilon\)</span>, that represents the unknown. In
modern parlance we would call this a <em>latent</em> variable. But in
the context Laplace uses it, the variable is so common that it has other
names such as a “slack” variable or the <em>noise</em> in the
system.</p>
<p>point 1: <span class="math inline">\(x= 1\)</span>, <span
class="math inline">\(y=3\)</span> [ 3 = m + c + _1 ] point 2: <span
class="math inline">\(x= 3\)</span>, <span
class="math inline">\(y=1\)</span> [ 1 = 3m + c + _2 ] point 3: <span
class="math inline">\(x= 2\)</span>, <span
class="math inline">\(y=2.5\)</span> [ 2.5 = 2m + c + _3 ]</p>
<p>Laplace’s trick has converted the <em>overdetermined</em> system into
an <em>underdetermined</em> system. He has now added three variables,
<span class="math inline">\(\{\epsilon_i\}_{i=1}^3\)</span>, which
represent the unknown corruptions of the real world. Laplace’s idea is
that we should represent that unknown corruption with a <em>probability
distribution</em>.</p>
<h2 id="a-probabilistic-process">A Probabilistic Process</h2>
<p>However, it was left to an admirer of Laplace to develop a practical
probability density for that purpose. It was Carl Friedrich Gauss who
suggested that the <em>Gaussian</em> density (which at the time was
unnamed!) should be used to represent this error.</p>
<p>The result is a <em>noisy</em> function, a function which has a
deterministic part, and a stochastic part. This type of function is
sometimes known as a probabilistic or stochastic process, to distinguish
it from a deterministic process.</p>
<h2 id="two-important-gaussian-properties">Two Important Gaussian
Properties</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/univariate-gaussian-properties.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/univariate-gaussian-properties.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The Gaussian density has many important properties, but for the
moment we’ll review two of them.</p>
<h2 id="sum-of-gaussians">Sum of Gaussians</h2>
<p>If we assume that a variable, <span
class="math inline">\(y_i\)</span>, is sampled from a Gaussian
density,</p>
<p><span class="math display">\[y_i \sim
\mathcal{N}\left(\mu_i,\sigma_i^2\right)\]</span></p>
<p>Then we can show that the sum of a set of variables, each drawn
independently from such a density is also distributed as Gaussian. The
mean of the resulting density is the sum of the means, and the variance
is the sum of the variances,</p>
<p><span class="math display">\[
\sum_{i=1}^{n} y_i \sim
\mathcal{N}\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2\right)
\]</span></p>
<p>Since we are very familiar with the Gaussian density and its
properties, it is not immediately apparent how unusual this is. Most
random variables, when you add them together, change the family of
density they are drawn from. For example, the Gaussian is exceptional in
this regard. Indeed, other random variables, if they are independently
drawn and summed together tend to a Gaussian density. That is the <a
href="https://en.wikipedia.org/wiki/Central_limit_theorem"><em>central
limit theorem</em></a> which is a major justification for the use of a
Gaussian density.</p>
<h2 id="scaling-a-gaussian">Scaling a Gaussian</h2>
<p>Less unusual is the <em>scaling</em> property of a Gaussian density.
If a variable, <span class="math inline">\(y\)</span>, is sampled from a
Gaussian density,</p>
<p><span class="math display">\[y\sim
\mathcal{N}\left(\mu,\sigma^2\right)\]</span> and we choose to scale
that variable by a <em>deterministic</em> value, <span
class="math inline">\(w\)</span>, then the <em>scaled variable</em> is
distributed as</p>
<p><span class="math display">\[wy\sim \mathcal{N}\left(w\mu,w^2
\sigma^2\right).\]</span> Unlike the summing properties, where adding
two or more random variables independently sampled from a family of
densitites typically brings the summed variable <em>outside</em> that
family, scaling many densities leaves the distribution of that variable
in the same <em>family</em> of densities. Indeed, many densities include
a <em>scale</em> parameter (e.g. the <a
href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma
density</a>) which is purely for this purpose. In the Gaussian the
standard deviation, <span class="math inline">\(\sigma\)</span>, is the
scale parameter. To see why this makes sense, let’s consider, <span
class="math display">\[z \sim \mathcal{N}\left(0,1\right),\]</span> then
if we scale by <span class="math inline">\(\sigma\)</span> so we have,
<span class="math inline">\(y=\sigma z\)</span>, we can write, <span
class="math display">\[y=\sigma z \sim
\mathcal{N}\left(0,\sigma^2\right)\]</span></p>
<p>Let’s first of all review the properties of the multivariate Gaussian
distribution that make linear Gaussian models easier to deal with. We’ll
return to the, perhaps surprising, result on the parameters within the
nonlinearity, <span class="math inline">\(\boldsymbol{ \theta}\)</span>,
shortly.</p>
<p>To work with linear Gaussian models, to find the marginal likelihood
all you need to know is the following rules. If <span
class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span> where <span class="math inline">\(\mathbf{ y}\)</span>, <span
class="math inline">\(\mathbf{ x}\)</span> and <span
class="math inline">\(\boldsymbol{ \epsilon}\)</span> are vectors and we
assume that <span class="math inline">\(\mathbf{ x}\)</span> and <span
class="math inline">\(\boldsymbol{ \epsilon}\)</span> are drawn from
multivariate Gaussians, <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span> then we know that <span class="math inline">\(\mathbf{
y}\)</span> is also drawn from a multivariate Gaussian with, <span
class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span></p>
<p>With appropriately defined covariance, <span
class="math inline">\(\boldsymbol{ \Sigma}\)</span>, this is actually
the marginal likelihood for Factor Analysis, or Probabilistic Principal
Component Analysis <span class="citation"
data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>,
because we integrated out the inputs (or <em>latent</em> variables they
would be called in that case).</p>
<h2 id="laplaces-idea">Laplace’s Idea</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-log-likelihood.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-log-likelihood.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Laplace had the idea to augment the observations by noise, that is
equivalent to considering a probability density whose mean is given by
the <em>prediction function</em> <span
class="math display">\[p\left(y_i|x_i\right)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{\left(y_i-f\left(x_i\right)\right)^{2}}{2\sigma^2}\right).\]</span></p>
<p>This is known as <em>stochastic process</em>. It is a function that
is corrupted by noise. Laplace didn’t suggest the Gaussian density for
that purpose, that was an innovation from Carl Friederich Gauss, which
is what gives the Gaussian density its name.</p>
<h2 id="height-as-a-function-of-weight">Height as a Function of
Weight</h2>
<p>In the standard Gaussian, parameterized by mean and variance, make
the mean a linear function of an <em>input</em>.</p>
<p>This leads to a regression model. <span class="math display">\[
\begin{align*}
  y_i=&amp;f\left(x_i\right)+\epsilon_i,\\
         \epsilon_i \sim &amp; \mathcal{N}\left(0,\sigma^2\right).
  \end{align*}
\]</span></p>
<p>Assume <span class="math inline">\(y_i\)</span> is height and <span
class="math inline">\(x_i\)</span> is weight.</p>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression
modelling. The data consists of the pace of Olympic Gold Medal Marathon
winners for the Olympics from 1896 to present. Let’s load in the data
and plot.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1896.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in that
year the Olympics was in St Louis, USA. Organizational problems and
challenges with dust kicked up by the cars following the race meant that
participants got lost, and only very few participants completed. More
recent years see more consistently quick marathons.</p>
<h2 id="running-example-olympic-marathons">Running Example: Olympic
Marathons</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-linear-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-linear-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Note that <code>x</code> and <code>y</code> are not
<code>pandas</code> data frames for this example, they are just arrays
of dimensionality <span class="math inline">\(n\times 1\)</span>, where
<span class="math inline">\(n\)</span> is the number of data.</p>
<p>The aim of this lab is to have you coding linear regression in
python. We will do it in two ways, once using iterative updates
(coordinate ascent) and then using linear algebra. The linear algebra
approach will not only work much better, it is also easy to extend to
multiple input linear regression and <em>non-linear</em> regression
using basis functions.</p>
<h2 id="maximum-likelihood-iterative-solution">Maximum Likelihood:
Iterative Solution</h2>
<p>Now we will take the maximum likelihood approach we derived in the
lecture to fit a line, <span class="math inline">\(y_i=mx_i +
c\)</span>, to the data you’ve plotted. We are trying to minimize the
error function: <span class="math display">\[
E(m, c) =  \sum_{i=1}^n(y_i-mx_i-c)^2
\]</span> with respect to <span class="math inline">\(m\)</span>, <span
class="math inline">\(c\)</span> and <span
class="math inline">\(\sigma^2\)</span>. We can start with an initial
guess for <span class="math inline">\(m\)</span>,</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="op">-</span><span class="fl">0.4</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">80</span></span></code></pre></div>
<p>Then we use the maximum likelihood update to find an estimate for the
offset, <span class="math inline">\(c\)</span>.</p>
<h2 id="log-likelihood-for-multivariate-regression">Log Likelihood for
Multivariate Regression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-multivariate-log-likelihood.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-multivariate-log-likelihood.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="quadratic-loss">Quadratic Loss</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-direct-solution.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-direct-solution.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now we’ve identified the empirical risk with the loss, we’ll use
<span class="math inline">\(E(\mathbf{ w})\)</span> to represent our
objective function. <span class="math display">\[
E(\mathbf{ w}) = \sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i, \mathbf{
w})\right)^2
\]</span> gives us our objective.</p>
<p>In the case of the linear prediction function, we can substitute
<span class="math inline">\(f(\mathbf{ x}_i, \mathbf{ w}) = \mathbf{
w}^\top \mathbf{ x}_i\)</span>. <span class="math display">\[
E(\mathbf{ w}) = \sum_{i=1}^n\left(y_i - \mathbf{ w}^\top \mathbf{
x}_i\right)^2
\]</span> To compute the gradient of the objective, we first expand the
brackets.</p>
<h2 id="bracket-expansion">Bracket Expansion</h2>
<p><span class="math display">\[
\begin{align*}
  E(\mathbf{ w},\sigma^2)  = &amp;
\frac{n}{2}\log \sigma^2 + \frac{1}{2\sigma^2}\sum
_{i=1}^{n}y_i^{2}-\frac{1}{\sigma^2}\sum
_{i=1}^{n}y_i\mathbf{ w}^{\top}\mathbf{
x}_i\\&amp;+\frac{1}{2\sigma^2}\sum
_{i=1}^{n}\mathbf{ w}^{\top}\mathbf{ x}_i\mathbf{ x}_i^{\top}\mathbf{ w}
+\text{const}.\\
    = &amp; \frac{n}{2}\log \sigma^2 + \frac{1}{2\sigma^2}\sum
_{i=1}^{n}y_i^{2}-\frac{1}{\sigma^2}
\mathbf{ w}^\top\sum_{i=1}^{n}\mathbf{
x}_iy_i\\&amp;+\frac{1}{2\sigma^2}
\mathbf{ w}^{\top}\left[\sum
_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]\mathbf{
w}+\text{const}.
\end{align*}
\]</span></p>
<h1 id="solution-with-linear-algebra">Solution with Linear Algebra</h1>
<p>In this section we’re going compute the minimum of the quadratic loss
with respect to the parameters. When we do this, we’ll also review
<em>linear algebra</em>. We will represent all our errors and functions
in the form of matrices and vectors.</p>
<p>Linear algebra is just a shorthand for performing lots of
multiplications and additions simultaneously. What does it have to do
with our system then? Well, the first thing to note is that the classic
linear function we fit for a one-dimensional regression has the form:
<span class="math display">\[
f(x) = mx + c
\]</span> the classical form for a straight line. From a linear
algebraic perspective, we are looking for multiplications and additions.
We are also looking to separate our parameters from our data. The data
is the <em>givens</em>. In French the word is données literally
translated means <em>givens</em> that’s great, because we don’t need to
change the data, what we need to change are the parameters (or
variables) of the model. In this function the data comes in through
<span class="math inline">\(x\)</span>, and the parameters are <span
class="math inline">\(m\)</span> and <span
class="math inline">\(c\)</span>.</p>
<p>What we’d like to create is a vector of parameters and a vector of
data. Then we could represent the system with vectors that represent the
data, and vectors that represent the parameters.</p>
<p>We look to turn the multiplications and additions into a linear
algebraic form, we have one multiplication (<span
class="math inline">\(m\times c\)</span>) and one addition (<span
class="math inline">\(mx + c\)</span>). But we can turn this into an
inner product by writing it in the following way, <span
class="math display">\[
f(x) = m \times x +
c \times 1,
\]</span> in other words, we’ve extracted the unit value from the
offset, <span class="math inline">\(c\)</span>. We can think of this
unit value like an extra item of data, because it is always given to us,
and it is always set to 1 (unlike regular data, which is likely to
vary!). We can therefore write each input data location, <span
class="math inline">\(\mathbf{ x}\)</span>, as a vector <span
class="math display">\[
\mathbf{ x}= \begin{bmatrix} 1\\ x\end{bmatrix}.
\]</span></p>
<p>Now we choose to also turn our parameters into a vector. The
parameter vector will be defined to contain <span
class="math display">\[
\mathbf{ w}= \begin{bmatrix} c \\ m\end{bmatrix}
\]</span> because if we now take the inner product between these two
vectors we recover <span class="math display">\[
\mathbf{ x}\cdot\mathbf{ w}= 1 \times c + x \times m = mx + c
\]</span> In <code>numpy</code> we can define this vector as follows</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the vector w</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>w[<span class="dv">0</span>] <span class="op">=</span> m</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>w[<span class="dv">1</span>] <span class="op">=</span> c</span></code></pre></div>
<p>This gives us the equivalence between original operation and an
operation in vector space. Whilst the notation here isn’t a lot shorter,
the beauty is that we will be able to add as many features as we like
and keep the same representation. In general, we are now moving to a
system where each of our predictions is given by an inner product. When
we want to represent a linear product in linear algebra, we tend to do
it with the transpose operation, so since we have <span
class="math inline">\(\mathbf{a}\cdot\mathbf{b} =
\mathbf{a}^\top\mathbf{b}\)</span> we can write <span
class="math display">\[
f(\mathbf{ x}_i) = \mathbf{ x}_i^\top\mathbf{ w}.
\]</span> Where we’ve assumed that each data point, <span
class="math inline">\(\mathbf{ x}_i\)</span>, is now written by
appending a 1 onto the original vector <span class="math display">\[
\mathbf{ x}_i = \begin{bmatrix}
1 \\
x_i
\end{bmatrix}
\]</span></p>
<h1 id="design-matrix">Design Matrix</h1>
<p>We can do this for the entire data set to form a <a
href="http://en.wikipedia.org/wiki/Design_matrix"><em>design
matrix</em></a> <span class="math inline">\(\boldsymbol{ \Phi}\)</span>,
<span class="math display">\[
\boldsymbol{ \Phi}
= \begin{bmatrix}
\mathbf{ x}_1^\top \\\
\mathbf{ x}_2^\top \\\
\vdots \\\
\mathbf{ x}_n^\top
\end{bmatrix} = \begin{bmatrix}
1 &amp; x_1 \\\
1 &amp; x_2 \\\
\vdots
&amp; \vdots \\\
1 &amp; x_n
\end{bmatrix},
\]</span> which in <code>numpy</code> can be done with the following
commands:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> np.hstack((np.ones_like(x), x))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Phi)</span></code></pre></div>
<h2 id="writing-the-objective-with-linear-algebra">Writing the Objective
with Linear Algebra</h2>
<p>When we think of the objective function, we can think of it as the
errors where the error is defined in a similar way to what it was in
Legendre’s day <span class="math inline">\(y_i - f(\mathbf{
x}_i)\)</span>, in statistics these errors are also sometimes called <a
href="http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics"><em>residuals</em></a>.
So, we can think as the objective and the prediction function as two
separate parts, first we have, <span class="math display">\[
E(\mathbf{ w}) = \sum_{i=1}^n(y_i - f(\mathbf{ x}_i; \mathbf{ w}))^2,
\]</span> where we’ve made the function <span
class="math inline">\(f(\cdot)\)</span>’s dependence on the parameters
<span class="math inline">\(\mathbf{ w}\)</span> explicit in this
equation. Then we have the definition of the function itself, <span
class="math display">\[
f(\mathbf{ x}_i; \mathbf{ w}) = \mathbf{ x}_i^\top \mathbf{ w}.
\]</span> Let’s look again at these two equations and see if we can
identify any inner products. The first equation is a sum of squares,
which is promising. Any sum of squares can be represented by an inner
product, <span class="math display">\[
a = \sum_{i=1}^{k} b^2_i = \mathbf{b}^\top\mathbf{b}.
\]</span> If we wish to represent <span class="math inline">\(E(\mathbf{
w})\)</span> in this way, all we need to do is convert the sum operator
to an inner product. We can get a vector from that sum operator by
placing both <span class="math inline">\(y_i\)</span> and <span
class="math inline">\(f(\mathbf{ x}_i; \mathbf{ w})\)</span> into
vectors, which we do by defining <span class="math display">\[
\mathbf{ y}= \begin{bmatrix}y_1\\ y_2\\ \vdots \\ y_n\end{bmatrix}
\]</span> and defining <span class="math display">\[
\mathbf{ f}(\mathbf{ x}_1; \mathbf{ w}) = \begin{bmatrix}f(\mathbf{
x}_1; \mathbf{ w})\\ f(\mathbf{ x}_2; \mathbf{ w})\\ \vdots \\
f(\mathbf{ x}_n; \mathbf{ w})\end{bmatrix}.
\]</span> The second of these is a vector-valued function. This term may
appear intimidating, but the idea is straightforward. A vector valued
function is simply a vector whose elements are themselves defined as
<em>functions</em>, i.e., it is a vector of functions, rather than a
vector of scalars. The idea is so straightforward, that we are going to
ignore it for the moment, and barely use it in the derivation. But it
will reappear later when we introduce <em>basis functions</em>. So, we
will for the moment ignore the dependence of <span
class="math inline">\(\mathbf{ f}\)</span> on <span
class="math inline">\(\mathbf{ w}\)</span> and <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> and simply summarise
it by a vector of numbers <span class="math display">\[
\mathbf{ f}= \begin{bmatrix}f_1\\f_2\\
\vdots \\ f_n\end{bmatrix}.
\]</span> This allows us to write our objective in the folowing, linear
algebraic form, <span class="math display">\[
E(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{
f})
\]</span> from the rules of inner products. But what of our matrix <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> of input data? At this
point, we need to dust off <a
href="http://en.wikipedia.org/wiki/Matrix_multiplication"><em>matrix-vector
multiplication</em></a>. Matrix multiplication is simply a convenient
way of performing many inner products together, and it’s exactly what we
need to summarize the operation <span class="math display">\[
f_i = \mathbf{ x}_i^\top\mathbf{ w}.
\]</span> This operation tells us that each element of the vector <span
class="math inline">\(\mathbf{ f}\)</span> (our vector valued function)
is given by an inner product between <span
class="math inline">\(\mathbf{ x}_i\)</span> and <span
class="math inline">\(\mathbf{ w}\)</span>. In other words, it is a
series of inner products. Let’s look at the definition of matrix
multiplication, it takes the form <span class="math display">\[
\mathbf{c} = \mathbf{B}\mathbf{a},
\]</span> where <span class="math inline">\(\mathbf{c}\)</span> might be
a <span class="math inline">\(k\)</span> dimensional vector (which we
can interpret as a <span class="math inline">\(k\times 1\)</span>
dimensional matrix), and <span class="math inline">\(\mathbf{B}\)</span>
is a <span class="math inline">\(k\times k\)</span> dimensional matrix
and <span class="math inline">\(\mathbf{a}\)</span> is a <span
class="math inline">\(k\)</span> dimensional vector (<span
class="math inline">\(k\times 1\)</span> dimensional matrix).</p>
<p>The result of this multiplication is of the form <span
class="math display">\[
\begin{bmatrix}c_1\\c_2 \\ \vdots \\
a_k\end{bmatrix} =
\begin{bmatrix} b_{1,1} &amp; b_{1, 2} &amp; \dots &amp; b_{1, k} \\
b_{2, 1} &amp; b_{2, 2} &amp; \dots &amp; b_{2, k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{k, 1} &amp; b_{k, 2} &amp; \dots &amp; b_{k, k} \end{bmatrix}
\begin{bmatrix}a_1\\a_2 \\
\vdots\\ c_k\end{bmatrix} = \begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 +
\dots +
b_{1, k}a_k\\
b_{2, 1}a_1 + b_{2, 2}a_2 + \dots + b_{2, k}a_k \\
\vdots\\
b_{k, 1}a_1 + b_{k, 2}a_2 + \dots + b_{k, k}a_k\end{bmatrix}.
\]</span> We see that each element of the result, <span
class="math inline">\(\mathbf{a}\)</span> is simply the inner product
between each <em>row</em> of <span
class="math inline">\(\mathbf{B}\)</span> and the vector <span
class="math inline">\(\mathbf{c}\)</span>. Because we have defined each
element of <span class="math inline">\(\mathbf{ f}\)</span> to be given
by the inner product between each <em>row</em> of the design matrix and
the vector <span class="math inline">\(\mathbf{ w}\)</span> we now can
write the full operation in one matrix multiplication,</p>
<p><span class="math display">\[
\mathbf{ f}= \boldsymbol{ \Phi}\mathbf{ w}.
\]</span></p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Phi<span class="op">@</span>w <span class="co"># The @ sign performs matrix multiplication</span></span></code></pre></div>
<p>Combining this result with our objective function, <span
class="math display">\[
E(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{
f})
\]</span> we find we have defined the <em>model</em> with two equations.
One equation tells us the form of our predictive function and how it
depends on its parameters, the other tells us the form of our objective
function.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>resid <span class="op">=</span> (y<span class="op">-</span>f)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> np.dot(resid.T, resid) <span class="co"># matrix multiplication on a single vector is equivalent to a dot product.</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Error function is:&quot;</span>, E)</span></code></pre></div>
<h1 id="objective-optimization">Objective Optimization</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-objective-optimisation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-objective-optimisation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Our <em>model</em> has now been defined with two equations: the
prediction function and the objective function. Now we will use
multivariate calculus to define an <em>algorithm</em> to fit the model.
The separation between model and algorithm is important and is often
overlooked. Our model contains a function that shows how it will be used
for prediction, and a function that describes the objective function we
need to optimize to obtain a good set of parameters.</p>
<p>The model linear regression model we have described is still the same
as the one we fitted above with a coordinate ascent algorithm. We have
only played with the notation to obtain the same model in a matrix and
vector notation. However, we will now fit this model with a different
algorithm, one that is much faster. It is such a widely used algorithm
that from the end user’s perspective it doesn’t even look like an
algorithm, it just appears to be a single operation (or function).
However, underneath the computer calls an algorithm to find the
solution. Further, the algorithm we obtain is very widely used, and
because of this it turns out to be highly optimized.</p>
<p>Once again, we are going to try and find the stationary points of our
objective by finding the <em>stationary points</em>. However, the
stationary points of a multivariate function, are a little bit more
complex to find. As before we need to find the point at which the
gradient is zero, but now we need to use <em>multivariate calculus</em>
to find it. This involves learning a few additional rules of
differentiation (that allow you to do the derivatives of a function with
respect to vector), but in the end it makes things quite a bit easier.
We define vectorial derivatives as follows, <span
class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} =
\begin{bmatrix}\frac{\text{d}E(\mathbf{
w})}{\text{d}w_1}\\\frac{\text{d}E(\mathbf{
w})}{\text{d}w_2}\end{bmatrix}.
\]</span> where <span class="math inline">\(\frac{\text{d}E(\mathbf{
w})}{\text{d}w_1}\)</span> is the <a
href="http://en.wikipedia.org/wiki/Partial_derivative">partial
derivative</a> of the error function with respect to <span
class="math inline">\(w_1\)</span>.</p>
<p>Differentiation through multiplications and additions is relatively
straightforward, and since linear algebra is just multiplication and
addition, then its rules of differentiation are quite straightforward
too, but slightly more complex than regular derivatives.</p>
<h2 id="multivariate-derivatives">Multivariate Derivatives</h2>
<p>We will need two rules of multivariate or <em>matrix</em>
differentiation. The first is differentiation of an inner product. By
remembering that the inner product is made up of multiplication and
addition, we can hope that its derivative is quite straightforward, and
so it proves to be. We can start by thinking about the definition of the
inner product, <span class="math display">\[
\mathbf{a}^\top\mathbf{z} = \sum_{i} a_i
z_i,
\]</span> which if we were to take the derivative with respect to <span
class="math inline">\(z_k\)</span> would simply return the gradient of
the one term in the sum for which the derivative was non-zero, that of
<span class="math inline">\(a_k\)</span>, so we know that <span
class="math display">\[
\frac{\text{d}}{\text{d}z_k} \mathbf{a}^\top \mathbf{z} = a_k
\]</span> and by our definition for multivariate derivatives, we can
simply stack all the partial derivatives of this form in a vector to
obtain the result that <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{z}}
\mathbf{a}^\top \mathbf{z} = \mathbf{a}.
\]</span> The second rule that’s required is differentiation of a
‘matrix quadratic’. A scalar quadratic in <span
class="math inline">\(z\)</span> with coefficient <span
class="math inline">\(c\)</span> has the form <span
class="math inline">\(cz^2\)</span>. If <span
class="math inline">\(\mathbf{z}\)</span> is a <span
class="math inline">\(k\times 1\)</span> vector and <span
class="math inline">\(\mathbf{C}\)</span> is a <span
class="math inline">\(k \times k\)</span> <em>matrix</em> of
coefficients then the matrix quadratic form is written as <span
class="math inline">\(\mathbf{z}^\top \mathbf{C}\mathbf{z}\)</span>,
which is itself a <em>scalar</em> quantity, but it is a function of a
<em>vector</em>.</p>
<h3 id="matching-dimensions-in-matrix-multiplications">Matching
Dimensions in Matrix Multiplications</h3>
<p>There’s a trick for telling a multiplication leads to a scalar
result. When you are doing mathematics with matrices, it’s always worth
pausing to perform a quick sanity check on the dimensions. Matrix
multplication only works when the dimensions match. To be precise, the
‘inner’ dimension of the matrix must match. What is the inner dimension?
If we multiply two matrices <span
class="math inline">\(\mathbf{A}\)</span> and <span
class="math inline">\(\mathbf{B}\)</span>, the first of which has <span
class="math inline">\(k\)</span> rows and <span
class="math inline">\(\ell\)</span> columns and the second of which has
<span class="math inline">\(p\)</span> rows and <span
class="math inline">\(q\)</span> columns, then we can check whether the
multiplication works by writing the dimensionalities next to each other,
<span class="math display">\[
\mathbf{A} \mathbf{B} \rightarrow (k \times
\underbrace{\ell)(p}_\text{inner dimensions} \times q) \rightarrow
(k\times q).
\]</span> The inner dimensions are the two inside dimensions, <span
class="math inline">\(\ell\)</span> and <span
class="math inline">\(p\)</span>. The multiplication will only work if
<span class="math inline">\(\ell=p\)</span>. The result of the
multiplication will then be a <span class="math inline">\(k\times
q\)</span> matrix: this dimensionality comes from the ‘outer
dimensions’. Note that matrix multiplication is not <a
href="http://en.wikipedia.org/wiki/Commutative_property"><em>commutative</em></a>.
And if you change the order of the multiplication, <span
class="math display">\[
\mathbf{B} \mathbf{A} \rightarrow (\ell \times
\underbrace{k)(q}_\text{inner dimensions} \times p) \rightarrow (\ell
\times p).
\]</span> Firstly, it may no longer even work, because now the condition
is that <span class="math inline">\(k=q\)</span>, and secondly the
result could be of a different dimensionality. An exception is if the
matrices are square matrices (e.g., same number of rows as columns) and
they are both <em>symmetric</em>. A symmetric matrix is one for which
<span class="math inline">\(\mathbf{A}=\mathbf{A}^\top\)</span>, or
equivalently, <span class="math inline">\(a_{i,j} = a_{j,i}\)</span> for
all <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span>.</p>
<p>For applying and developing machine learning algorithms you should
get familiar with working with matrices and vectors. You should have
come across them before, but you may not have used them as extensively
as we are doing now. It’s worth getting used to using this trick to
check your work and ensure you know what the dimension of an output
matrix should be. For our matrix quadratic form, it turns out that we
can see it as a special type of inner product. <span
class="math display">\[
\mathbf{z}^\top\mathbf{C}\mathbf{z} \rightarrow (1\times
\underbrace{k) (k}_\text{inner dimensions}\times k) (k\times 1)
\rightarrow
\mathbf{b}^\top\mathbf{z}
\]</span> where <span class="math inline">\(\mathbf{b} =
\mathbf{C}\mathbf{z}\)</span> so therefore the result is a scalar, <span
class="math display">\[
\mathbf{b}^\top\mathbf{z} \rightarrow
(1\times \underbrace{k) (k}_\text{inner dimensions}\times 1) \rightarrow
(1\times 1)
\]</span> where a <span class="math inline">\((1\times 1)\)</span>
matrix is recognised as a scalar.</p>
<p>This implies that we should be able to differentiate this form, and
indeed the rule for its differentiation is slightly more complex than
the inner product, but still quite simple, <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{z}}
\mathbf{z}^\top\mathbf{C}\mathbf{z}= \mathbf{C}\mathbf{z} +
\mathbf{C}^\top
\mathbf{z}.
\]</span> Note that in the special case where <span
class="math inline">\(\mathbf{C}\)</span> is symmetric then we have
<span class="math inline">\(\mathbf{C} = \mathbf{C}^\top\)</span> and
the derivative simplifies to <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{z}} \mathbf{z}^\top\mathbf{C}\mathbf{z}=
2\mathbf{C}\mathbf{z}.
\]</span></p>
<h2 id="differentiate-the-objective">Differentiate the Objective</h2>
<p>First, we need to compute the full objective by substituting our
prediction function into the objective function to obtain the objective
in terms of <span class="math inline">\(\mathbf{ w}\)</span>. Doing this
we obtain <span class="math display">\[
E(\mathbf{ w})= (\mathbf{ y}- \boldsymbol{ \Phi}\mathbf{ w})^\top
(\mathbf{ y}- \boldsymbol{ \Phi}\mathbf{ w}).
\]</span> We now need to differentiate this <em>quadratic form</em> to
find the minimum. We differentiate with respect to the <em>vector</em>
<span class="math inline">\(\mathbf{ w}\)</span>. But before we do that,
we’ll expand the brackets in the quadratic form to obtain a series of
scalar terms. The rules for bracket expansion across the vectors are
similar to those for the scalar system giving, <span
class="math display">\[
(\mathbf{a} - \mathbf{b})^\top
(\mathbf{c} - \mathbf{d}) = \mathbf{a}^\top \mathbf{c} - \mathbf{a}^\top
\mathbf{d} - \mathbf{b}^\top \mathbf{c} + \mathbf{b}^\top \mathbf{d}
\]</span> which substituting for <span class="math inline">\(\mathbf{a}
= \mathbf{c} = \mathbf{ y}\)</span> and <span
class="math inline">\(\mathbf{b}=\mathbf{d} = \boldsymbol{ \Phi}\mathbf{
w}\)</span> gives <span class="math display">\[
E(\mathbf{ w})=
\mathbf{ y}^\top\mathbf{ y}- 2\mathbf{ y}^\top\boldsymbol{ \Phi}\mathbf{
w}+
\mathbf{ w}^\top\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}
\]</span> where we used the fact that <span
class="math inline">\(\mathbf{ y}^\top\boldsymbol{ \Phi}\mathbf{
w}=\mathbf{ w}^\top\boldsymbol{ \Phi}^\top\mathbf{ y}\)</span>.</p>
<p>Now we can use our rules of differentiation to compute the derivative
of this form, which is, <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{ w}}E(\mathbf{ w})=- 2\boldsymbol{
\Phi}^\top \mathbf{ y}+
2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w},
\]</span> where we have exploited the fact that <span
class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\)</span>
is symmetric to obtain this result.</p>
<h3 id="exercise-1">Exercise 1</h3>
<p>Use the equivalence between our vector and our matrix formulations of
linear regression, alongside our definition of vector derivates, to
match the gradients we’ve computed directly for <span
class="math inline">\(\frac{\text{d}E(c, m)}{\text{d}c}\)</span> and
<span class="math inline">\(\frac{\text{d}E(c, m)}{\text{d}m}\)</span>
to those for <span class="math inline">\(\frac{\text{d}E(\mathbf{
w})}{\text{d}\mathbf{ w}}\)</span>.</p>
<h1 id="update-equation-for-global-optimum">Update Equation for Global
Optimum</h1>
<p>We need to find the minimum of our objective function. Using our
objective function, we can minimize for our parameter vector <span
class="math inline">\(\mathbf{ w}\)</span>. Firstly, we seek stationary
points by find parameter vectors that solve for when the gradients are
zero, <span class="math display">\[
\mathbf{0}=- 2\boldsymbol{ \Phi}^\top
\mathbf{ y}+ 2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w},
\]</span> where <span class="math inline">\(\mathbf{0}\)</span> is a
<em>vector</em> of zeros. Rearranging this equation, we find the
solution to be <span class="math display">\[
\boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{
\Phi}^\top
\mathbf{ y}
\]</span> which is a matrix equation of the familiar form <span
class="math inline">\(\mathbf{A}\mathbf{x} = \mathbf{b}\)</span>.</p>
<h2 id="solving-the-multivariate-system">Solving the Multivariate
System</h2>
<p>The solution for <span class="math inline">\(\mathbf{ w}\)</span> can
be written mathematically in terms of a matrix inverse of <span
class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{
\Phi}\)</span>, but computation of a matrix inverse requires an
algorithm to resolve it. You’ll know this if you had to invert, by hand,
a <span class="math inline">\(3\times 3\)</span> matrix in high school.
From a numerical stability perspective, it is also best not to compute
the matrix inverse directly, but rather to ask the computer to
<em>solve</em> the system of linear equations given by <span
class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{
\Phi}^\top\mathbf{ y}
\]</span> for <span class="math inline">\(\mathbf{ w}\)</span>.</p>
<h2 id="multivariate-linear-regression">Multivariate Linear
Regression</h2>
<p>A major advantage of the new system is that we can build a linear
regression on a multivariate system. The matrix calculus didn’t specify
what the length of the vector <span class="math inline">\(\mathbf{
x}\)</span> should be, or equivalently the size of the design
matrix.</p>
<h2 id="movie-body-count-data">Movie Body Count Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/movie-body-count-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/movie-body-count-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This is a data set created by Simon Garnier and Rany Olson for
exploring the differences between R and Python for data science. The
data contains information about different movies augmented by estimates
about how many on-screen deaths are contained in the movie. The data is
craped from <a href="http://www.moviebodycounts.com"
class="uri">http://www.moviebodycounts.com</a>. The data contains the
following featuers for each movie: <code>Year</code>,
<code>Body_Count</code>, <code>MPAA_Rating</code>, <code>Genre</code>,
<code>Director</code>, <code>Actors</code>, <code>Length_Minutes</code>,
<code>IMDB_Rating</code>.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.movie_body_count()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>movies <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span></code></pre></div>
<p>The data is provided to us in the form of a pandas data frame, we can
see the features we’re provided with by inspecting the columns of the
data frame.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;, &#39;</span>.join(movies.columns))</span></code></pre></div>
<h2 id="multivariate-regression-on-movie-body-count-data">Multivariate
Regression on Movie Body Count Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/movie-body-count-linear-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/movie-body-count-linear-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now we will build a design matrix based on the numeric features:
year, Body_Count, Length_Minutes in an effort to predict the rating. We
build the design matrix as follows:</p>
<p>Bias as an additional feature.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>select_features <span class="op">=</span> [<span class="st">&#39;Year&#39;</span>, <span class="st">&#39;Body_Count&#39;</span>, <span class="st">&#39;Length_Minutes&#39;</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> movies[select_features]</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>Phi[<span class="st">&#39;Eins&#39;</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># add a column for the offset</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> movies[[<span class="st">&#39;IMDB_Rating&#39;</span>]]</span></code></pre></div>
<p>Now let’s perform a linear regression. But this time, we will create
a pandas data frame for the result so we can store it in a form that we
can visualise easily.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>np.linalg.solve(Phi.T<span class="op">@</span>Phi, Phi.T<span class="op">@</span>y),  <span class="co"># solve linear regression here</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                 index <span class="op">=</span> Phi.columns,  <span class="co"># columns of Phi become rows of w</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                 columns<span class="op">=</span>[<span class="st">&#39;regression_coefficient&#39;</span>]) <span class="co"># the column of Phi is the value of regression coefficient</span></span></code></pre></div>
<p>We can check the residuals to see how good our estimates are. First
we create a pandas data frame containing the predictions and use it to
compute the residuals.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>ypred <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>(Phi<span class="op">@</span>w).values, columns<span class="op">=</span>[<span class="st">&#39;IMDB_Rating&#39;</span>])</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>resid <span class="op">=</span> y<span class="op">-</span>ypred</span></code></pre></div>
<div class="figure">
<div id="movie-body-count-residuals-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/movie-body-count-rating-residuals.svg" width="80%" style=" ">
</object>
</div>
<div id="movie-body-count-residuals-magnify" class="magnify"
onclick="magnifyFigure(&#39;movie-body-count-residuals&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="movie-body-count-residuals-caption" class="caption-frame">
<p>Figure: Residual values for the ratings from the prediction of the
movie rating given the data from the film.</p>
</div>
</div>
<p>Which shows our model <em>hasn’t</em> yet done a great job of
representation, because the spread of values is large. We can check what
the rating is dominated by in terms of regression coefficients.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>w</span></code></pre></div>
<p>Although we have to be a little careful about interpretation because
our input values live on different scales, however it looks like we are
dominated by the bias, with a small negative effect for later films (but
bear in mind the years are large, so this effect is probably larger than
it looks) and a positive effect for length. So it looks like long
earlier films generally do better, but the residuals are so high that we
probably haven’t modelled the system very well.</p>
<h1 id="underdetermined-system">Underdetermined System</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/underdetermined-system.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/underdetermined-system.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>What about the situation where you have more parameters than data in
your simultaneous equation? This is known as an <em>underdetermined</em>
system. In fact, this set up is in some sense <em>easier</em> to solve,
because we don’t need to think about introducing a slack variable
(although it might make a lot of sense from a <em>modelling</em>
perspective to do so).</p>
<p>The way Laplace proposed resolving an overdetermined system, was to
introduce slack variables, <span
class="math inline">\(\epsilon_i\)</span>, which needed to be estimated
for each point. The slack variable represented the difference between
our actual prediction and the true observation. This is known as the
<em>residual</em>. By introducing the slack variable, we now have an
additional <span class="math inline">\(n\)</span> variables to estimate,
one for each data point, <span
class="math inline">\(\{\epsilon_i\}\)</span>. This turns the
overdetermined system into an underdetermined system. Introduction of
<span class="math inline">\(n\)</span> variables, plus the original
<span class="math inline">\(m\)</span> and <span
class="math inline">\(c\)</span> gives us <span
class="math inline">\(n+2\)</span> parameters to be estimated from <span
class="math inline">\(n\)</span> observations, which makes the system
<em>underdetermined</em>. However, we then made a probabilistic
assumption about the slack variables, we assumed that the slack
variables were distributed according to a probability density. And for
the moment we have been assuming that density was the Gaussian, <span
class="math display">\[\epsilon_i \sim
\mathcal{N}\left(0,\sigma^2\right),\]</span> with zero mean and variance
<span class="math inline">\(\sigma^2\)</span>.</p>
<p>The follow up question is whether we can do the same thing with the
parameters. If we have two parameters and only one unknown, can we place
a probability distribution over the parameters as we did with the slack
variables? The answer is yes.</p>
<h2 id="underdetermined-system-1">Underdetermined System</h2>
<div class="figure">
<div id="under-determined-system-9-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/under_determined_system009.svg" width="40%" style=" ">
</object>
</div>
<div id="under-determined-system-9-magnify" class="magnify"
onclick="magnifyFigure(&#39;under-determined-system-9&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="under-determined-system-9-caption" class="caption-frame">
<p>Figure: An underdetermined system can be fit by considering
uncertainty. Multiple solutions are consistent with one specified
point.</p>
</div>
</div>
<h2 id="two-dimensional-gaussian">Two Dimensional Gaussian</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/two-d-gaussian.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/two-d-gaussian.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Consider the distribution of height (in meters) of an adult male
human population. We will approximate the marginal density of heights as
a Gaussian density with mean given by <span
class="math inline">\(1.7\text{m}\)</span> and a standard deviation of
<span class="math inline">\(0.15\text{m}\)</span>, implying a variance
of <span class="math inline">\(\sigma^2=0.0225\)</span>, <span
class="math display">\[
  p(h) \sim \mathcal{N}\left(1.7,0.0225\right).
  \]</span> Similarly, we assume that weights of the population are
distributed a Gaussian density with a mean of <span
class="math inline">\(75 \text{kg}\)</span> and a standard deviation of
<span class="math inline">\(6 kg\)</span> (implying a variance of 36),
<span class="math display">\[
  p(w) \sim \mathcal{N}\left(75,36\right).
  \]</span></p>
<div class="figure">
<div id="height-weight-gaussian-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/height_weight_gaussian.svg" width="70%" style=" ">
</object>
</div>
<div id="height-weight-gaussian-magnify" class="magnify"
onclick="magnifyFigure(&#39;height-weight-gaussian&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="height-weight-gaussian-caption" class="caption-frame">
<p>Figure: Gaussian distributions for height and weight.</p>
</div>
</div>
<h2 id="independence-assumption">Independence Assumption</h2>
<p>First of all, we make an independence assumption, we assume that
height and weight are independent. The definition of probabilistic
independence is that the joint density, <span class="math inline">\(p(w,
h)\)</span>, factorizes into its marginal densities, <span
class="math display">\[
  p(w, h) = p(w)p(h).
  \]</span> Given this assumption we can sample from the joint
distribution by independently sampling weights and heights.</p>
<div class="figure">
<div id="independent-height-weight-7-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/independent_height_weight007.svg" width="70%" style=" ">
</object>
</div>
<div id="independent-height-weight-7-magnify" class="magnify"
onclick="magnifyFigure(&#39;independent-height-weight-7&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="independent-height-weight-7-caption" class="caption-frame">
<p>Figure: Samples from independent Gaussian variables that might
represent heights and weights.</p>
</div>
</div>
<p>In reality height and weight are <em>not</em> independent. Taller
people tend on average to be heavier, and heavier people are likely to
be taller. This is reflected by the <em>body mass index</em>. A ratio
suggested by one of the fathers of statistics, Adolphe Quetelet.
Quetelet was interested in the notion of the <em>average man</em> and
collected various statistics about people. He defined the BMI to be,
<span class="math display">\[
\text{BMI} = \frac{w}{h^2}
\]</span>To deal with this dependence we now introduce the notion of
<em>correlation</em> to the multivariate Gaussian density.</p>
<h2 id="sampling-two-dimensional-variables">Sampling Two Dimensional
Variables</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/two-d-gaussian-correlated-sample.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/two-d-gaussian-correlated-sample.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="correlated-height-weight-7-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/correlated_height_weight007.svg" width="70%" style=" ">
</object>
</div>
<div id="correlated-height-weight-7-magnify" class="magnify"
onclick="magnifyFigure(&#39;correlated-height-weight-7&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="correlated-height-weight-7-caption" class="caption-frame">
<p>Figure: Samples from <em>correlated</em> Gaussian variables that
might represent heights and weights.</p>
</div>
</div>
<h2 id="independent-gaussians">Independent Gaussians</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/two-d-gaussian-maths.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/two-d-gaussian-maths.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><span class="math display">\[
p(w, h) = p(w)p(h)
\]</span></p>
<p><span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi \sigma_1^2}\sqrt{2\pi\sigma_2^2}}
\exp\left(-\frac{1}{2}\left(\frac{(w-\mu_1)^2}{\sigma_1^2} +
\frac{(h-\mu_2)^2}{\sigma_2^2}\right)\right)
\]</span></p>
<p><span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi\sigma_1^22\pi\sigma_2^2}}
\exp\left(-\frac{1}{2}\left(\begin{bmatrix}w \\ h\end{bmatrix} -
\begin{bmatrix}\mu_1 \\
\mu_2\end{bmatrix}\right)^\top\begin{bmatrix}\sigma_1^2&amp;
0\\0&amp;\sigma_2^2\end{bmatrix}^{-1}\left(\begin{bmatrix}w \\
h\end{bmatrix} - \begin{bmatrix}\mu_1 \\
\mu_2\end{bmatrix}\right)\right)
\]</span></p>
<p><span class="math display">\[
p(\mathbf{ y}) = \frac{1}{\det{2\pi \mathbf{D}}^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}(\mathbf{ y}- \boldsymbol{
\mu})^\top\mathbf{D}^{-1}(\mathbf{ y}- \boldsymbol{ \mu})\right)
\]</span></p>
<h2 id="correlated-gaussian">Correlated Gaussian</h2>
<p>Form correlated from original by rotating the data space using matrix
<span class="math inline">\(\mathbf{R}\)</span>.</p>
<p><span class="math display">\[
p(\mathbf{ y}) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}(\mathbf{ y}- \boldsymbol{
\mu})^\top\mathbf{D}^{-1}(\mathbf{ y}- \boldsymbol{ \mu})\right)
\]</span></p>
<p><span class="math display">\[
p(\mathbf{ y}) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}(\mathbf{R}^\top\mathbf{ y}-
\mathbf{R}^\top\boldsymbol{
\mu})^\top\mathbf{D}^{-1}(\mathbf{R}^\top\mathbf{ y}-
\mathbf{R}^\top\boldsymbol{ \mu})\right)
\]</span></p>
<p><span class="math display">\[
p(\mathbf{ y}) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}(\mathbf{ y}- \boldsymbol{
\mu})^\top\mathbf{R}\mathbf{D}^{-1}\mathbf{R}^\top(\mathbf{ y}-
\boldsymbol{ \mu})\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\mathbf{C}^{-1} = \mathbf{R}\mathbf{D}^{-1} \mathbf{R}^\top
\]</span></p>
<p><span class="math display">\[
p(\mathbf{ y}) = \frac{1}{\det{2\pi\mathbf{C}}^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}(\mathbf{ y}- \boldsymbol{
\mu})^\top\mathbf{C}^{-1} (\mathbf{ y}- \boldsymbol{ \mu})\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\mathbf{C}= \mathbf{R}\mathbf{D} \mathbf{R}^\top
\]</span></p>
<h2 id="basis-functions">Basis Functions</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-nn.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-nn.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Here’s the idea, instead of working directly on the original input
space, <span class="math inline">\(\mathbf{ x}\)</span>, we build models
in a new space, <span class="math inline">\(\boldsymbol{ \phi}(\mathbf{
x})\)</span> where <span class="math inline">\(\boldsymbol{
\phi}(\cdot)\)</span> is a <em>vector-valued</em> function that is
defined on the space <span class="math inline">\(\mathbf{
x}\)</span>.</p>
<h2 id="quadratic-basis">Quadratic Basis</h2>
<p>Remember, that a <em>vector-valued function</em> is just a vector
that contains functions instead of values. Here’s an example for a one
dimensional input space, <span class="math inline">\(x\)</span>, being
projected to a <em>quadratic</em> basis. First we consider each basis
function in turn, we can think of the elements of our vector as being
indexed so that we have <span class="math display">\[
\begin{align*}
\phi_1(x) &amp; = 1, \\
\phi_2(x) &amp; = x, \\
\phi_3(x) &amp; = x^2.
\end{align*}
\]</span> Now we can consider them together by placing them in a vector,
<span class="math display">\[
\boldsymbol{ \phi}(x) = \begin{bmatrix} 1\\ x \\ x^2\end{bmatrix}.
\]</span> For the vector-valued function, we have simply collected the
different functions together in the same vector making them notationally
easier to deal with in our mathematics.</p>
<p>When we consider the vector-valued function for each data point, then
we place all the data into a matrix. The result is a matrix valued
function, <span class="math display">\[
\boldsymbol{ \Phi}(\mathbf{ x}) =
\begin{bmatrix} 1 &amp; x_1 &amp;
x_1^2 \\
1 &amp; x_2 &amp; x_2^2\\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2
\end{bmatrix}
\]</span> where we are still in the one dimensional input setting so
<span class="math inline">\(\mathbf{ x}\)</span> here represents a
vector of our inputs with <span class="math inline">\(n\)</span>
elements.</p>
<p>Let’s try constructing such a matrix for a set of inputs. First of
all, we create a function that returns the matrix valued function.</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quadratic(x, <span class="op">**</span>kwargs):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Take in a vector of input values and return the design matrix associated </span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">    with the basis functions.&quot;&quot;&quot;</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.hstack([np.ones((x.shape[<span class="dv">0</span>], <span class="dv">1</span>)), x, x<span class="op">**</span><span class="dv">2</span>])</span></code></pre></div>
<h2 id="functions-derived-from-quadratic-basis">Functions Derived from
Quadratic Basis</h2>
<p><span class="math display">\[
f(x) = {\color{red}{w_0}}   + {\color{magenta}{w_1 x}} +
{\color{blue}{w_2 x^2}}
\]</span></p>
<div class="figure">
<div id="quadratic-basis-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/quadratic_basis002.svg" width="80%" style=" ">
</object>
</div>
<div id="quadratic-basis-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;quadratic-basis-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="quadratic-basis-2-caption" class="caption-frame">
<p>Figure: The set of functions which are combined to form a
<em>quadratic</em> basis.</p>
</div>
</div>
<p>This function takes in an <span class="math inline">\(n\times
1\)</span> dimensional vector and returns an <span
class="math inline">\(n\times 3\)</span> dimensional <em>design
matrix</em> containing the basis functions. We can plot those basis
functions against there input as follows.</p>
<p>The actual function we observe is then made up of a sum of these
functions. This is the reason for the name basis. The term
<em>basis</em> means ‘the underlying support or foundation for an idea,
argument, or process’, and in this context they form the underlying
support for our prediction function. Our prediction function can only be
composed of a weighted linear sum of our basis functions.</p>
<h2 id="quadratic-functions">Quadratic Functions</h2>
<div class="figure">
<div id="quadratic-function-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/quadratic_function002.svg" width="80%" style=" ">
</object>
</div>
<div id="quadratic-function-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;quadratic-function-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="quadratic-function-2-caption" class="caption-frame">
<p>Figure: Functions constructed by weighted sum of the components of a
quadratic basis.</p>
</div>
</div>
<h2 id="rectified-linear-units">Rectified Linear Units</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/relu-basis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/relu-basis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The rectified linear unit is a basis function that emerged out of the
deep learning community. Rectified linear units are popular in the
current generation of multilayer perceptron models, or deep networks.
These basis functions start flat, and then become linear functions at a
certain threshold. <span class="math display">\[
\phi_j(x) = xH(v_j x+ v_0)
\]</span></p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> relu</span></code></pre></div>
<div class="figure">
<div id="relu-basis-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/relu_basis004.svg" width="80%" style=" ">
</object>
</div>
<div id="relu-basis-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;relu-basis-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="relu-basis-2-caption" class="caption-frame">
<p>Figure: The set of functions which are combined to form a rectified
linear unit basis.</p>
</div>
</div>
<h2 id="functions-derived-from-relu-basis">Functions Derived from Relu
Basis</h2>
<p><span class="math display">\[
f(x) = \color{red}{w_0}   + \color{magenta}{w_1 xH(x+1.0) } +
\color{blue}{w_2 xH(x+0.33) } + \color{green}{w_3 xH(x-0.33)}
+  \color{cyan}{w_4 xH(x-1.0)}
\]</span></p>
<div class="figure">
<div id="relu-function-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/relu_function002.svg" width="80%" style=" ">
</object>
</div>
<div id="relu-function-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;relu-function-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="relu-function-2-caption" class="caption-frame">
<p>Figure: A rectified linear unit basis is made up of different
rectified linear unit functions centered at different points.</p>
</div>
</div>
<h2 id="gaussian-processes">Gaussian Processes</h2>
<p>Models where we model the entire joint distribution of our training
data, <span class="math inline">\(p(\mathbf{ y}, \mathbf{X})\)</span>
are sometimes described as <em>generative models</em>. Because we can
use sampling to generate data sets that represent all our assumptions.
However, as we discussed in the sessions on and , this can be a bad
idea, because if our assumptions are wrong then we can make poor
predictions. We can try to make more complex assumptions about data to
alleviate the problem, but then this typically leads to challenges for
tractable application of the sum and rules of probability that are
needed to compute the relevant marginal and conditional densities. If we
know the form of the question we wish to answer then we typically try
and represent that directly, through <span
class="math inline">\(p(\mathbf{ y}|\mathbf{X})\)</span>. In practice,
we also have been making assumptions of conditional independence given
the model parameters, <span class="math display">\[
p(\mathbf{ y}|\mathbf{X}, \mathbf{ w}) =
\prod_{i=1}^{n} p(y_i | \mathbf{ x}_i, \mathbf{ w})
\]</span> Gaussian processes are <em>not</em> normally considered to be
<em>generative models</em>, but we will be much more interested in the
principles of conditioning in Gaussian processes because we will use
conditioning to make predictions between our test and training data. We
will avoid the data conditional indpendence assumption in favour of a
richer assumption about the data, in a Gaussian process we assume data
is <em>jointly Gaussian</em> with a particular mean and covariance,
<span class="math display">\[
\mathbf{ y}|\mathbf{X}\sim
\mathcal{N}\left(\mathbf{m}(\mathbf{X}),\mathbf{K}(\mathbf{X})\right),
\]</span> where the conditioning is on the inputs <span
class="math inline">\(\mathbf{X}\)</span> which are used for computing
the mean and covariance. For this reason they are known as mean and
covariance functions.</p>
<h2 id="linear-model-overview">Linear Model Overview</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-model-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-model-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>However, we are focussing on what happens in models which are
non-linear in the inputs, whereas the above would be <em>linear</em> in
the inputs. To consider these, we introduce a matrix, called the design
matrix. We set each activation function computed at each data point to
be <span class="math display">\[
\phi_{i,j} = \phi(\mathbf{ w}^{(1)}_{j}, \mathbf{ x}_{i})
\]</span> and define the matrix of activations (known as the <em>design
matrix</em> in statistics) to be, <span class="math display">\[
\boldsymbol{ \Phi}=
\begin{bmatrix}
\phi_{1, 1} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, h} \\
\phi_{1, 2} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{n, 1} &amp; \phi_{n, 2} &amp; \dots &amp; \phi_{n, h}
\end{bmatrix}.
\]</span> By convention this matrix always has <span
class="math inline">\(n\)</span> rows and <span
class="math inline">\(h\)</span> columns, now if we define the vector of
all noise corruptions, <span class="math inline">\(\boldsymbol{
\epsilon}= \left[\epsilon_1, \dots \epsilon_n\right]^\top\)</span>.</p>
<p>If we define the prior distribution over the vector <span
class="math inline">\(\mathbf{ w}\)</span> to be Gaussian, <span
class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span> then we can use rules of multivariate Gaussians to see that,
<span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top + \sigma^2 \mathbf{I}\right).
\]</span></p>
<p>In other words, our training data is distributed as a multivariate
Gaussian, with zero mean and a covariance given by <span
class="math display">\[
\mathbf{K}= \alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top + \sigma^2
\mathbf{I}.
\]</span></p>
<p>This is an <span class="math inline">\(n\times n\)</span> size
matrix. Its elements are in the form of a function. The maths shows that
any element, index by <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span>, is a function <em>only</em> of inputs
associated with data points <span class="math inline">\(i\)</span> and
<span class="math inline">\(j\)</span>, <span
class="math inline">\(\mathbf{ y}_i\)</span>, <span
class="math inline">\(\mathbf{ y}_j\)</span>. <span
class="math inline">\(k_{i,j} = k\left(\mathbf{ x}_i, \mathbf{
x}_j\right)\)</span></p>
<p>If we look at the portion of this function associated only with <span
class="math inline">\(f(\cdot)\)</span>, i.e. we remove the noise, then
we can write down the covariance associated with our neural network,
<span class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)
\]</span> so the elements of the covariance or <em>kernel</em> matrix
are formed by inner products of the rows of the <em>design
matrix</em>.</p>
<h2 id="gaussian-process">Gaussian Process</h2>
<p>This is the essence of a Gaussian process. Instead of making
assumptions about our density over each data point, <span
class="math inline">\(y_i\)</span> as i.i.d. we make a joint Gaussian
assumption over our data. The covariance matrix is now a function of
both the parameters of the activation function, <span
class="math inline">\(\mathbf{V}\)</span>, and the input variables,
<span class="math inline">\(\mathbf{X}\)</span>. This comes about
through integrating out the parameters of the model, <span
class="math inline">\(\mathbf{ w}\)</span>.</p>
<h2 id="basis-functions-1">Basis Functions</h2>
<p>We can basically put anything inside the basis functions, and many
people do. These can be deep kernels <span class="citation"
data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or we can learn the
parameters of a convolutional neural network inside there.</p>
<p>Viewing a neural network in this way is also what allows us to beform
sensible <em>batch</em> normalizations <span class="citation"
data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</p>
<h2 id="radial-basis-functions">Radial Basis Functions</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/radial-basis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/radial-basis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Another type of basis is sometimes known as a ‘radial basis’ because
the effect basis functions are constructed on ‘centres’ and the effect
of each basis function decreases as the radial distance from each centre
increases.</p>
<p><span class="math display">\[
\phi_j(x) = \exp\left(-\frac{(x-\mu_j)^2}{\ell^2}\right)
\]</span></p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> radial</span></code></pre></div>
<div class="figure">
<div id="radial-basis-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/radial_basis002.svg" width="80%" style=" ">
</object>
</div>
<div id="radial-basis-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;radial-basis-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="radial-basis-2-caption" class="caption-frame">
<p>Figure: The set of functions which are combined to form the radial
basis.</p>
</div>
</div>
<h2 id="functions-derived-from-radial-basis">Functions Derived from
Radial Basis</h2>
<p><span class="math display">\[
f(x) = \color{red}{w_1 e^{-2(x+1)^2}}  + \color{magenta}{w_2e^{-2x^2}} +
\color{blue}{w_3 e^{-2(x-1)^2}}
\]</span></p>
<div class="figure">
<div id="radial-function-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/radial_function002.svg" width="80%" style=" ">
</object>
</div>
<div id="radial-function-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;radial-function-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="radial-function-2-caption" class="caption-frame">
<p>Figure: A radial basis is made up of different locally effective
functions centered at different points.</p>
</div>
</div>
<h2 id="marginal-likelihood">Marginal Likelihood</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-from-basis-functions.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-from-basis-functions.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>To understand the Gaussian process we’re going to build on our
understanding of the marginal likelihood for Bayesian regression. In the
session on we sampled directly from the weight vector, <span
class="math inline">\(\mathbf{ w}\)</span> and applied it to the basis
matrix <span class="math inline">\(\boldsymbol{ \Phi}\)</span> to obtain
a sample from the prior and a sample from the posterior. It is often
helpful to think of modeling techniques as <em>generative</em> models.
To give some thought as to what the process for obtaining data from the
model is. From the perspective of Gaussian processes, we want to start
by thinking of basis function models, where the parameters are sampled
from a prior, but move to thinking about sampling from the marginal
likelihood directly.</p>
<h2 id="sampling-from-the-prior">Sampling from the Prior</h2>
<p>The first thing we’ll do is to set up the parameters of the model,
these include the parameters of the prior, the parameters of the basis
functions and the noise level.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set prior variance on w</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">4.</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set the order of the polynomial basis set</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>degree <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set the noise variance</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> <span class="fl">0.01</span></span></code></pre></div>
<p>Now we have the variance, we can sample from the prior distribution
to see what form we are imposing on the functions <em>a priori</em>.</p>
<p>Let’s now compute a range of values to make predictions at, spanning
the <em>new</em> space of inputs,</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> polynomial(x, degree, loc, scale):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    degrees <span class="op">=</span> np.arange(degree<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((x<span class="op">-</span>loc)<span class="op">/</span>scale)<span class="op">**</span>degrees</span></code></pre></div>
<p>now let’s build the basis matrices. First we load in the data</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>loc <span class="op">=</span> <span class="fl">1950.</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> <span class="fl">100.</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>num_data <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>num_pred_data <span class="op">=</span> <span class="dv">100</span> <span class="co"># how many points to use for plotting predictions</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>x_pred <span class="op">=</span> np.linspace(<span class="dv">1880</span>, <span class="dv">2030</span>, num_pred_data)[:, np.newaxis] <span class="co"># input locations for predictions</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, degree<span class="op">=</span>degree, loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, degree<span class="op">=</span>degree, loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span></code></pre></div>
<h2 id="weight-space-view">Weight Space View</h2>
<p>To generate typical functional predictions from the model, we need a
set of model parameters. We assume that the parameters are drawn
independently from a Gaussian density, <span class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span> then we can combine this with the definition of our prediction
function <span class="math inline">\(f(\mathbf{ x})\)</span>, <span
class="math display">\[
f(\mathbf{ x}) = \mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}).
\]</span> We can now sample from the prior density to obtain a vector
<span class="math inline">\(\mathbf{ w}\)</span> using the function
<code>np.random.normal</code> and combine these parameters with our
basis to create some samples of what <span
class="math inline">\(f(\mathbf{ x})\)</span> looks like,</p>
<h2 id="function-space-view">Function Space View</h2>
<p>The process we have used to generate the samples is a two stage
process. To obtain each function, we first generated a sample from the
prior, <span class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha \mathbf{I}\right)
\]</span> then if we compose our basis matrix, <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> from the basis
functions associated with each row then we get, <span
class="math display">\[
\boldsymbol{ \Phi}= \begin{bmatrix}\boldsymbol{ \phi}(\mathbf{ x}_1) \\
\vdots \\
\boldsymbol{ \phi}(\mathbf{ x}_n)\end{bmatrix}
\]</span> then we can write down the vector of function values, as
evaluated at <span class="math display">\[
\mathbf{ f}= \begin{bmatrix} f_1
\\ \vdots f_n\end{bmatrix}
\]</span> in the form <span class="math display">\[
\mathbf{ f}= \boldsymbol{ \Phi}\mathbf{ w}.
\]</span></p>
<p>Now we can use standard properties of multivariate Gaussians to write
down the probability density that is implied over <span
class="math inline">\(\mathbf{ f}\)</span>. In particular we know that
if <span class="math inline">\(\mathbf{ w}\)</span> is sampled from a
multivariate normal (or multivariate Gaussian) with covariance <span
class="math inline">\(\alpha \mathbf{I}\)</span> and zero mean, then
assuming that <span class="math inline">\(\boldsymbol{ \Phi}\)</span> is
a deterministic matrix (i.e. it is not sampled from a probability
density) then the vector <span class="math inline">\(\mathbf{
f}\)</span> will also be distributed according to a zero mean
multivariate normal as follows, <span class="math display">\[
\mathbf{ f}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top\right).
\]</span></p>
<p>The question now is, what happens if we sample <span
class="math inline">\(\mathbf{ f}\)</span> directly from this density,
rather than first sampling <span class="math inline">\(\mathbf{
w}\)</span> and then multiplying by <span
class="math inline">\(\boldsymbol{ \Phi}\)</span>. Let’s try this. First
of all we define the covariance as <span class="math display">\[
\mathbf{K}= \alpha
\boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top.
\]</span></p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> alpha<span class="op">*</span>Phi_pred<span class="op">@</span>Phi_pred.T</span></code></pre></div>
<p>Now we can use the <code>np.random.multivariate_normal</code> command
for sampling from a multivariate normal with covariance given by <span
class="math inline">\(\mathbf{K}\)</span> and zero mean,</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    f_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_pred.flatten(), f_sample.flatten(), linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>mlai.write_figure(<span class="st">&#39;gp-sample-basis-function.svg&#39;</span>, directory<span class="op">=</span><span class="st">&#39;./kern&#39;</span>)</span></code></pre></div>
<div class="figure">
<div id="gp-sample-basis-function-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gp-sample-basis-function.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-sample-basis-function-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-sample-basis-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-sample-basis-function-caption" class="caption-frame">
<p>Figure: Samples directly from the covariance function implied by the
basis function based covariance, <span class="math inline">\(\alpha
\boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top\)</span>.</p>
</div>
</div>
<p>The samples appear very similar to those which we obtained
indirectly. That is no surprise because they are effectively drawn from
the same mutivariate normal density. However, when sampling <span
class="math inline">\(\mathbf{ f}\)</span> directly we created the
covariance for <span class="math inline">\(\mathbf{ f}\)</span>. We can
visualise the form of this covaraince in an image in python with a
colorbar to show scale.</p>
<div class="figure">
<div id="basis-covariance-function-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/basis-covariance-function.svg" width="60%" style=" ">
</object>
</div>
<div id="basis-covariance-function-magnify" class="magnify"
onclick="magnifyFigure(&#39;basis-covariance-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="basis-covariance-function-caption" class="caption-frame">
<p>Figure: Covariance of the function implied by the basis set <span
class="math inline">\(\alpha\boldsymbol{ \Phi}\boldsymbol{
\Phi}^\top\)</span>.</p>
</div>
</div>
<p>This image is the covariance expressed between different points on
the function. In regression we normally also add independent Gaussian
noise to obtain our observations <span class="math inline">\(\mathbf{
y}\)</span>, <span class="math display">\[
\mathbf{ y}= \mathbf{ f}+ \boldsymbol{\epsilon}
\]</span> where the noise is sampled from an independent Gaussian
distribution with variance <span
class="math inline">\(\sigma^2\)</span>, <span class="math display">\[
\epsilon \sim \mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right).
\]</span> we can use properties of Gaussian variables, i.e. the fact
that sum of two Gaussian variables is also Gaussian, and that it’s
covariance is given by the sum of the two covariances, whilst the mean
is given by the sum of the means, to write down the marginal likelihood,
<span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{0},\boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top +\sigma^2\mathbf{I}\right).
\]</span> Sampling directly from this density gives us the noise
corrupted functions,</p>
<div class="figure">
<div id="gp-sample-basis-functions-plus-noise-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gp-sample-basis-function-plus-noise.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-sample-basis-functions-plus-noise-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-sample-basis-functions-plus-noise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-sample-basis-functions-plus-noise-caption"
class="caption-frame">
<p>Figure: Samples directly from the covariance function implied by the
noise corrupted basis function based covariance, <span
class="math inline">\(\alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top +
\sigma^2 \mathbf{I}\)</span>.</p>
</div>
</div>
<p>where the effect of our noise term is to roughen the sampled
functions, we can also increase the variance of the noise to see a
different effect,</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> alpha<span class="op">*</span>Phi_pred<span class="op">@</span>Phi_pred.T <span class="op">+</span> sigma2<span class="op">*</span>np.eye(x_pred.size)</span></code></pre></div>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    y_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_pred.flatten(), y_sample.flatten())</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>mlai.write_figure(<span class="st">&#39;gp-sample-basis-function-plus-large-noise.svg&#39;</span>, </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                  directory<span class="op">=</span><span class="st">&#39;./kern&#39;</span>)</span></code></pre></div>
<div class="figure">
<div id="gp-sample-basis-functions-plus-large-noise-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gp-sample-basis-function-plus-large-noise.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-sample-basis-functions-plus-large-noise-magnify"
class="magnify"
onclick="magnifyFigure(&#39;gp-sample-basis-functions-plus-large-noise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-sample-basis-functions-plus-large-noise-caption"
class="caption-frame">
<p>Figure: Samples directly from the covariance function implied by the
noise corrupted basis function based covariance, <span
class="math inline">\(\alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top +
\mathbf{I}\)</span>.</p>
</div>
</div>
<h2 id="non-degenerate-gaussian-processes">Non-degenerate Gaussian
Processes</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/non-degenerate-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/non-degenerate-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The process described above is degenerate. The covariance function is
of rank at most <span class="math inline">\(h\)</span> and since the
theoretical amount of data could always increase <span
class="math inline">\(n\rightarrow \infty\)</span>, the covariance
function is not full rank. This means as we increase the amount of data
to infinity, there will come a point where we can’t normalize the
process because the multivariate Gaussian has the form, <span
class="math display">\[
\mathcal{N}\left(\mathbf{ f}|\mathbf{0},\mathbf{K}\right) =
\frac{1}{\left(2\pi\right)^{\frac{n}{2}}\det{\mathbf{K}}^\frac{1}{2}}
\exp\left(-\frac{\mathbf{ f}^\top\mathbf{K}\mathbf{ f}}{2}\right)
\]</span> and a non-degenerate kernel matrix leads to <span
class="math inline">\(\det{\mathbf{K}} = 0\)</span> defeating the
normalization (it’s equivalent to finding a projection in the high
dimensional Gaussian where the variance of the the resulting univariate
Gaussian is zero, i.e. there is a null space on the covariance, or
alternatively you can imagine there are one or more directions where the
Gaussian has become the delta function).</p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Radford Neal
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/radford-neal.jpg" clip-path="url(#clip3)"/>
</svg>
</div>
<p>In the machine learning field, it was Radford Neal <span
class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> that
realized the potential of the next step. In his 1994 thesis, he was
considering Bayesian neural networks, of the type we described above,
and in considered what would happen if you took the number of hidden
nodes, or neurons, to infinity, i.e. <span
class="math inline">\(h\rightarrow \infty\)</span>.</p>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neal-infinite-priors-magnify" class="magnify"
onclick="magnifyFigure(&#39;neal-infinite-priors&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="neal-infinite-priors-caption" class="caption-frame">
<p>Figure: Page 37 of <a
href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s
1994 thesis</a></p>
</div>
</div>
<p>In loose terms, what Radford considers is what happens to the
elements of the covariance function, <span class="math display">\[
  \begin{align*}
  k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) &amp; = \alpha
\boldsymbol{ \phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top
\boldsymbol{ \phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)\\
  &amp; = \alpha \sum_k \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{
x}_i\right) \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{ x}_j\right)
  \end{align*}
  \]</span> if instead of considering a finite number you sample
infinitely many of these activation functions, sampling parameters from
a prior density, <span class="math inline">\(p(\mathbf{ v})\)</span>,
for each one, <span class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \int
\phi\left(\mathbf{ w}^{(1)}, \mathbf{ x}_i\right) \phi\left(\mathbf{
w}^{(1)}, \mathbf{ x}_j\right) p(\mathbf{ w}^{(1)}) \text{d}\mathbf{
w}^{(1)}
\]</span> And that’s not <em>only</em> for Gaussian <span
class="math inline">\(p(\mathbf{ v})\)</span>. In fact this result holds
for a range of activations, and a range of prior densities because of
the <em>central limit theorem</em>.</p>
<p>To write it in the form of a probabilistic program, as long as the
distribution for <span class="math inline">\(\phi_i\)</span> implied by
this short probabilistic program, <span class="math display">\[
  \begin{align*}
  \mathbf{ v}&amp; \sim p(\cdot)\\
  \phi_i &amp; = \phi\left(\mathbf{ v}, \mathbf{ x}_i\right),
  \end{align*}
  \]</span> has finite variance, then the result of taking the number of
hidden units to infinity, with appropriate scaling, is also a Gaussian
process.</p>
<h2 id="further-reading">Further Reading</h2>
<p>To understand this argument in more detail, I highly recommend
reading chapter 2 of Neal’s thesis <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span>, which remains easy to
read and clear today. Indeed, for readers interested in Bayesian neural
networks, both Raford Neal’s and David MacKay’s PhD thesis <span
class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span>
remain essential reading. Both theses embody a clarity of thought, and
an ability to weave together threads from different fields that was the
business of machine learning in the 1990s. Radford and David were also
pioneers in making their software widely available and publishing
material on the web.</p>
<h2 id="gaussian-process-1">Gaussian Process</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-function-space.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-function-space.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In our we sampled from the prior over paraemters. Through the
properties of multivariate Gaussian densities this prior over parameters
implies a particular density for our data observations, <span
class="math inline">\(\mathbf{ y}\)</span>. In this session we sampled
directly from this distribution for our data, avoiding the intermediate
weight-space representation. This is the approach taken by <em>Gaussian
processes</em>. In a Gaussian process you specify the <em>covariance
function</em> directly, rather than <em>implicitly</em> through a basis
matrix and a prior over parameters. Gaussian processes have the
advantage that they can be <em>nonparametric</em>, which in simple terms
means that they can have <em>infinite</em> basis functions. In the
lectures we introduced the <em>exponentiated quadratic</em> covariance,
also known as the RBF or the Gaussian or the squared exponential
covariance function. This covariance function is specified by <span
class="math display">\[
k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha \exp\left( -\frac{\left\Vert
\mathbf{ x}-\mathbf{ x}^\prime\right\Vert^2}{2\ell^2}\right),
\]</span> where <span class="math inline">\(\left\Vert\mathbf{ x}-
\mathbf{ x}^\prime\right\Vert^2\)</span> is the squared distance between
the two input vectors <span class="math display">\[
\left\Vert\mathbf{ x}- \mathbf{ x}^\prime\right\Vert^2 = (\mathbf{ x}-
\mathbf{ x}^\prime)^\top (\mathbf{ x}- \mathbf{ x}^\prime)
\]</span> Let’s build a covariance matrix based on this function. First
we define the form of the covariance function,</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> eq_cov</span></code></pre></div>
<p>We can use this to compute <em>directly</em> the covariance for <span
class="math inline">\(\mathbf{ f}\)</span> at the points given by
<code>x_pred</code>. Let’s define a new function <code>K()</code> which
does this,</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> Kernel</span></code></pre></div>
<p>Now we can image the resulting covariance,</p>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> Kernel(function<span class="op">=</span>eq_cov, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">10.</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> kernel.K(x_pred, x_pred)</span></code></pre></div>
<p>To visualise the covariance between the points we can use the
<code>imshow</code> function in matplotlib.</p>
<p>Finally, we can sample functions from the marginal likelihood.</p>
<h3 id="exercise-2">Exercise 2</h3>
<p><strong>Moving Parameters</strong> Have a play with the parameters
for this covariance function (the lengthscale and the variance) and see
what effects the parameters have on the types of functions you
observe.</p>
<h2 id="bayesian-inference-by-rejection-sampling">Bayesian Inference by
Rejection Sampling</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One view of Bayesian inference is to assume we are given a mechanism
for generating samples, where we assume that mechanism is representing
an accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world
by discarding all those prior samples that are inconsistent with our
observations. The <em>likelihood</em> defines mathematically what we
mean by inconsistent with the observations. The higher the noise level
in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two
sampling techniques known as <em>rejection sampling</em> and
<em>importance sampling</em>. It is realized in practice in an approach
known as <em>approximate Bayesian computation</em> (ABC) or
likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because
most samples will be inconsistent with the observations and as a result
the mechanism must be operated many times to obtain a few posterior
samples.</p>
<p>However, in the Gaussian process case, when the likelihood also
assumes Gaussian noise, we can operate this mechanism mathematically,
and obtain the posterior density <em>analytically</em>. This is the
benefit of Gaussian processes.</p>
<p>First, we will load in two python functions for computing the
covariance function.</p>
<p>Next, we sample from a multivariate normal density (a multivariate
Gaussian), using the covariance function as the covariance matrix.</p>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for
generating samples (the <em>prior</em>), and we discard all samples
inconsistent with our data, leaving the samples of interest (the
<em>posterior</em>). This is a rejection sampling view of Bayesian
inference. The Gaussian process allows us to do this analytically by
multiplying the <em>prior</em> by the <em>likelihood</em>.</p>
</div>
</div>
<h2 id="gaussian-process-2">Gaussian Process</h2>
<p>The Gaussian process perspective takes the marginal likelihood of the
data to be a joint Gaussian density with a covariance given by <span
class="math inline">\(\mathbf{K}\)</span>. So the model likelihood is of
the form, <span class="math display">\[
p(\mathbf{ y}|\mathbf{X}) =
\frac{1}{(2\pi)^{\frac{n}{2}}|\mathbf{K}|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\mathbf{ y}^\top \left(\mathbf{K}+\sigma^2
\mathbf{I}\right)^{-1}\mathbf{ y}\right)
\]</span> where the input data, <span
class="math inline">\(\mathbf{X}\)</span>, influences the density
through the covariance matrix, <span
class="math inline">\(\mathbf{K}\)</span> whose elements are computed
through the covariance function, <span class="math inline">\(k(\mathbf{
x}, \mathbf{ x}^\prime)\)</span>.</p>
<p>This means that the negative log likelihood (the objective function)
is given by, <span class="math display">\[
E(\boldsymbol{\theta}) = \frac{1}{2} \log |\mathbf{K}|
+ \frac{1}{2} \mathbf{ y}^\top \left(\mathbf{K}+
\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}
\]</span> where the <em>parameters</em> of the model are also embedded
in the covariance function, they include the parameters of the kernel
(such as lengthscale and variance), and the noise variance, <span
class="math inline">\(\sigma^2\)</span>. Let’s create a set of classes
in python for storing these variables.</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> Model</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> MapModel</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> ProbModel</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> ProbMapModel</span></code></pre></div>
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> GP</span></code></pre></div>
<h2 id="making-predictions">Making Predictions</h2>
<p>We now have a probability density that represents functions. How do
we make predictions with this density? The density is known as a process
because it is <em>consistent</em>. By consistency, here, we mean that
the model makes predictions for <span class="math inline">\(\mathbf{
f}\)</span> that are unaffected by future values of <span
class="math inline">\(\mathbf{ f}^*\)</span> that are currently
unobserved (such as test points). If we think of <span
class="math inline">\(\mathbf{ f}^*\)</span> as test points, we can
still write down a joint probability density over the training
observations, <span class="math inline">\(\mathbf{ f}\)</span> and the
test observations, <span class="math inline">\(\mathbf{ f}^*\)</span>.
This joint probability density will be Gaussian, with a covariance
matrix given by our covariance function, <span
class="math inline">\(k(\mathbf{ x}_i, \mathbf{ x}_j)\)</span>. <span
class="math display">\[
\begin{bmatrix}\mathbf{ f}\\ \mathbf{ f}^*\end{bmatrix} \sim
\mathcal{N}\left(\mathbf{0},\begin{bmatrix} \mathbf{K}&amp;
\mathbf{K}_\ast \\
\mathbf{K}_\ast^\top &amp; \mathbf{K}_{\ast,\ast}\end{bmatrix}\right)
\]</span> where here <span class="math inline">\(\mathbf{K}\)</span> is
the covariance computed between all the training points, <span
class="math inline">\(\mathbf{K}_\ast\)</span> is the covariance matrix
computed between the training points and the test points and <span
class="math inline">\(\mathbf{K}_{\ast,\ast}\)</span> is the covariance
matrix computed betwen all the tests points and themselves. To be clear,
let’s compute these now for our example, using <code>x</code> and
<code>y</code> for the training data (although <code>y</code> doesn’t
enter the covariance) and <code>x_pred</code> as the test locations.</p>
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set covariance function parameters</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>variance <span class="op">=</span> <span class="fl">16.0</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>lengthscale <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set noise variance</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> Kernel(eq_cov, variance<span class="op">=</span>variance, lengthscale<span class="op">=</span>lengthscale)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> kernel.K(x, x)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>K_star <span class="op">=</span> kernel.K(x, x_pred)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>K_starstar <span class="op">=</span> kernel.K(x_pred, x_pred)</span></code></pre></div>
<p>Now we use this structure to visualise the covariance between test
data and training data. This structure is how information is passed
between test and training data. Unlike the maximum likelihood formalisms
we’ve been considering so far, the structure expresses
<em>correlation</em> between our different data points. However, just
like the we now have a <em>joint density</em> between some variables of
interest. In particular we have the joint density over <span
class="math inline">\(p(\mathbf{ f}, \mathbf{ f}^*)\)</span>. The joint
density is <em>Gaussian</em> and <em>zero mean</em>. It is specified
entirely by the <em>covariance matrix</em>, <span
class="math inline">\(\mathbf{K}\)</span>. That covariance matrix is, in
turn, defined by a covariance function. Now we will visualise the form
of that covariance in the form of the matrix, <span
class="math display">\[
\begin{bmatrix} \mathbf{K}&amp; \mathbf{K}_\ast \\ \mathbf{K}_\ast^\top
&amp; \mathbf{K}_{\ast,\ast}\end{bmatrix}
\]</span></p>
<div class="figure">
<div id="block-predictive-covariance-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/block-predictive-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="block-predictive-covariance-magnify" class="magnify"
onclick="magnifyFigure(&#39;block-predictive-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="block-predictive-covariance-caption" class="caption-frame">
<p>Figure: Different blocks of the covariance function. The upper left
block is the covariance of the training data with itself, <span
class="math inline">\(\mathbf{K}\)</span>. The top right is the cross
covariance between training data (rows) and prediction locations
(columns). The lower left is the same matrix transposed. The bottom
right is the covariance matrix of the test data with itself.</p>
</div>
</div>
<p>There are four blocks to this plot. The upper left block is the
covariance of the training data with itself, <span
class="math inline">\(\mathbf{K}\)</span>. We see some structure here
due to the missing data from the first and second world wars. Alongside
this covariance (to the right and below) we see the cross covariance
between the training and the test data (<span
class="math inline">\(\mathbf{K}_*\)</span> and <span
class="math inline">\(\mathbf{K}_*^\top\)</span>). This is giving us the
covariation between our training and our test data. Finally the lower
right block The banded structure we now observe is because some of the
training points are near to some of the test points. This is how we
obtain ‘communication’ between our training data and our test data. If
there is no structure in <span
class="math inline">\(\mathbf{K}_*\)</span> then our belief about the
test data simply matches our prior.</p>
<h2 id="prediction-across-two-points-with-gps">Prediction Across Two
Points with GPs</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gptwopointpred.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gptwopointpred.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4949</span>)</span></code></pre></div>
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai.plot <span class="im">as</span> plot</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<h2 id="sampling-a-function">Sampling a Function</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpdistfunc.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpdistfunc.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We will consider a Gaussian distribution with a particular structure
of covariance matrix. We will generate <em>one</em> sample from a
25-dimensional Gaussian density. <span class="math display">\[
\mathbf{ f}=\left[f_{1},f_{2}\dots f_{25}\right].
\]</span> in the figure below we plot these data on the <span
class="math inline">\(y\)</span>-axis against their <em>indices</em> on
the <span class="math inline">\(x\)</span>-axis.</p>
<div class="sourceCode" id="cb60"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> Kernel</span></code></pre></div>
<div class="sourceCode" id="cb62"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> polynomial_cov</span></code></pre></div>
<div class="sourceCode" id="cb64"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> exponentiated_quadratic</span></code></pre></div>
<div class="figure">
<div id="gp-two-point-sample-1-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/two_point_sample008.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-two-point-sample-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-two-point-sample-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-two-point-sample-1-caption" class="caption-frame">
<p>Figure: A 25 dimensional correlated random variable (values ploted
against index)</p>
</div>
</div>
<h3 id="sampling-a-function-from-a-gaussian">Sampling a Function from a
Gaussian</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-two.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-two.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/two_point_sample001.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span> along with the conditional
distribution of <span class="math inline">\(f_2\)</span> given <span
class="math inline">\(f_1\)</span></p>
</div>
</div>
<h3 id="joint-density-of-f_1-and-f_2">Joint Density of <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span></h3>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span> along with the conditional
distribution of <span class="math inline">\(f_2\)</span> given <span
class="math inline">\(f_1\)</span></p>
</div>
</div>
<h2 id="uluru">Uluru</h2>
<div class="figure">
<div id="uluru-as-probability-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uluru-as-probability-magnify" class="magnify"
onclick="magnifyFigure(&#39;uluru-as-probability&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uluru-as-probability-caption" class="caption-frame">
<p>Figure: Uluru, the sacred rock in Australia. If we think of it as a
probability density, viewing it from this side gives us one
<em>marginal</em> from the density. Figuratively speaking, slicing
through the rock would give a conditional density.</p>
</div>
</div>
<p>When viewing these contour plots, I sometimes find it helpful to
think of Uluru, the prominent rock formation in Australia. The rock
rises above the surface of the plane, just like a probability density
rising above the zero line. The rock is three dimensional, but when we
view Uluru from the classical position, we are looking at one side of
it. This is equivalent to viewing the marginal density.</p>
<p>The joint density can be viewed from above, using contours. The
conditional density is equivalent to <em>slicing</em> the rock. Uluru is
a holy rock, so this has to be an imaginary slice. Imagine we cut down a
vertical plane orthogonal to our view point (e.g. coming across our view
point). This would give a profile of the rock, which when renormalized,
would give us the conditional distribution, the value of conditioning
would be the location of the slice in the direction we are facing.</p>
<h2 id="prediction-with-correlated-gaussians">Prediction with Correlated
Gaussians</h2>
<p>Of course in practice, rather than manipulating mountains physically,
the advantage of the Gaussian density is that we can perform these
manipulations mathematically.</p>
<p>Prediction of <span class="math inline">\(f_2\)</span> given <span
class="math inline">\(f_1\)</span> requires the <em>conditional
density</em>, <span class="math inline">\(p(f_2|f_1)\)</span>.Another
remarkable property of the Gaussian density is that this conditional
distribution is <em>also</em> guaranteed to be a Gaussian density. It
has the form, <span class="math display">\[
p(f_2|f_1) = \mathcal{N}\left(f_2|\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2}
- \frac{k_{1,2}^2}{k_{1,1}}\right)
\]</span>where we have assumed that the covariance of the original joint
density was given by <span class="math display">\[
\mathbf{K}= \begin{bmatrix} k_{1, 1} &amp; k_{1, 2}\\ k_{2, 1} &amp;
k_{2, 2}.\end{bmatrix}
\]</span></p>
<p>Using these formulae we can determine the conditional density for any
of the elements of our vector <span class="math inline">\(\mathbf{
f}\)</span>. For example, the variable <span
class="math inline">\(f_8\)</span> is less correlated with <span
class="math inline">\(f_1\)</span> than <span
class="math inline">\(f_2\)</span>. If we consider this variable we see
the conditional density is more diffuse.</p>
<h3 id="joint-density-of-f_1-and-f_8">Joint Density of <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_8\)</span></h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-eight.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-eight.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="two-point-sample-13-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-13-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-13&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-13-caption" class="caption-frame">
<p>Figure: Sample from the joint Gaussian model, points indexed by 1 and
8 highlighted.</p>
</div>
</div>
<h3 id="prediction-of-f_8-from-f_1">Prediction of <span
class="math inline">\(f_{8}\)</span> from <span
class="math inline">\(f_{1}\)</span></h3>
<div class="figure">
<div id="two-point-sample-one-eight-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-eight-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-one-eight&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-eight-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_8\)</span> along with the conditional
distribution of <span class="math inline">\(f_8\)</span> given <span
class="math inline">\(f_1\)</span></p>
</div>
</div>
<ul>
<li>The single contour of the Gaussian density represents the
<font color="blue">joint distribution, <span
class="math inline">\(p(f_1, f_8)\)</span></font></li>
</ul>
<p>. . .</p>
<ul>
<li>We observe a value for <font color="green"><span
class="math inline">\(f_1=-?\)</span></font></li>
</ul>
<p>. . .</p>
<ul>
<li><p>Conditional density: <font color="red"><span
class="math inline">\(p(f_8|f_1=?)\)</span></font>.</p></li>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{ f},\mathbf{
f}}^{-1}\mathbf{ f},\mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{
f},*}\right)}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\boldsymbol{ \mu},\boldsymbol{ \Sigma}\right)}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}= \mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{
f},\mathbf{ f}}^{-1}\mathbf{ f}
\]</span> <span class="math display">\[
\boldsymbol{ \Sigma}= \mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{ f},*}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
<h2 id="the-importance-of-the-covariance-function">The Importance of the
Covariance Function</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-covariance-function-importance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-covariance-function-importance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The covariance function encapsulates our assumptions about the data.
The equations for the distribution of the prediction function, given the
training observations, are highly sensitive to the covariation between
the test locations and the training locations as expressed by the matrix
<span class="math inline">\(\mathbf{K}_*\)</span>. We defined a matrix
<span class="math inline">\(\mathbf{A}\)</span> which allowed us to
express our conditional mean in the form, <span class="math display">\[
\boldsymbol{ \mu}_f= \mathbf{A}^\top \mathbf{ y},
\]</span> where <span class="math inline">\(\mathbf{ y}\)</span> were
our <em>training observations</em>. In other words our mean predictions
are always a linear weighted combination of our <em>training data</em>.
The weights are given by computing the covariation between the training
and the test data (<span class="math inline">\(\mathbf{K}_*\)</span>)
and scaling it by the inverse covariance of the training data
observations, <span class="math inline">\(\left[\mathbf{K}+ \sigma^2
\mathbf{I}\right]^{-1}\)</span>. This inverse is the main computational
object that needs to be resolved for a Gaussian process. It has a
computational burden which is <span
class="math inline">\(O(n^3)\)</span> and a storage burden which is
<span class="math inline">\(O(n^2)\)</span>. This makes working with
Gaussian processes computationally intensive for the situation where
<span class="math inline">\(n&gt;10,000\)</span>.</p>
<div class="figure">
<div id="intro-to-gps-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-gps-magnify" class="magnify"
onclick="magnifyFigure(&#39;intro-to-gps&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-gps-caption" class="caption-frame">
<p>Figure: Introduction to Gaussian processes given by Neil Lawrence at
the 2014 Gaussian process Winter School at the University of
Sheffield.</p>
</div>
</div>
<h2 id="improving-the-numerics">Improving the Numerics</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-numerics-and-optimization.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-numerics-and-optimization.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In practice we shouldn’t be using matrix inverse directly to solve
the GP system. One more stable way is to compute the <em>Cholesky
decomposition</em> of the kernel matrix. The log determinant of the
covariance can also be derived from the Cholesky decomposition.</p>
<div class="sourceCode" id="cb66"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> update_inverse</span></code></pre></div>
<div class="sourceCode" id="cb68"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>GP.update_inverse <span class="op">=</span> update_inverse</span></code></pre></div>
<h2 id="capacity-control">Capacity Control</h2>
<p>Gaussian processes are sometimes seen as part of a wider family of
methods known as kernel methods. Kernel methods are also based around
covariance functions, but in the field they are known as Mercer kernels.
Mercer kernels have interpretations as inner products in potentially
infinite dimensional Hilbert spaces. This interpretation arises because,
if we take <span class="math inline">\(\alpha=1\)</span>, then the
kernel can be expressed as <span class="math display">\[
\mathbf{K}= \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top
\]</span> which imples the elements of the kernel are given by, <span
class="math display">\[
k(\mathbf{ x}, \mathbf{ x}^\prime) = \boldsymbol{ \phi}(\mathbf{
x})^\top \boldsymbol{ \phi}(\mathbf{ x}^\prime).
\]</span> So we see that the kernel function is developed from an inner
product between the basis functions. Mercer’s theorem tells us that any
valid <em>positive definite function</em> can be expressed as this inner
product but with the caveat that the inner product could be <em>infinite
length</em>. This idea has been used quite widely to <em>kernelize</em>
algorithms that depend on inner products. The kernel functions are
equivalent to covariance functions and they are parameterized
accordingly. In the kernel modeling community it is generally accepted
that kernel parameter estimation is a difficult problem and the normal
solution is to cross validate to obtain parameters. This can cause
difficulties when a large number of kernel parameters need to be
estimated. In Gaussian process modelling kernel parameter estimation (in
the simplest case proceeds) by maximum likelihood. This involves taking
gradients of the likelihood with respect to the parameters of the
covariance function.</p>
<h2 id="gradients-of-the-likelihood">Gradients of the Likelihood</h2>
<p>The easiest conceptual way to obtain the gradients is a two step
process. The first step involves taking the gradient of the likelihood
with respect to the covariance function, the second step involves
considering the gradient of the covariance function with respect to its
parameters.</p>
<h2 id="overall-process-scale">Overall Process Scale</h2>
<p>In general we won’t be able to find parameters of the covariance
function through fixed point equations, we will need to do gradient
based optimization.</p>
<h2 id="capacity-control-and-data-fit">Capacity Control and Data
Fit</h2>
<p>The objective function can be decomposed into two terms, a capacity
control term, and a data fit term. The capacity control term is the log
determinant of the covariance. The data fit term is the matrix inner
product between the data and the inverse covariance.</p>
<h2 id="learning-covariance-parameters">Learning Covariance
Parameters</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Can we determine covariance parameters from the data?</p>
<p><span class="math display">\[
\mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}{\det{\mathbf{K}}^{\frac{1}{2}}}}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}\color{blue}{\det{\mathbf{K}}^{\frac{1}{2}}}}\color{red}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
    \log \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=&amp;\color{blue}{-\frac{1}{2}\log\det{\mathbf{K}}}\color{red}{-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}} \\ &amp;-\frac{n}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
E(\boldsymbol{ \theta}) = \color{blue}{\frac{1}{2}\log\det{\mathbf{K}}}
+ \color{red}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}
\]</span></p>
<h2 id="capacity-control-through-the-determinant">Capacity Control
through the Determinant</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The parameters are <em>inside</em> the covariance function (matrix).
<span class="math display">\[k_{i, j} = k(\mathbf{ x}_i, \mathbf{ x}_j;
\boldsymbol{ \theta})\]</span></p>
<p><span> <span class="math display">\[\mathbf{K}=
\mathbf{R}\boldsymbol{ \Lambda}^2 \mathbf{R}^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\boldsymbol{ \Lambda}\)</span> represents
distance on axes. <span class="math inline">\(\mathbf{R}\)</span> gives
rotation.
</td>
</tr>
</table>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Lambda}\)</span> is
<em>diagonal</em>, <span
class="math inline">\(\mathbf{R}^\top\mathbf{R}=
\mathbf{I}\)</span>.</li>
<li>Useful representation since <span
class="math inline">\(\det{\mathbf{K}} = \det{\boldsymbol{ \Lambda}^2} =
\det{\boldsymbol{ \Lambda}}^2\)</span>.</li>
</ul>
<div class="figure">
<div id="gp-optimise-determinant-figure-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-determinant-figure-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-optimise-determinant-figure&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-determinant-figure-caption" class="caption-frame">
<p>Figure: The determinant of the covariance is dependent only on the
eigenvalues. It represents the ‘footprint’ of the Gaussian.</p>
</div>
</div>
<h2 id="quadratic-data-fit">Quadratic Data Fit</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-quadratic-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-optimise-quadratic&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-quadratic-caption" class="caption-frame">
<p>Figure: The data fit term of the Gaussian process is a quadratic loss
centered around zero. This has eliptical contours, the principal axes of
which are given by the covariance matrix.</p>
</div>
</div>
<h2 id="data-fit-term">Data Fit Term</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="gp-optimise-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimise006.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimise010.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimise016.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gp-optimise021.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="gp-optimise-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-optimise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-caption" class="caption-frame">
<p>Figure: Variation in the data fit term, the capacity term and the
negative log likelihood for different lengthscales.</p>
</div>
</div>
<h2 id="exponentiated-quadratic-covariance">Exponentiated Quadratic
Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/eq-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/eq-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The exponentiated quadratic covariance, also known as the Gaussian
covariance or the RBF covariance and the squared exponential. Covariance
between two points is related to the negative exponential of the squared
distnace between those points. This covariance function can be derived
in a few different ways: as the infinite limit of a radial basis
function neural network, as diffusion in the heat equation, as a
Gaussian filter in <em>Fourier space</em> or as the composition as a
series of linear filters applied to a base function.</p>
<p>The covariance takes the following form, <span
class="math display">\[
k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha \exp\left(-\frac{\left\Vert
\mathbf{ x}-\mathbf{ x}^\prime \right\Vert_2^2}{2\ell^2}\right)
\]</span> where <span class="math inline">\(\ell\)</span> is the
<em>length scale</em> or <em>time scale</em> of the process and <span
class="math inline">\(\alpha\)</span> represents the overall process
variance.</p>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\exp\left(-\frac{\left\Vert \mathbf{ x}-\mathbf{ x}^\prime
\right\Vert_2^2}{2\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eq-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;eq-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eq-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function.</p>
</div>
</div>
<h2 id="gpss-gaussian-process-summer-school">GPSS: Gaussian Process
Summer School</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-summer-school.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-summer-school.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div style="width:1.5cm;text-align:center">

</div>
<p>If you’re interested in finding out more about Gaussian processes,
you can attend the Gaussian process summer school, or view the lectures
and material on line. Details of the school, future events and past
events can be found at the website <a href="http://gpss.cc"
class="uri">http://gpss.cc</a>.</p>
<div class="sourceCode" id="cb69"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian
Process Framework in Python</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Gaussian processes are a flexible tool for non-parametric analysis
with uncertainty. The GPy software was started in Sheffield to provide a
easy to use interface to GPs. One which allowed the user to focus on the
modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing
Gaussian process models in Python. It is designed for teaching and
modelling. We welcome contributions which can be made through the GitHub
repository <a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian
process models in python. This allows GPs to be combined with a wide
variety of software libraries.</p>
<p>The software itself is available on <a
href="https://github.com/SheffieldML/GPy">GitHub</a> and the team
welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language,
i.e., you specify the model rather than the algorithm. As well as a
large range of covariance functions the software allows for non-Gaussian
likelihoods, multivariate outputs, dimensionality reduction and
approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a
href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<h2 id="gpy-tutorial">GPy Tutorial</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpy-tutorial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpy-tutorial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
James Hensman
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/james-hensman.png" clip-path="url(#clip4)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Nicolas Durrande
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/nicolas-durrande2.jpg" clip-path="url(#clip5)"/>
</svg>
</div>
<p>This GPy tutorial is based on material we share in the Gaussian
process summer school for teaching these models <a
href="https://gpss.cc" class="uri">https://gpss.cc</a>. It contains
material from various members and former members of the Sheffield
machine learning group, but particular mention should be made of <a
href="https://sites.google.com/site/nicolasdurrandehomepage/">Nicolas
Durrande</a> and <a href="https://jameshensman.github.io/">James
Hensman</a>, see <a
href="http://gpss.cc/gpss17/labs/GPSS_Lab1_2017.ipynb"
class="uri">http://gpss.cc/gpss17/labs/GPSS_Lab1_2017.ipynb</a>.</p>
<div class="sourceCode" id="cb70"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<p>To give a feel for the software we’ll start by creating an
exponentiated quadratic covariance function, <span
class="math display">\[
k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha \exp\left(-\frac{\left\Vert
\mathbf{ x}- \mathbf{ x}^\prime \right\Vert_2^2}{2\ell^2}\right),
\]</span> where the length scale is <span
class="math inline">\(\ell\)</span> and the variance is <span
class="math inline">\(\alpha\)</span>.</p>
<p>To set this up in GPy we create a kernel in the following manner.</p>
<div class="sourceCode" id="cb71"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>input_dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>lengthscale <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span>input_dim, variance<span class="op">=</span>alpha, lengthscale<span class="op">=</span>lengthscale)</span></code></pre></div>
<p>That builds a kernel object for us. The kernel can be displayed.</p>
<div class="sourceCode" id="cb72"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>display(kern)</span></code></pre></div>
<p>Or because it’s one dimensional, you can also plot the kernel as a
function of its inputs (while the other is fixed).</p>
<div class="figure">
<div id="gpy-eq-covariance-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gpy-eq-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-covariance-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-eq-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-covariance-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function as plotted by
the <code>GPy.kern.plot</code> command.</p>
</div>
</div>
<p>You can set the length scale of the covariance to different values
and plot the result.</p>
<div class="sourceCode" id="cb73"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span>input_dim)     <span class="co"># By default, the parameters are set to 1.</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>lengthscales <span class="op">=</span> np.asarray([<span class="fl">0.2</span>,<span class="fl">0.5</span>,<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">4.</span>])</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-covariance-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gpy-eq-covariance-lengthscales.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-covariance-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-eq-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-covariance-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function plotted for
different length scales by <code>GPy.kern.plot</code> command.</p>
</div>
</div>
<h2 id="covariance-functions-in-gpy">Covariance Functions in GPy</h2>
<p>Many covariance functions are already implemented in GPy. Instead of
rbf, try constructing and plotting the following covariance functions:
<code>exponential</code>, <code>Matern32</code>, <code>Matern52</code>,
<code>Brownian</code>, <code>linear</code>, <code>bias</code>,
<code>rbfcos</code>, <code>periodic_Matern32</code>, etc. Some of these
covariance functions, such as <code>rbfcos</code>, are not parametrized
by a variance and a length scale. Further, not all kernels are
stationary (i.e., they can’t all be written as <span
class="math inline">\(k(\mathbf{ x}, \mathbf{ x}^\prime) = f(\mathbf{
x}-\mathbf{ x}^\prime)\)</span>, see for example the Brownian covariance
function). So for plotting it may be interesting to change the value of
the fixed input.</p>
<h2 id="combining-covariance-functions-in-gpy">Combining Covariance
Functions in GPy</h2>
<p>In GPy you can easily combine covariance functions you have created
using the sum and product operators, <code>+</code> and <code>*</code>.
So, for example, if we wish to combine an exponentiated quadratic
covariance with a Matern 5/2 then we can write</p>
<div class="sourceCode" id="cb74"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>kern2 <span class="op">=</span> GPy.kern.Matern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">2.</span>, lengthscale<span class="op">=</span><span class="fl">4.</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> kern1 <span class="op">+</span> kern2</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>display(kern)</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-plus-matern52-covariance-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gpy-eq-plus-matern52-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-plus-matern52-covariance-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-eq-plus-matern52-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-plus-matern52-covariance-caption" class="caption-frame">
<p>Figure: A combination of the exponentiated quadratic covariance plus
the Matern <span class="math inline">\(5/2\)</span> covariance.</p>
</div>
</div>
<p>Or if we wanted to multiply them, we can write</p>
<div class="sourceCode" id="cb75"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>kern2 <span class="op">=</span> GPy.kern.Matern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">2.</span>, lengthscale<span class="op">=</span><span class="fl">4.</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> kern1 <span class="op">*</span> kern2</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>display(kern)</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-times-matern52-covariance-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//kern/gpy-eq-times-matern52-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-times-matern52-covariance-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-eq-times-matern52-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-times-matern52-covariance-caption"
class="caption-frame">
<p>Figure: A combination of the exponentiated quadratic covariance
multiplied by the Matern <span class="math inline">\(5/2\)</span>
covariance.</p>
</div>
</div>
<p>You can learn about how to implement <a
href="https://gpy.readthedocs.io/en/latest/tuto_creating_new_kernels.html">new
kernel objects in GPy here</a>.</p>
<div class="figure">
<div id="nicolas-durrande-on-kernel-design-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/-sY8zW3Om1Y?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="nicolas-durrande-on-kernel-design-magnify" class="magnify"
onclick="magnifyFigure(&#39;nicolas-durrande-on-kernel-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nicolas-durrande-on-kernel-design-caption"
class="caption-frame">
<p>Figure: Designing the covariance function for your Gaussian process
is a key place in which you introduce your understanding of the data
problem. To learn more about the design of covariance functions, see
this talk from Nicolas Durrande at GPSS in 2016.</p>
</div>
</div>
<h2 id="a-gaussian-process-regression-model">A Gaussian Process
Regression Model</h2>
<p>We will now combine the Gaussian process prior with some data to form
a GP regression model with GPy. We will generate data from the function
<span class="math display">\[
f( x) = − \cos(\pi x) + \sin(4\pi x)
\]</span> over the domain <span class="math inline">\([0, 1]\)</span>,
adding some noise to gives <span class="math display">\[
y(x) = f(x) + \epsilon,
\]</span> with the noise being Gaussian distributed, <span
class="math inline">\(\epsilon\sim
\mathcal{N}\left(0,0.01\right)\)</span>.</p>
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="fl">0.05</span>,<span class="fl">0.95</span>,<span class="dv">10</span>)[:,np.newaxis]</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> <span class="op">-</span>np.cos(np.pi<span class="op">*</span>X) <span class="op">+</span> np.sin(<span class="dv">4</span><span class="op">*</span>np.pi<span class="op">*</span>X) <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">1</span>))</span></code></pre></div>
<div class="figure">
<div id="noisy-sine-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/noisy-sine.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-magnify" class="magnify"
onclick="magnifyFigure(&#39;noisy-sine&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-caption" class="caption-frame">
<p>Figure: Data from the noisy sine wave for fitting with a GPy
model.</p>
</div>
</div>
<p>A GP regression model based on an exponentiated quadratic covariance
function can be defined by first defining a covariance function.</p>
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span><span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">1.</span>)</span></code></pre></div>
<p>And then combining it with the data to form a Gaussian process
model.</p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X,Y,kern)</span></code></pre></div>
<p>Just as for the covariance function object, we can find out about the
model using the command <code>display(model)</code>.</p>
<div class="sourceCode" id="cb79"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>display(model)</span></code></pre></div>
<p>Note that by default the model includes some observation noise with
variance 1. We can see the posterior mean prediction and visualize the
marginal posterior variances using <code>model.plot()</code>.</p>
<div class="figure">
<div id="noisy-sine-gp-fit-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/noisy-sine-gp-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-gp-fit-magnify" class="magnify"
onclick="magnifyFigure(&#39;noisy-sine-gp-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-gp-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the noisy sine data. Here the
parameters of the process and the covariance function haven’t yet been
optimized.</p>
</div>
</div>
<p>You can also look directly at the predictions for the model
using.</p>
<div class="sourceCode" id="cb80"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>Xstar <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)[:, np.newaxis]</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>Ystar, Vstar <span class="op">=</span> model.predict(Xstar)</span></code></pre></div>
<p>Which gives you the mean (<code>Ystar</code>), the variance
(<code>Vstar</code>) at the locations given by <code>Xstar</code>.</p>
<h2 id="covariance-function-parameter-estimation">Covariance Function
Parameter Estimation</h2>
<p>As we have seen during the lectures, the parameters values can be
estimated by maximizing the likelihood of the observations. Since we
don’t want any of the variances to become negative during the
optimization, we can constrain all parameters to be positive before
running the optimization.</p>
<div class="sourceCode" id="cb81"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>model.constrain_positive()</span></code></pre></div>
<p>The warnings are because the parameters are already constrained by
default, the software is warning us that they are being
reconstrained.</p>
<p>Now we can optimize the model using the <code>model.optimize()</code>
method. Here we switch messages on, which allows us to see the
progression of the optimization.</p>
<div class="sourceCode" id="cb82"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>By default, the optimization is using a limited memory BFGS optimizer
<span class="citation" data-cites="Byrd:lbfgsb95">(Byrd et al.,
1995)</span>.</p>
<p>Once again, we can display the model, now to see how the parameters
have changed.</p>
<div class="sourceCode" id="cb83"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>display(model)</span></code></pre></div>
<p>The length scale is much smaller, as well as the noise level. The
variance of the exponentiated quadratic has also reduced.</p>
<div class="figure">
<div id="noisy-sine-gp-optimized-fit-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//gp/noisy-sine-gp-optimized-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-gp-optimized-fit-magnify" class="magnify"
onclick="magnifyFigure(&#39;noisy-sine-gp-optimized-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-gp-optimized-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the noisy sine data with parameters
optimized.</p>
</div>
</div>
<h2 id="review">Review</h2>
<h2 id="other-software">Other Software</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/other-gp-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/other-gp-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>GPy has inspired other software solutions, first of all <a
href="https://github.com/GPflow/GPflow">GPflow</a>, which uses Tensor
Flow’s automatic differentiation engine to allow rapid prototyping of
new covariance functions and algorithms. More recently, <a
href="https://github.com/cornellius-gp/gpytorch">GPyTorch</a> uses
PyTorch for the same purpose.</p>
<p>The Probabilistic programming language <a
href="https://pyro.ai/">pyro</a> also has GP support.</p>
<h2 id="further-reading-1">Further Reading</h2>
<ul>
<li><p>Chapter 2 of <span class="citation"
data-cites="Neal:bayesian94">Neal (1994)</span></p></li>
<li><p>Rest of <span class="citation" data-cites="Neal:bayesian94">Neal
(1994)</span></p></li>
<li><p>All of <span class="citation"
data-cites="MacKay:bayesian92">MacKay (1992)</span></p></li>
</ul>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Byrd:lbfgsb95" class="csl-entry" role="listitem">
Byrd, R.H., Lu, P., Nocedal, J., 1995. A limited memory algorithm for
bound constrained optimization. SIAM Journal on Scientific and
Statistical Computing 16, 1190–1208.
</div>
<div id="ref-Cho:deep09" class="csl-entry" role="listitem">
Cho, Y., Saul, L.K., 2009. <a
href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel
methods for deep learning</a>, in: Bengio, Y., Schuurmans, D., Lafferty,
J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural
Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.
</div>
<div id="ref-Gething:hmis06" class="csl-entry" role="listitem">
Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,
Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data
from health management information systems in <span>A</span>frica using
space–time geostatistics. PLoS Medicine 3. <a
href="https://doi.org/10.1371/journal.pmed.0030271">https://doi.org/10.1371/journal.pmed.0030271</a>
</div>
<div id="ref-Ioffe:batch15" class="csl-entry" role="listitem">
Ioffe, S., Szegedy, C., 2015. <a
href="http://proceedings.mlr.press/v37/ioffe15.html">Batch
normalization: Accelerating deep network training by reducing internal
covariate shift</a>, in: Bach, F., Blei, D. (Eds.), Proceedings of the
32nd International Conference on Machine Learning, Proceedings of
Machine Learning Research. PMLR, Lille, France, pp. 448–456.
</div>
<div id="ref-Laplace:essai14" class="csl-entry" role="listitem">
Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.
Courcier, Paris.
</div>
<div id="ref-MacKay:bayesian92" class="csl-entry" role="listitem">
MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).
California Institute of Technology.
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Neal:bayesian94" class="csl-entry" role="listitem">
Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).
Dept. of Computer Science, University of Toronto.
</div>
<div id="ref-Rasmussen:book06" class="csl-entry" role="listitem">
Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine
learning. mit, Cambridge, MA.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="listitem">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC
Press.
</div>
<div id="ref-Tipping:probpca99" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component
analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a
href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>
</div>
</div>

