---
title: "Gaussian Processes"
venue: "Virtual Data Science Nigeria"
abstract: "<p>Classical machine learning and statistical approaches to learning, such as neural networks and linear regression, assume a parametric form for functions. Gaussian process models are an alternative approach that assumes a probabilistic prior over functions. This brings benefits, in that uncertainty of function estimation is sustained throughout inference, and some challenges: algorithms for fitting Gaussian processes tend to be more complex than parametric models. In this sessions I will introduce Gaussian processes and explain why sustaining uncertainty is important.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
date: 2020-11-13
published: 2020-11-13
time: "15:00 (West Africa Standard Time)"
week: 0
session: 4
reveal: 04-gaussian-processes.slides.html
ipynb: 04-gaussian-processes.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="setup">Setup</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_mlai/includes/mlai-notebook-setup.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_mlai/includes/mlai-notebook-setup.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>First we download some libraries and files to support the notebook.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<h2 id="pods">pods</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In Sheffield we created a suite of software tools for ‘Open Data Science’. Open data science is an approach to sharing code, models and data that should make it easier for companies, health professionals and scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>sods<span class="op">/</span>ods</span></code></pre></div>
<p>from the command prompt where you can access your python installation.</p>
<p>The code is also available on github: <a href="https://github.com/sods/ods" class="uri">https://github.com/sods/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual manner.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="figure">
<div id="gaussian-processes-for-machine-learning-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gp/rasmussen-williams-book.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gaussian-processes-for-machine-learning-magnify" class="magnify" onclick="magnifyFigure(&#39;gaussian-processes-for-machine-learning&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-processes-for-machine-learning-caption" class="caption-frame">
<p>Figure: A key reference for Gaussian process models remains the excellent book “Gaussian Processes for Machine Learning” (<span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span>). The book is also <a href="http://www.gaussianprocess.org/gpml/" target="_blank" >freely available online</a>.</p>
</div>
</div>
<p><span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span> is still one of the most important references on Gaussian process models. It is <a href="http://www.gaussianprocess.org/gpml/">available freely online</a>.</p>
<!--include{_gp/includes/what-is-a-gp.md}-->
<h2 id="gaussian-processes">Gaussian Processes</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-lectures.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-lectures.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Models where we model the entire joint distribution of our training data, <span class="math inline">$p(\dataVector, \inputMatrix)$</span> are sometimes described as <em>generative models</em>. Because we can use sampling to generate data sets that represent all our assumptions. However, as we discussed in the sessions on  and , this can be a bad idea, because if our assumptions are wrong then we can make poor predictions. We can try to make more complex assumptions about data to alleviate the problem, but then this typically leads to challenges for tractable application of the sum and rules of probability that are needed to compute the relevant marginal and conditional densities. If we know the form of the question we wish to answer then we typically try and represent that directly, through <span class="math inline">$p(\dataVector|\inputMatrix)$</span>. In practice, we also have been making assumptions of conditional independence given the model parameters, <br /><span class="math display">$$
p(\dataVector|\inputMatrix, \mappingVector) =
\prod_{i=1}^{\numData} p(\dataScalar_i | \inputVector_i, \mappingVector)
$$</span><br /> Gaussian processes are <em>not</em> normally considered to be <em>generative models</em>, but we will be much more interested in the principles of conditioning in Gaussian processes because we will use conditioning to make predictions between our test and training data. We will avoid the data conditional indpendence assumption in favour of a richer assumption about the data, in a Gaussian process we assume data is <em>jointly Gaussian</em> with a particular mean and covariance, <br /><span class="math display">$$
\dataVector|\inputMatrix \sim \gaussianSamp{\mathbf{m}(\inputMatrix)}{\kernelMatrix(\inputMatrix)},
$$</span><br /> where the conditioning is on the inputs <span class="math inline">$\inputMatrix$</span> which are used for computing the mean and covariance. For this reason they are known as mean and covariance functions.</p>
<h2 id="basis-functions">Basis Functions</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/basis-functions-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/basis-functions-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Here’s the idea, instead of working directly on the original input space, <span class="math inline">$\inputVector$</span>, we build models in a new space, <span class="math inline">$\basisVector(\inputVector)$</span> where <span class="math inline">$\basisVector(\cdot)$</span> is a <em>vector-valued</em> function that is defined on the space <span class="math inline">$\inputVector$</span>.</p>
<h2 id="quadratic-basis">Quadratic Basis</h2>
<p>Remember, that a <em>vector-valued function</em> is just a vector that contains functions instead of values. Here’s an example for a one dimensional input space, <span class="math inline"><em>x</em></span>, being projected to a <em>quadratic</em> basis. First we consider each basis function in turn, we can think of the elements of our vector as being indexed so that we have <br /><span class="math display">$$
\begin{align*}
\basisFunc_1(\inputScalar) = 1, \\
\basisFunc_2(\inputScalar) = x, \\
\basisFunc_3(\inputScalar) = \inputScalar^2.
\end{align*}
$$</span><br /> Now we can consider them together by placing them in a vector, <br /><span class="math display">$$
\basisVector(\inputScalar) = \begin{bmatrix} 1\\ x \\ \inputScalar^2\end{bmatrix}.
$$</span><br /> For the vector-valued function, we have simply collected the different functions together in the same vector making them notationally easier to deal with in our mathematics.</p>
<p>When we consider the vector-valued function for each data point, then we place all the data into a matrix. The result is a matrix valued function, <br /><span class="math display">$$
\basisMatrix(\inputVector) = 
\begin{bmatrix} 1 &amp; \inputScalar_1 &amp;
\inputScalar_1^2 \\
1 &amp; \inputScalar_2 &amp; \inputScalar_2^2\\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; \inputScalar_n &amp; \inputScalar_n^2
\end{bmatrix}
$$</span><br /> where we are still in the one dimensional input setting so <span class="math inline">$\inputVector$</span> here represents a vector of our inputs with <span class="math inline">$\numData$</span> elements.</p>
<p>Let’s try constructing such a matrix for a set of inputs. First of all, we create a function that returns the matrix valued function.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">def</span> quadratic(x, <span class="op">**</span>kwargs):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="co">&quot;&quot;&quot;Take in a vector of input values and return the design matrix associated </span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co">    with the basis functions.&quot;&quot;&quot;</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>    <span class="cf">return</span> np.hstack([np.ones((x.shape[<span class="dv">0</span>], <span class="dv">1</span>)), x, x<span class="op">**</span><span class="dv">2</span>])</span></code></pre></div>
<h2 id="functions-derived-from-quadratic-basis">Functions Derived from Quadratic Basis</h2>
<p><br /><span class="math display">$$
\mappingFunction(\inputScalar) = {\color{red}\mappingScalar_0}   + {\color{magenta}\mappingScalar_1 \inputScalar} + {\color{blue}\mappingScalar_2 \inputScalar^2}
$$</span><br /></p>
<div class="figure">
<div id="-figure" class="figure-frame">
<object class="svgplot quadratic-basis-2" data="../slides/diagrams/ml/quadratic_basis002.svg" width="The set of functions which are combined to form a *quadratic* basis." style=" ">
</object>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>This function takes in an <span class="math inline">$\numData \times 1$</span> dimensional vector and returns an <span class="math inline">$\numData \times 3$</span> dimensional <em>design matrix</em> containing the basis functions. We can plot those basis functions against there input as follows.</p>
<p>The actual function we observe is then made up of a sum of these functions. This is the reason for the name basis. The term <em>basis</em> means ‘the underlying support or foundation for an idea, argument, or process’, and in this context they form the underlying support for our prediction function. Our prediction function can only be composed of a weighted linear sum of our basis functions.</p>
<h2 id="quadratic-functions">Quadratic Functions</h2>
<div class="figure">
<div id="quadratic-function-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/quadratic_function002.svg" width="80%" style=" ">
</object>
</div>
<div id="quadratic-function-2-magnify" class="magnify" onclick="magnifyFigure(&#39;quadratic-function-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="quadratic-function-2-caption" class="caption-frame">
<p>Figure: Functions constructed by weighted sum of the components of a quadratic basis.</p>
</div>
</div>
<h2 id="marginal-likelihood">Marginal Likelihood</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-from-basis-functions.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-from-basis-functions.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>To understand the Gaussian process we’re going to build on our understanding of the marginal likelihood for Bayesian regression. In the session on  we sampled directly from the weight vector, <span class="math inline">$\mappingVector$</span> and applied it to the basis matrix <span class="math inline">$\basisMatrix$</span> to obtain a sample from the prior and a sample from the posterior. It is often helpful to think of modeling techniques as <em>generative</em> models. To give some thought as to what the process for obtaining data from the model is. From the perspective of Gaussian processes, we want to start by thinking of basis function models, where the parameters are sampled from a prior, but move to thinking about sampling from the marginal likelihood directly.</p>
<h2 id="sampling-from-the-prior">Sampling from the Prior</h2>
<p>The first thing we’ll do is to set up the parameters of the model, these include the parameters of the prior, the parameters of the basis functions and the noise level.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># set prior variance on w</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>alpha <span class="op">=</span> <span class="fl">4.</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co"># set the order of the polynomial basis set</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>degree <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="co"># set the noise variance</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>sigma2 <span class="op">=</span> <span class="fl">0.01</span></span></code></pre></div>
<p>Now we have the variance, we can sample from the prior distribution to see what form we are imposing on the functions <em>a priori</em>.</p>
<p>Let’s now compute a range of values to make predictions at, spanning the <em>new</em> space of inputs,</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">def</span> polynomial(x, degree, loc, scale):</span>
<span id="cb11-2"><a href="#cb11-2"></a>    degrees <span class="op">=</span> np.arange(degree<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb11-3"><a href="#cb11-3"></a>    <span class="cf">return</span> ((x<span class="op">-</span>loc)<span class="op">/</span>scale)<span class="op">**</span>degrees</span></code></pre></div>
<p>now let’s build the basis matrices. First we load in the data</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb13-2"><a href="#cb13-2"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb13-3"><a href="#cb13-3"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>loc <span class="op">=</span> <span class="fl">1950.</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>scale <span class="op">=</span> <span class="fl">100.</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>num_data <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb14-4"><a href="#cb14-4"></a>num_pred_data <span class="op">=</span> <span class="dv">100</span> <span class="co"># how many points to use for plotting predictions</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>x_pred <span class="op">=</span> np.linspace(<span class="dv">1880</span>, <span class="dv">2030</span>, num_pred_data)[:, <span class="va">None</span>] <span class="co"># input locations for predictions</span></span>
<span id="cb14-6"><a href="#cb14-6"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, degree<span class="op">=</span>degree, loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span>
<span id="cb14-7"><a href="#cb14-7"></a>Phi <span class="op">=</span> polynomial(x, degree<span class="op">=</span>degree, loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</span></code></pre></div>
<h2 id="weight-space-view">Weight Space View</h2>
<p>To generate typical functional predictions from the model, we need a set of model parameters. We assume that the parameters are drawn independently from a Gaussian density, <br /><span class="math display">$$
\weightVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
$$</span><br /> then we can combine this with the definition of our prediction function <span class="math inline">$\mappingFunction(\inputVector)$</span>, <br /><span class="math display">$$
\mappingFunction(\inputVector) = \weightVector^\top \basisVector(\inputVector).
$$</span><br /> We can now sample from the prior density to obtain a vector <span class="math inline">$\weightVector$</span> using the function <code>np.random.normal</code> and combine these parameters with our basis to create some samples of what <span class="math inline">$\mappingFunction(\inputVector)$</span> looks like,</p>
<h2 id="function-space-view">Function Space View</h2>
<p>The process we have used to generate the samples is a two stage process. To obtain each function, we first generated a sample from the prior, <br /><span class="math display">$$
\weightVector \sim \gaussianSamp{\zerosVector}{\alpha \eye}
$$</span><br /> then if we compose our basis matrix, <span class="math inline">$\basisMatrix$</span> from the basis functions associated with each row then we get, <br /><span class="math display">$$
\basisMatrix = \begin{bmatrix}\basisVector(\inputVector_1) \\ \vdots \\
\basisVector(\inputVector_\numData)\end{bmatrix}
$$</span><br /> then we can write down the vector of function values, as evaluated at <br /><span class="math display">$$
\mappingFunctionVector = \begin{bmatrix} \mappingFunction_1
\\ \vdots \mappingFunction_\numData\end{bmatrix}
$$</span><br /> in the form <br /><span class="math display">$$
\mappingFunctionVector = \basisMatrix
\weightVector.
$$</span><br />}</p>
<p>Now we can use standard properties of multivariate Gaussians to write down the probability density that is implied over <span class="math inline">$\mappingFunctionVector$</span>. In particular we know that if <span class="math inline">$\weightVector$</span> is sampled from a multivariate normal (or multivariate Gaussian) with covariance <span class="math inline">$\alpha \eye$</span> and zero mean, then assuming that <span class="math inline">$\basisMatrix$</span> is a deterministic matrix (i.e. it is not sampled from a probability density) then the vector <span class="math inline">$\mappingFunctionVector$</span> will also be distributed according to a zero mean multivariate normal as follows, <br /><span class="math display">$$
\mappingFunctionVector \sim \gaussianSamp{\zerosVector}{\alpha \basisMatrix\basisMatrix^\top}.
$$</span><br /></p>
<p>The question now is, what happens if we sample <span class="math inline">$\mappingFunctionVector$</span> directly from this density, rather than first sampling <span class="math inline">$\weightVector$</span> and then multiplying by <span class="math inline">$\basisMatrix$</span>. Let’s try this. First of all we define the covariance as <br /><span class="math display">$$
\kernelMatrix = \alpha
\basisMatrix\basisMatrix^\top.
$$</span><br /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>K <span class="op">=</span> alpha<span class="op">*</span>Phi_pred<span class="op">@</span>Phi_pred.T</span></code></pre></div>
<p>Now we can use the <code>np.random.multivariate_normal</code> command for sampling from a multivariate normal with covariance given by <span class="math inline">$\kernelMatrix$</span> and zero mean,</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">10</span>):</span>
<span id="cb16-2"><a href="#cb16-2"></a>    f_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</span>
<span id="cb16-3"><a href="#cb16-3"></a>    plt.plot(x_pred.flatten(), f_sample.flatten())</span></code></pre></div>
<p>The samples appear very similar to those which we obtained indirectly. That is no surprise because they are effectively drawn from the same mutivariate normal density. However, when sampling <span class="math inline">$\mappingFunctionVector$</span> directly we created the covariance for <span class="math inline">$\mappingFunctionVector$</span>. We can visualise the form of this covaraince in an image in python with a colorbar to show scale.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb17-2"><a href="#cb17-2"></a>im <span class="op">=</span> ax.imshow(K, interpolation<span class="op">=</span><span class="st">&#39;none&#39;</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a>fig.colorbar(im)</span></code></pre></div>
<p>{This image is the covariance expressed between different points on the function. In regression we normally also add independent Gaussian noise to obtain our observations <span class="math inline">$\dataVector$</span>, <br /><span class="math display">$$
\dataVector = \mappingFunctionVector + \boldsymbol{\epsilon}
$$</span><br /> where the noise is sampled from an independent Gaussian distribution with variance <span class="math inline">$\dataStd^2$</span>, <br /><span class="math display">$$
\epsilon \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}.
$$</span><br /> we can use properties of Gaussian variables, i.e. the fact that sum of two Gaussian variables is also Gaussian, and that it’s covariance is given by the sum of the two covariances, whilst the mean is given by the sum of the means, to write down the marginal likelihood, <br /><span class="math display">$$
\dataVector \sim \gaussianSamp{\zerosVector}{\basisMatrix\basisMatrix^\top +\dataStd^2\eye}.
$$</span><br /> Sampling directly from this density gives us the noise corrupted functions,</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>K <span class="op">=</span> alpha<span class="op">*</span>Phi_pred<span class="op">@</span>Phi_pred.T <span class="op">+</span> sigma2<span class="op">*</span>np.eye(x_pred.size)</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb18-3"><a href="#cb18-3"></a>    y_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</span>
<span id="cb18-4"><a href="#cb18-4"></a>    plt.plot(x_pred.flatten(), y_sample.flatten())</span></code></pre></div>
<p>where the effect of our noise term is to roughen the sampled functions, we can also increase the variance of the noise to see a different effect,</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>sigma2 <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>K <span class="op">=</span> alpha<span class="op">*</span>Phi_pred<span class="op">@</span>Phi_pred.T <span class="op">+</span> sigma2<span class="op">*</span>np.eye(x_pred.size)</span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb19-4"><a href="#cb19-4"></a>    y_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</span>
<span id="cb19-5"><a href="#cb19-5"></a>    plt.plot(x_pred.flatten(), y_sample.flatten())</span></code></pre></div>
<h3 id="exercise-2">Exercise 2</h3>
<p><strong>Function Space Reflection</strong> How do you include the noise term when sampling in the weight space point of view?</p>
<h2 id="non-degenerate-gaussian-processes">Non-degenerate Gaussian Processes</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-degenerate-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-degenerate-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The process described above is degenerate. The covariance function is of rank at most <span class="math inline">$\numHidden$</span> and since the theoretical amount of data could always increase <span class="math inline">$\numData \rightarrow \infty$</span>, the covariance function is not full rank. This means as we increase the amount of data to infinity, there will come a point where we can’t normalize the process because the multivariate Gaussian has the form, <br /><span class="math display">$$
\gaussianDist{\mappingFunctionVector}{\zerosVector}{\kernelMatrix} = \frac{1}{\left(2\pi\right)^{\frac{\numData}{2}}\det{\kernelMatrix}^\frac{1}{2}} \exp\left(-\frac{\mappingFunctionVector^\top\kernelMatrix \mappingFunctionVector}{2}\right)
$$</span><br /> and a non-degenerate kernel matrix leads to <span class="math inline">$\det{\kernelMatrix} = 0$</span> defeating the normalization (it’s equivalent to finding a projection in the high dimensional Gaussian where the variance of the the resulting univariate Gaussian is zero, i.e. there is a null space on the covariance, or alternatively you can imagine there are one or more directions where the Gaussian has become the delta function).</p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Radford Neal
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/radford-neal.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<p>In the machine learning field, it was Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal 1994)</span> that realized the potential of the next step. In his 1994 thesis, he was considering Bayesian neural networks, of the type we described above, and in considered what would happen if you took the number of hidden nodes, or neurons, to infinity, i.e. <span class="math inline">$\numHidden \rightarrow \infty$</span>.</p>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neal-infinite-priors-magnify" class="magnify" onclick="magnifyFigure(&#39;neal-infinite-priors&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="neal-infinite-priors-caption" class="caption-frame">
<p>Figure: Page 37 of <a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s 1994 thesis</a></p>
</div>
</div>
<p>In loose terms, what Radford considers is what happens to the elements of the covariance function, <br /><span class="math display">$$
  \begin{align*}
  \kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) &amp; = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)\\
  &amp; = \alpha \sum_k \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_j\right)
  \end{align*}
  $$</span><br /> if instead of considering a finite number you sample infinitely many of these activation functions, sampling parameters from a prior density, <span class="math inline">$p(\mappingVectorTwo)$</span>, for each one, <br /><span class="math display">$$
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \int \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}, \inputVector_j\right) p(\mappingVector^{(1)}) \text{d}\mappingVector^{(1)}
$$</span><br /> And that’s not <em>only</em> for Gaussian <span class="math inline">$p(\mappingVectorTwo)$</span>. In fact this result holds for a range of activations, and a range of prior densities because of the <em>central limit theorem</em>.</p>
<p>To write it in the form of a probabilistic program, as long as the distribution for <span class="math inline"><em>ϕ</em><sub><em>i</em></sub></span> implied by this short probabilistic program, <br /><span class="math display">$$
  \begin{align*}
  \mappingVectorTwo &amp; \sim p(\cdot)\\
  \phi_i &amp; = \activationScalar\left(\mappingVectorTwo, \inputVector_i\right), 
  \end{align*}
  $$</span><br /> has finite variance, then the result of taking the number of hidden units to infinity, with appropriate scaling, is also a Gaussian process.</p>
<h2 id="further-reading">Further Reading</h2>
<p>To understand this argument in more detail, I highly recommend reading chapter 2 of Neal’s thesis <span class="citation" data-cites="Neal:bayesian94">(Neal 1994)</span>, which remains easy to read and clear today. Indeed, for readers interested in Bayesian neural networks, both Raford Neal’s and David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay 1992)</span> remain essential reading. Both theses embody a clarity of thought, and an ability to weave together threads from different fields that was the business of machine learning in the 1990s. Radford and David were also pioneers in making their software widely available and publishing material on the web.</p>
<h2 id="gaussian-process">Gaussian Process</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-function-space.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-function-space.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In our  we sampled from the prior over paraemters. Through the properties of multivariate Gaussian densities this prior over parameters implies a particular density for our data observations, <span class="math inline">$\dataVector$</span>. In this session we sampled directly from this distribution for our data, avoiding the intermediate weight-space representation. This is the approach taken by <em>Gaussian processes</em>. In a Gaussian process you specify the <em>covariance function</em> directly, rather than <em>implicitly</em> through a basis matrix and a prior over parameters. Gaussian processes have the advantage that they can be <em>nonparametric</em>, which in simple terms means that they can have <em>infinite</em> basis functions. In the lectures we introduced the <em>exponentiated quadratic</em> covariance, also known as the RBF or the Gaussian or the squared exponential covariance function. This covariance function is specified by <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left( -\frac{\left\Vert \inputVector-\inputVector^\prime\right\Vert^2}{2\ell^2}\right),
$$</span><br /> where <span class="math inline">$\left\Vert\inputVector - \inputVector^\prime\right\Vert^2$</span> is the squared distance between the two input vectors <br /><span class="math display">$$
\left\Vert\inputVector - \inputVector^\prime\right\Vert^2 = (\inputVector - \inputVector^\prime)^\top (\inputVector - \inputVector^\prime) 
$$</span><br /> Let’s build a covariance matrix based on this function. First we define the form of the covariance function,</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="im">from</span> mlai <span class="im">import</span> eq_cov</span></code></pre></div>
<p>We can use this to compute <em>directly</em> the covariance for <span class="math inline">$\mappingFunctionVector$</span> at the points given by <code>x_pred</code>. Let’s define a new function <code>K()</code> which does this,</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">from</span> mlai <span class="im">import</span> Kernel</span></code></pre></div>
<p>Now we can image the resulting covariance,</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>kernel <span class="op">=</span> Kernel(function<span class="op">=</span>eq_cov, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">10.</span>)</span>
<span id="cb22-2"><a href="#cb22-2"></a>K <span class="op">=</span> kernel.K(x_pred, x_pred)</span></code></pre></div>
<p>To visualise the covariance between the points we can use the <code>imshow</code> function in matplotlib.</p>
<p>Finally, we can sample functions from the marginal likelihood.</p>
<h3 id="exercise-3">Exercise 3</h3>
<p><strong>Moving Parameters</strong> Have a play with the parameters for this covariance function (the lengthscale and the variance) and see what effects the parameters have on the types of functions you observe.</p>
<h2 id="bayesian-inference-by-rejection-sampling">Bayesian Inference by Rejection Sampling</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>One view of Bayesian inference is to assume we are given a mechanism for generating samples, where we assume that mechanism is representing on accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world by discarding all those samples that are inconsistent with our prior. The <em>likelihood</em> defines mathematically what we mean by inconsistent with the prior. The higher the noise level in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are considered to be samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two sampling techniques known as <em>rejection sampling</em> and <em>importance sampling</em>. It is realized in practice in an approach known as <em>approximate Bayesian computation</em> (ABC) or likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because most samples will be inconsistent with the data and as a result the mechanism has to be operated many times to obtain a few posterior samples.</p>
<p>However, in the Gaussian process case, when the likelihood also assumes Gaussian noise, we can operate this mechanism mathematically, and obtain the posterior density <em>analytically</em>. This is the benefit of Gaussian processes.</p>
<p>First we will load in two python functions for computing the covariance function.</p>
<p>Next we sample from a multivariate normal density (a multivariate Gaussian), using the covariance function as the covariance matrix.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>plot.rejection_samples(kernel<span class="op">=</span>kernel, </span>
<span id="cb23-2"><a href="#cb23-2"></a>    diagrams<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp&#39;</span>)</span></code></pre></div>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for generating samples (the <em>prior</em>), and we discard all samples inconsistent with our data, leaving the samples of interest (the <em>posterior</em>). This is a rejection sampling view of Bayesian inference. The Gaussian process allows us to do this analytically by multiplying the <em>prior</em> by the <em>likelihood</em>.</p>
</div>
</div>
<h2 id="gaussian-process-1">Gaussian Process</h2>
<p>The Gaussian process perspective takes the marginal likelihood of the data to be a joint Gaussian density with a covariance given by <span class="math inline">$\kernelMatrix$</span>. So the model likelihood is of the form, <br /><span class="math display">$$
p(\dataVector|\inputMatrix) =
\frac{1}{(2\pi)^{\frac{\numData}{2}}|\kernelMatrix|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\dataVector^\top \left(\kernelMatrix+\dataStd^2
\eye\right)^{-1}\dataVector\right)
$$</span><br /> where the input data, <span class="math inline">$\inputMatrix$</span>, influences the density through the covariance matrix, <span class="math inline">$\kernelMatrix$</span> whose elements are computed through the covariance function, <span class="math inline">$\kernelScalar(\inputVector, \inputVector^\prime)$</span>.</p>
<p>This means that the negative log likelihood (the objective function) is given by, <br /><span class="math display">$$
\errorFunction(\boldsymbol{\theta}) = \frac{1}{2} \log |\kernelMatrix|
+ \frac{1}{2} \dataVector^\top \left(\kernelMatrix +
\dataStd^2\eye\right)^{-1}\dataVector
$$</span><br /> where the <em>parameters</em> of the model are also embedded in the covariance function, they include the parameters of the kernel (such as lengthscale and variance), and the noise variance, <span class="math inline">$\dataStd^2$</span>. Let’s create a class in python for storing these variables.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="im">from</span> mlai <span class="im">import</span> GP</span></code></pre></div>
<h2 id="making-predictions">Making Predictions</h2>
<p>We now have a probability density that represents functions. How do we make predictions with this density? The density is known as a process because it is <em>consistent</em>. By consistency, here, we mean that the model makes predictions for <span class="math inline">$\mappingFunctionVector$</span> that are unaffected by future values of <span class="math inline">$\mappingFunctionVector^*$</span> that are currently unobserved (such as test points). If we think of <span class="math inline">$\mappingFunctionVector^*$</span> as test points, we can still write down a joint probability density over the training observations, <span class="math inline">$\mappingFunctionVector$</span> and the test observations, <span class="math inline">$\mappingFunctionVector^*$</span>. This joint probability density will be Gaussian, with a covariance matrix given by our covariance function, <span class="math inline">$\kernelScalar(\inputVector_i, \inputVector_j)$</span>. <br /><span class="math display">$$
\begin{bmatrix}\mappingFunctionVector \\ \mappingFunctionVector^*\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\
\kernelMatrix_\ast^\top &amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}}
$$</span><br /> where here <span class="math inline">$\kernelMatrix$</span> is the covariance computed between all the training points, <span class="math inline">$\kernelMatrix_\ast$</span> is the covariance matrix computed between the training points and the test points and <span class="math inline">$\kernelMatrix_{\ast,\ast}$</span> is the covariance matrix computed betwen all the tests points and themselves. To be clear, let’s compute these now for our example, using <code>x</code> and <code>y</code> for the training data (although <code>y</code> doesn’t enter the covariance) and <code>x_pred</code> as the test locations.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># set covariance function parameters</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>variance <span class="op">=</span> <span class="fl">16.0</span></span>
<span id="cb25-3"><a href="#cb25-3"></a>lengthscale <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co"># set noise variance</span></span>
<span id="cb25-5"><a href="#cb25-5"></a>sigma2 <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb25-6"><a href="#cb25-6"></a></span>
<span id="cb25-7"><a href="#cb25-7"></a>kernel <span class="op">=</span> Kernel(eq_cov, variance<span class="op">=</span>variance, lengthscale<span class="op">=</span>lengthscale)</span>
<span id="cb25-8"><a href="#cb25-8"></a>K <span class="op">=</span> kernel.K(x, x)</span>
<span id="cb25-9"><a href="#cb25-9"></a>K_star <span class="op">=</span> kernel.K(x, x_pred)</span>
<span id="cb25-10"><a href="#cb25-10"></a>K_starstar <span class="op">=</span> kernel.K(x_pred, x_pred)</span></code></pre></div>
<p>Now we use this structure to visualise the covariance between test data and training data. This structure is how information is passed between test and training data. Unlike the maximum likelihood formalisms we’ve been considering so far, the structure expresses <em>correlation</em> between our different data points. However, just like the  we now have a <em>joint density</em> between some variables of interest. In particular we have the joint density over <span class="math inline">$p(\mappingFunctionVector, \mappingFunctionVector^*)$</span>. The joint density is <em>Gaussian</em> and <em>zero mean</em>. It is specified entirely by the <em>covariance matrix</em>, <span class="math inline">$\kernelMatrix$</span>. That covariance matrix is, in turn, defined by a covariance function. Now we will visualise the form of that covariance in the form of the matrix, <br /><span class="math display">$$
\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\ \kernelMatrix_\ast^\top
&amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}
$$</span><br /></p>
<p>There are four blocks to this color plot. The upper left block is the covariance of the training data with itself, <span class="math inline">$\kernelMatrix$</span>. We see some structure here due to the missing data from the first and second world wars. Alongside this covariance (to the right and below) we see the cross covariance between the training and the test data (<span class="math inline">$\kernelMatrix_*$</span> and <span class="math inline">$\kernelMatrix_*^\top$</span>). This is giving us the covariation between our training and our test data. Finally the lower right block The banded structure we now observe is because some of the training points are near to some of the test points. This is how we obtain ‘communication’ between our training data and our test data. If there is no structure in <span class="math inline">$\kernelMatrix_*$</span> then our belief about the test data simply matches our prior.</p>
<h2 id="prediction-across-two-points-with-gps">Prediction Across Two Points with GPs</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gptwopointpred.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gptwopointpred.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-2"><a href="#cb26-2"></a>np.random.seed(<span class="dv">4949</span>)</span></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="im">import</span> teaching_plots <span class="im">as</span> plot</span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_2$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_2$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<ul>
<li>The single contour of the Gaussian density represents the <font color="red">joint distribution, <span class="math inline">$p(\mappingFunction_1, \mappingFunction_2)$</span></font></li>
</ul>
<div class="incremental">
<ul>
<li>We observe that <font color="green"><span class="math inline">$\mappingFunction_1=?$</span></font></li>
</ul>
</div>
<div class="incremental">
<ul>
<li><p>Conditional density: <font color="red"><span class="math inline">$p(\mappingFunction_2|\mappingFunction_1=?)$</span></font></p></li>
<li><p>Prediction of <span class="math inline">$\mappingFunction_2$</span> from <span class="math inline">$\mappingFunction_1$</span> requires <em>conditional density</em>.</p></li>
<li><p>Conditional density is <em>also</em> Gaussian. <br /><span class="math display">$$
p(\mappingFunction_2|\mappingFunction_1) = {\mathcal{N}\left(\mappingFunction_2|\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1,\kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}\right)}
$$</span><br /> where covariance of joint density is given by <br /><span class="math display">$$
\kernelMatrix= \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}\end{bmatrix}
$$</span><br /></p></li>
</ul>
<div class="figure">
<div id="two-point-sample-13-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-13-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-13&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-13-caption" class="caption-frame">
<p>Figure: Sample from the joint Gaussian model, points indexed by 1 and 8 highlighted.</p>
</div>
</div>
<div class="figure">
<div id="two-point-sample-one-eight-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-eight-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-eight&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-eight-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_8$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_8$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<ul>
<li>The single contour of the Gaussian density represents the <font color="blue">joint distribution, <span class="math inline">$p(\mappingFunction_1, \mappingFunction_8)$</span></font></li>
</ul>
</div>
<div class="incremental">
<ul>
<li>We observe a value for <font color="green"><span class="math inline">$\mappingFunction_1=-?$</span></font></li>
</ul>
</div>
<div class="incremental">
<ul>
<li><p>Conditional density: <font color="red"><span class="math inline">$p(\mappingFunction_5|\mappingFunction_1=?)$</span></font>.</p></li>
<li><p>Prediction of <span class="math inline">$\mappingFunctionVector_*$</span> from <span class="math inline">$\mappingFunctionVector$</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <br /><span class="math display">$$
p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|\kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector,\kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}\right)}
$$</span><br /> </large></p></li>
<li><p>Here covariance of joint density is given by <br /><span class="math display">$$
\kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
$$</span><br /></p></li>
<li><p>Prediction of <span class="math inline">$\mappingFunctionVector_*$</span> from <span class="math inline">$\mappingFunctionVector$</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <br /><span class="math display">$$
p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|\meanVector,\conditionalCovariance\right)}
$$</span><br /> <br /><span class="math display">$$
\meanVector= \kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector
$$</span><br /> <br /><span class="math display">$$
\conditionalCovariance = \kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}
$$</span><br /> </large></p></li>
<li><p>Here covariance of joint density is given by <br /><span class="math display">$$
\kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
$$</span><br /></p></li>
</ul>
</div>
<h2 id="the-importance-of-the-covariance-function">The Importance of the Covariance Function</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-covariance-function-importance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-covariance-function-importance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The covariance function encapsulates our assumptions about the data. The equations for the distribution of the prediction function, given the training observations, are highly sensitive to the covariation between the test locations and the training locations as expressed by the matrix <span class="math inline">$\kernelMatrix_*$</span>. We defined a matrix <span class="math inline"><strong>A</strong></span> which allowed us to express our conditional mean in the form, <br /><span class="math display">$$
\meanVector_\mappingFunction = \mathbf{A}^\top \dataVector,
$$</span><br /> where <span class="math inline">$\dataVector$</span> were our <em>training observations</em>. In other words our mean predictions are always a linear weighted combination of our <em>training data</em>. The weights are given by computing the covariation between the training and the test data (<span class="math inline">$\kernelMatrix_*$</span>) and scaling it by the inverse covariance of the training data observations, <span class="math inline">$\left[\kernelMatrix + \dataStd^2 \eye\right]^{-1}$</span>. This inverse is the main computational object that needs to be resolved for a Gaussian process. It has a computational burden which is <span class="math inline">$O(\numData^3)$</span> and a storage burden which is <span class="math inline">$O(\numData^2)$</span>. This makes working with Gaussian processes computationally intensive for the situation where <span class="math inline">$\numData&gt;10,000$</span>.</p>
<div class="figure">
<div id="intro-to-gps-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-gps-magnify" class="magnify" onclick="magnifyFigure(&#39;intro-to-gps&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-gps-caption" class="caption-frame">
<p>Figure: Introduction to Gaussian processes given by Neil Lawrence at the 2014 Gaussian process Winter School at the University of Sheffield.</p>
</div>
</div>
<h2 id="improving-the-numerics">Improving the Numerics</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-numerics-and-optimization.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-numerics-and-optimization.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In practice we shouldn’t be using matrix inverse directly to solve the GP system. One more stable way is to compute the <em>Cholesky decomposition</em> of the kernel matrix. The log determinant of the covariance can also be derived from the Cholesky decomposition.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="im">from</span> mlai <span class="im">import</span> update_inverse</span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>GP.update_inverse <span class="op">=</span> update_inverse</span></code></pre></div>
<h2 id="capacity-control">Capacity Control</h2>
<p>Gaussian processes are sometimes seen as part of a wider family of methods known as kernel methods. Kernel methods are also based around covariance functions, but in the field they are known as Mercer kernels. Mercer kernels have interpretations as inner products in potentially infinite dimensional Hilbert spaces. This interpretation arises because, if we take <span class="math inline"><em>α</em> = 1</span>, then the kernel can be expressed as <br /><span class="math display">$$
\kernelMatrix = \basisMatrix\basisMatrix^\top 
$$</span><br /> which imples the elements of the kernel are given by, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime).
$$</span><br /> So we see that the kernel function is developed from an inner product between the basis functions. Mercer’s theorem tells us that any valid <em>positive definite function</em> can be expressed as this inner product but with the caveat that the inner product could be <em>infinite length</em>. This idea has been used quite widely to <em>kernelize</em> algorithms that depend on inner products. The kernel functions are equivalent to covariance functions and they are parameterized accordingly. In the kernel modeling community it is generally accepted that kernel parameter estimation is a difficult problem and the normal solution is to cross validate to obtain parameters. This can cause difficulties when a large number of kernel parameters need to be estimated. In Gaussian process modelling kernel parameter estimation (in the simplest case proceeds) by maximum likelihood. This involves taking gradients of the likelihood with respect to the parameters of the covariance function.</p>
<h2 id="gradients-of-the-likelihood">Gradients of the Likelihood</h2>
<p>The easiest conceptual way to obtain the gradients is a two step process. The first step involves taking the gradient of the likelihood with respect to the covariance function, the second step involves considering the gradient of the covariance function with respect to its parameters.</p>
<h2 id="overall-process-scale">Overall Process Scale</h2>
<p>In general we won’t be able to find parameters of the covariance function through fixed point equations, we will need to do gradient based optimization.</p>
<h2 id="capacity-control-and-data-fit">Capacity Control and Data Fit</h2>
<p>The objective function can be decomposed into two terms, a capacity control term, and a data fit term. The capacity control term is the log determinant of the covariance. The data fit term is the matrix inner product between the data and the inverse covariance.</p>
<h2 id="learning-covariance-parameters">Learning Covariance Parameters</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-eigenvector.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-optimize-eigenvector.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Can we determine covariance parameters from the data?</p>
<p><br /><span class="math display">$$
\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}\color{blue}{\det{\kernelMatrix}^{\frac{1}{2}}}}\color{red}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;\color{blue}{-\frac{1}{2}\log\det{\kernelMatrix}}\color{red}{-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\errorFunction(\parameterVector) = \color{blue}{\frac{1}{2}\log\det{\kernelMatrix}} + \color{red}{\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
$$</span><br /></p>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <br /><span class="math display">$$\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)$$</span><br /></p>
<p><span> <br /><span class="math display">$$\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top$$</span><br /></span></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>gpoptimizePlot1</span></code></pre></div>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">$\eigenvalueMatrix$</span> represents distance on axes. <span class="math inline">$\rotationMatrix$</span> gives rotation.
</td>
</tr>
</table>
<ul>
<li><span class="math inline">$\eigenvalueMatrix$</span> is <em>diagonal</em>, <span class="math inline">$\rotationMatrix^\top\rotationMatrix = \eye$</span>.</li>
<li>Useful representation since <span class="math inline">$\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2$</span>.</li>
</ul>
<!--
\only<1>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant1.svg}}\only<2>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant2.svg}}\only<3>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant3.svg}}\only<4>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant4.svg}}\only<5>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant5.svg}}\only<6>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant6.svg}}\only<7>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant7.svg}}\only<8>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant8.svg}}\only<9>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant9.svg}}\only<10>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant10.svg}}-->
<!--```{.python}
gpoptimizePlot3
```

\only<1>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic1.svg}}\only<2>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic2.svg}}\only<3>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic3.svg}}-->
<div class="figure">
<div id="gp-optimise-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="gp-optimise-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-caption" class="caption-frame">
<p>Figure: Variation in the data fit term, the capacity term and the negative log likelihood for different lengthscales.</p>
</div>
</div>
<h2 id="exponentiated-quadratic-covariance">Exponentiated Quadratic Covariance</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The exponentiated quadratic covariance, also known as the Gaussian covariance or the RBF covariance and the squared exponential. Covariance between two points is related to the negative exponential of the squared distnace between those points. This covariance function can be derived in a few different ways: as the infinite limit of a radial basis function neural network, as diffusion in the heat equation, as a Gaussian filter in <em>Fourier space</em> or as the composition as a series of linear filters applied to a base function.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)$$</span><br />
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eq-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;eq-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eq-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function.</p>
</div>
</div>
<h2 id="example-prediction-of-malaria-incidence-in-uganda">Example: Prediction of Malaria Incidence in Uganda</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/martin-mubangizi.png" clip-path="url(#clip1)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacheco
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/ricardo-andrade-pacheco.png" clip-path="url(#clip2)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/john-quinn.jpg" clip-path="url(#clip3)"/>
</svg>
</div>
<p>As an example of using Gaussian process models within the full pipeline from data to decsion, we’ll consider the prediction of Malaria incidence in Uganda. For the purposes of this study malaria reports come in two forms, HMIS reports from health centres and Sentinel data, which is curated by the WHO. There are limited sentinel sites and many HMIS sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in collaboration with John Quinn and Martin Mubangizi <span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al. 2014; Mubangizi et al. 2014)</span>. John and Martin were initally from the AI-DEV group from the University of Makerere in Kampala and more latterly they were based at UN Global Pulse in Kampala.</p>
<p>Malaria data is spatial data. Uganda is split into districts, and health reports can be found for each district. This suggests that models such as conditional random fields could be used for spatial modelling, but there are two complexities with this. First of all, occasionally districts split into two. Secondly, sentinel sites are a specific location within a district, such as Nagongera which is a sentinel site based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify" onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districs. Data SRTM/NASA from <a href="https://dds.cr.usgs.gov/srtm/version2_1" class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<p><span style="text-align:right"><span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al. 2014; Mubangizi et al. 2014)</span></span></p>
<p>The common standard for collecting health data on the African continent is from the Health management information systems (HMIS). However, this data suffers from missing values <span class="citation" data-cites="Gething:hmis06">(Gething et al. 2006)</span> and diagnosis of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is located.</p>
</div>
</div>
<p><a href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World Health Organization Sentinel Surveillance systems</a> are set up “when high-quality data are needed about a particular disease that cannot be obtained through a passive system”. Several sentinel sites give accurate assessment of malaria disease levels in Uganda, including a site in Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify" onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to investigate whether Gaussian process models could be used to assimilate information from these two different sources of disease informaton. Further, we were interested in whether local information on rainfall and temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside rainfall and temperature, to improve predictions from HMIS data of levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify" onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school held at Makerere in Kampala in 2013. The school led, in turn, to the Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify" onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole district over time. Estimate is constructed with a Gaussian process with an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have fitted the reports with a Gaussian process with an additive covariance function. It has two components, one is a long time scale component (in red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the blue line (the short term report signal) above the red (which represents the long term trend? If so we have higher than expected reports. If this is the case <em>and</em> the gradient is still positive (i.e. reports are going up) we encode this with a <em>red</em> color. If it is the case and the gradient of the blue line is negative (i.e. reports are going down) we encode this with an <em>amber</em> color. Conversely, if the blue line is below the red <em>and</em> decreasing, we color <em>green</em>. On the other hand if it is below red but increasing, we color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad situation getting worse, amber is bad, but improving. Green is good and getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify" onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the districts to give an immediate impression of the current status of the disease across the country.</p>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian Process Framework in Python</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Gaussian processes are a flexible tool for non-parametric analysis with uncertainty. The GPy software was started in Sheffield to provide a easy to use interface to GPs. One which allowed the user to focus on the modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e. you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<h2 id="gpy-tutorial">GPy Tutorial</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-tutorial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-tutorial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
James Hensman
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/james-hensman.png" clip-path="url(#clip4)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Nicolas Durrande
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/nicolas-Durrande2.jpg" clip-path="url(#clip5)"/>
</svg>
</div>
<p>This GPy tutorial is based on material we share in the Gaussian process summer school for teaching these models <a href="https://gpss.cc" class="uri">https://gpss.cc</a>. It contains material from various members and former members of the Sheffield machine learning group, but particular mention should be made of <a href="https://sites.google.com/site/nicolasdurrandehomepage/">Nicolas Durrande</a> and <a href="https://jameshensman.github.io/">James Hensman</a>, see <a href="http://gpss.cc/gpss17/labs/GPSS_Lab1_2017.ipynb" class="uri">http://gpss.cc/gpss17/labs/GPSS_Lab1_2017.ipynb</a>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="im">import</span> GPy</span></code></pre></div>
<p>To give a feel for the sofware we’ll start by creating an exponentiated quadratic covariance function, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector - \inputVector^\prime}^2}{2\ell^2}\right),
$$</span><br /> where the length scale is <span class="math inline">ℓ</span> and the variance is <span class="math inline"><em>α</em></span>.</p>
<p>To set this up in GPy we create a kernel in the following manner.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>input_dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>alpha <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb36-3"><a href="#cb36-3"></a>lengthscale <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span>input_dim, variance<span class="op">=</span>alpha, lengthscale<span class="op">=</span>lengthscale)</span></code></pre></div>
<p>That builds a kernel object for us. The kernel can be displayed.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>display(kern)</span></code></pre></div>
<p>Or because it’s one dimensional, you can also plot the kernel as a function of its inputs (while the other is fixed).</p>
<div class="figure">
<div id="gpy-eq-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-covariance-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function as plotted by the <code>GPy.kern.plot</code> command.</p>
</div>
</div>
<p>You can set the lengthscale of the covariance to different values and plot the result.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span>input_dim)     <span class="co"># By default, the parameters are set to 1.</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>lengthscales <span class="op">=</span> np.asarray([<span class="fl">0.2</span>,<span class="fl">0.5</span>,<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">4.</span>])</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-covariance-lengthscales.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-covariance-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function plotted for different lengthscales by <code>GPy.kern.plot</code> command.</p>
</div>
</div>
<h2 id="covariance-functions-in-gpy">Covariance Functions in GPy</h2>
<p>Many covariance functions are already implemented in GPy. Instead of rbf, try constructing and plotting the following covariance functions: <code>exponential</code>, <code>Matern32</code>, <code>Matern52</code>, <code>Brownian</code>, <code>linear</code>, <code>bias</code>, <code>rbfcos</code>, <code>periodic_Matern32</code>, etc. Some of these covariance functions, such as <code>rbfcos</code>, are not parametrized by a variance and a lengthscale. Furthermore, not all kernels are stationary (i.e., they can’t all be written as <span class="math inline">$\kernelScalar(\inputVector, \inputVector^\prime) = f(\inputVector-\inputVector^\prime)$</span>, see for example the Brownian covariance function). For plotting so it may be interesting to change the value of the fixed input.</p>
<h2 id="combining-covariance-functions-in-gpy">Combining Covariance Functions in GPy</h2>
<p>In GPy you can easily combine covariance functions you have created using the sum and product operators, <code>+</code> and <code>*</code>. So, for example, if we wish to combine an exponentiated quadratic covariance with a Matern 5/2 then we can write</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb39-2"><a href="#cb39-2"></a>kern2 <span class="op">=</span> GPy.kern.Matern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">2.</span>, lengthscale<span class="op">=</span><span class="fl">4.</span>)</span>
<span id="cb39-3"><a href="#cb39-3"></a>kern <span class="op">=</span> kern1 <span class="op">+</span> kern2</span>
<span id="cb39-4"><a href="#cb39-4"></a>display(kern)</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-plus-matern52-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-plus-matern52-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-plus-matern52-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-plus-matern52-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-plus-matern52-covariance-caption" class="caption-frame">
<p>Figure: A combination of the exponentiated quadratic covariance plus the Matern <span class="math inline">5/2</span> covariance.</p>
</div>
</div>
<p>Or if we wanted to multiply them we can write</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb40-2"><a href="#cb40-2"></a>kern2 <span class="op">=</span> GPy.kern.Matern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">2.</span>, lengthscale<span class="op">=</span><span class="fl">4.</span>)</span>
<span id="cb40-3"><a href="#cb40-3"></a>kern <span class="op">=</span> kern1 <span class="op">*</span> kern2</span>
<span id="cb40-4"><a href="#cb40-4"></a>display(kern)</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-times-matern52-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-times-matern52-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-times-matern52-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-times-matern52-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-times-matern52-covariance-caption" class="caption-frame">
<p>Figure: A combination of the exponentiated quadratic covariance multiplied by the Matern <span class="math inline">5/2</span> covariance.</p>
</div>
</div>
<p>You can learn about how to implement <a href="https://gpy.readthedocs.io/en/latest/tuto_creating_new_kernels.html">new kernel objects in GPy here</a>.</p>
<div class="figure">
<div id="nicolas-durrande-on-kernel-design-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/-sY8zW3Om1Y?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="nicolas-durrande-on-kernel-design-magnify" class="magnify" onclick="magnifyFigure(&#39;nicolas-durrande-on-kernel-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nicolas-durrande-on-kernel-design-caption" class="caption-frame">
<p>Figure: Designing the covariance function for your Gaussian process is a key place in which you introduce your understanding of the data problem. To learn more about the design of covariance functions, see this talk from Nicolas Durrande at GPSS in 2016.</p>
</div>
</div>
<h2 id="a-gaussian-process-regression-model">A Gaussian Process Regression Model</h2>
<p>We will now combine the Gaussian process prior with some data to form a GP regression model with GPy. We will generate data from the function <br /><span class="math display">$$
\mappingFunction( \inputScalar ) = − \cos(\pi \inputScalar ) + \sin(4\pi \inputScalar )
$$</span><br /> over the domain <span class="math inline">[0, 1]</span>, adding some noise to gives <br /><span class="math display">$$
\dataScalar(\inputScalar) = \mappingFunction(\inputScalar) + \noiseScalar,
$$</span><br /> with the noise being Gaussian distributed, <span class="math inline">$\noiseScalar \sim \gaussianSamp{0}{0.01}$</span>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a>X <span class="op">=</span> np.linspace(<span class="fl">0.05</span>,<span class="fl">0.95</span>,<span class="dv">10</span>)[:,np.newaxis]</span>
<span id="cb41-2"><a href="#cb41-2"></a>Y <span class="op">=</span> <span class="op">-</span>np.cos(np.pi<span class="op">*</span>X) <span class="op">+</span> np.sin(<span class="dv">4</span><span class="op">*</span>np.pi<span class="op">*</span>X) <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">1</span>))</span></code></pre></div>
<div class="figure">
<div id="noisy-sine-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/noisy-sine.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-magnify" class="magnify" onclick="magnifyFigure(&#39;noisy-sine&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-caption" class="caption-frame">
<p>Figure: Data from the noisy sine wave for fitting with a GPy model.</p>
</div>
</div>
<p>A GP regression model based on an exponentiated quadratic covariance function can be defined by first defining a covariance function.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span><span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">1.</span>)</span></code></pre></div>
<p>And then combining it with the data to form a Gaussian process model.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X,Y,kern)</span></code></pre></div>
<p>Just as for the covariance function object, we can find out about the model using the command <code>display(model)</code>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>display(model)</span></code></pre></div>
<p>Note that by default the model includes some observation noise with variance 1. We can see the posterior mean prediction and visualize the marginal posterior variances using <code>model.plot()</code>.</p>
<div class="figure">
<div id="noisy-sine-gp-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/noisy-sine-gp-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-gp-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;noisy-sine-gp-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-gp-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the noisy sine data. Here the parameters of the process and the covariance function haven’t yet been optimized.</p>
</div>
</div>
<p>You can also look directly at the predictions for the model using.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>Xstar <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)[:, np.newaxis]</span>
<span id="cb45-2"><a href="#cb45-2"></a>Ystar, Vstar <span class="op">=</span> model.predict(Xstar)</span></code></pre></div>
<p>Which gives you the mean (<code>Ystar</code>), the variance (<code>Vstar</code>) at the locations given by <code>Xstar</code>.</p>
<h2 id="covariance-function-parameter-estimation">Covariance Function Parameter Estimation</h2>
<p>As we have seen during the lectures, the parameters values can be estimated by maximizing the likelihood of the observations. Since we don’t want one of the variance to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a>model.constrain_positive()</span></code></pre></div>
<p>The warnings are because the parameters are already constrained by default, the software is warning us that they are being reconstrained.</p>
<p>Now we can optimize the model using the <code>model.optimize()</code> method. Here we switch messages on, which allows us to see the progession of the optimization.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>By default the optimization is using a limited memory BFGS optimizer <span class="citation" data-cites="Byrd:lbfgsb95">(Byrd, Lu, and Nocedal 1995)</span>.</p>
<p>Once again we can display the model, now to see how the parameters have changed.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a>display(model)</span></code></pre></div>
<p>The lengthscale is much smaller, as well as the noise level. The variance of the exponentiated quadratic has also reduced.</p>
<div class="figure">
<div id="noisy-sine-gp-optimized-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/noisy-sine-gp-optimized-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-gp-optimized-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;noisy-sine-gp-optimized-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-gp-optimized-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the noisy sine data with parameters optimized.</p>
</div>
</div>
<h2 id="nigerian-covid-data">Nigerian Covid Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigeria-covid-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigeria-covid-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h2 id="gaussian-process-on-nigerian-covid-data">Gaussian Process on Nigerian Covid Data</h2>
<h2 id="review">Review</h2>
<h2 id="gpss-gaussian-process-summer-school">GPSS: Gaussian Process Summer School</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-summer-school.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-summer-school.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div style="width:1.5cm;text-align:center">

</div>
<p>If you’re interested in finding out more about Gaussian processes, you can attend the Gaussian process summer school, or view the lectures and material on line. Details of the school, future events and past events can be found at the website <a href="http://gpss.cc" class="uri">http://gpss.cc</a>.</p>
<h2 id="other-software">Other Software</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/other-gp-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/other-gp-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>GPy has inspired other software solutions, first of all <a href="https://github.com/GPflow/GPflow">GPflow</a>, which uses Tensor Flow’s automatic differentiation engine to allow rapid prototyping of new covariance functions and algorithms. More recently, <a href="https://github.com/cornellius-gp/gpytorch">GPyTorch</a> uses PyTorch for the same purpose.</p>
<p>The Probabilistic programming language <a href="https://pyro.ai/">pyro</a> also has GP support.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Andrade:consistent14">
<p>Andrade-Pacheco, Ricardo, Martin Mubangizi, John Quinn, and Neil D. Lawrence. 2014. “Consistent Mapping of Government Malaria Records Across a Changing Territory Delimitation.” <em>Malaria Journal</em> 13 (Suppl 1). <a href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>.</p>
</div>
<div id="ref-Byrd:lbfgsb95">
<p>Byrd, Richard H., Peihuang Lu, and Jorge Nocedal. 1995. “A Limited Memory Algorithm for Bound Constrained Optimization.” <em>SIAM Journal on Scientific and Statistical Computing</em> 16 (5): 1190–1208.</p>
</div>
<div id="ref-Gething:hmis06">
<p>Gething, Peter W., Abdisalan M. Noor, Priscilla W. Gikandi, Esther A. A. Ogara, Simon I. Hay, Mark S. Nixon, Robert W. Snow, and Peter M. Atkinson. 2006. “Improving Imperfect Data from Health Management Information Systems in Africa Using Space–Time Geostatistics.” <em>PLoS Medicine</em> 3 (6). <a href="https://doi.org/10.1371/journal.pmed.0030271">https://doi.org/10.1371/journal.pmed.0030271</a>.</p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, David J. C. 1992. “Bayesian Methods for Adaptive Models.” PhD thesis, California Institute of Technology.</p>
</div>
<div id="ref-Mubangizi:malaria14">
<p>Mubangizi, Martin, Ricardo Andrade-Pacheco, Michael Thomas Smith, John Quinn, and Neil D. Lawrence. 2014. “Malaria Surveillance with Multiple Data Sources Using Gaussian Process Models.” In <em>1st International Conference on the Use of Mobile ICT in Africa</em>.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, Radford M. 1994. “Bayesian Learning for Neural Networks.” PhD thesis, Dept. of Computer Science, University of Toronto.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: mit.</p>
</div>
</div>

