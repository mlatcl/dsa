---
title: "What is Machine Learning?"
venue: "Data Science Africa Summer School, Addis Ababa, Ethiopia"
abstract: "<p>In this talk we will introduce the fundamental ideas in
machine learning. We’ll develop our exposition around the ideas of
prediction function and the objective function. We don’t so much focus
on the derivation of particular algorithms, but more the general
principles involved to give an idea of the machine learning
<em>landscape</em>.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: Amazon Cambridge and University of Sheffield
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
edit_url: https://github.com/mlatcl/dsa/edit/gh-pages/_lamd/what-is-machine-learning.md
date: 2019-06-03
published: 2019-06-03
session: 1
reveal: 01-what-is-machine-learning.slides.html
transition: None
ipynb: 01-what-is-machine-learning.ipynb
pptx: 01-what-is-machine-learning.pptx
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="introduction">Introduction</h1>
<h2 id="data-science-africa">Data Science Africa</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-science-africa.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-science-africa.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="data-science-africa-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//data-science-africa-logo.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="data-science-africa-magnify" class="magnify"
onclick="magnifyFigure(&#39;data-science-africa&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="data-science-africa-caption" class="caption-frame">
<p>Figure: Data Science Africa <a href="http://datascienceafrica.org"
class="uri">http://datascienceafrica.org</a> is a ground up initiative
for capacity building around data science, machine learning and
artificial intelligence on the African continent.</p>
</div>
</div>
<div class="figure">
<div id="dsa-events-october-2021-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//dsa/dsa-events-october-2021.svg" width="60%" style=" ">
</object>
</div>
<div id="dsa-events-october-2021-magnify" class="magnify"
onclick="magnifyFigure(&#39;dsa-events-october-2021&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dsa-events-october-2021-caption" class="caption-frame">
<p>Figure: Data Science Africa meetings held up to October 2021.</p>
</div>
</div>
<p>Data Science Africa is a bottom up initiative for capacity building
in data science, machine learning and artificial intelligence on the
African continent.</p>
<p>As of May 2023 there have been eleven workshops and schools, located
in seven different countries: Nyeri, Kenya (twice); Kampala, Uganda;
Arusha, Tanzania; Abuja, Nigeria; Addis Ababa, Ethiopia; Accra, Ghana;
Kampala, Uganda and Kimberley, South Africa (virtual), and in Kigali,
Rwanda.</p>
<p>The main notion is <em>end-to-end</em> data science. For example,
going from data collection in the farmer’s field to decision making in
the Ministry of Agriculture. Or going from malaria disease counts in
health centers to medicine distribution.</p>
<p>The philosophy is laid out in <span class="citation"
data-cites="Lawrence:dsa15">(Lawrence, 2015)</span>. The key idea is
that the modern <em>information infrastructure</em> presents new
solutions to old problems. Modes of development change because less
capital investment is required to take advantage of this infrastructure.
The philosophy is that local capacity building is the right way to
leverage these challenges in addressing data science problems in the
African context.</p>
<p>Data Science Africa is now a non-govermental organization registered
in Kenya. The organising board of the meeting is entirely made up of
scientists and academics based on the African continent.</p>
<div class="figure">
<div id="africa-benefit-data-revolution-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//data-science/africa-benefit-data-revolution.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="africa-benefit-data-revolution-magnify" class="magnify"
onclick="magnifyFigure(&#39;africa-benefit-data-revolution&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="africa-benefit-data-revolution-caption" class="caption-frame">
<p>Figure: The lack of existing physical infrastructure on the African
continent makes it a particularly interesting environment for deploying
solutions based on the <em>information infrastructure</em>. The idea is
explored more in this Guardian op-ed on Guardian article on <a
href="https://www.theguardian.com/media-network/2015/aug/25/africa-benefit-data-science-information">How
African can benefit from the data revolution</a>.</p>
</div>
</div>
<p>Guardian article on <a
href="https://www.theguardian.com/media-network/2015/aug/25/africa-benefit-data-science-information">Data
Science Africa</a></p>
<h2 id="example-prediction-of-malaria-incidence-in-uganda">Example:
Prediction of Malaria Incidence in Uganda</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip0)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacecho
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip1)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip2)"/>
</svg>
</div>
<p>As an example of using Gaussian process models within the full
pipeline from data to decsion, we’ll consider the prediction of Malaria
incidence in Uganda. For the purposes of this study malaria reports come
in two forms, HMIS reports from health centres and Sentinel data, which
is curated by the WHO. There are limited sentinel sites and many HMIS
sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in
collaboration with John Quinn and Martin Mubangizi <span
class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>. John and Martin were
initally from the AI-DEV group from the University of Makerere in
Kampala and more latterly they were based at UN Global Pulse in Kampala.
You can see the work summarized on the UN Global Pulse <a
href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">disease
outbreaks project site here</a>.</p>
<ul>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
<p>Malaria data is spatial data. Uganda is split into districts, and
health reports can be found for each district. This suggests that models
such as conditional random fields could be used for spatial modelling,
but there are two complexities with this. First of all, occasionally
districts split into two. Secondly, sentinel sites are a specific
location within a district, such as Nagongera which is a sentinel site
based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify"
onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
<p>The common standard for collecting health data on the African
continent is from the Health management information systems (HMIS).
However, this data suffers from missing values <span class="citation"
data-cites="Gething:hmis06">(Gething et al., 2006)</span> and diagnosis
of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is
located.</p>
</div>
</div>
<p><a
href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World
Health Organization Sentinel Surveillance systems</a> are set up “when
high-quality data are needed about a particular disease that cannot be
obtained through a passive system”. Several sentinel sites give accurate
assessment of malaria disease levels in Uganda, including a site in
Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify"
onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature
for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to
investigate whether Gaussian process models could be used to assimilate
information from these two different sources of disease informaton.
Further, we were interested in whether local information on rainfall and
temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside
rainfall and temperature, to improve predictions from HMIS data of
levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify"
onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school
held at Makerere in Kampala in 2013. The school led, in turn, to the
Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole
district over time. Estimate is constructed with a Gaussian process with
an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have
fitted the reports with a Gaussian process with an additive covariance
function. It has two components, one is a long time scale component (in
red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the
blue line (the short term report signal) above the red (which represents
the long term trend? If so we have higher than expected reports. If this
is the case <em>and</em> the gradient is still positive (i.e. reports
are going up) we encode this with a <em>red</em> color. If it is the
case and the gradient of the blue line is negative (i.e. reports are
going down) we encode this with an <em>amber</em> color. Conversely, if
the blue line is below the red <em>and</em> decreasing, we color
<em>green</em>. On the other hand if it is below red but increasing, we
color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad
situation getting worse, amber is bad, but improving. Green is good and
getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of
the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria
situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the
districts to give an immediate impression of the current status of the
disease across the country.</p>
<h2 id="machine-learning">Machine Learning</h2>
<p>This talk is a general introduction to machine learning, we will
highlight the technical challenges and the current solutions. We will
give an overview of what is machine learning and why it is
important.</p>
<h2 id="rise-of-machine-learning">Rise of Machine Learning</h2>
<p>Machine learning is the combination of data and models, through
computation, to make predictions. <span class="math display">\[
\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow}
\text{prediction}
\]</span></p>
<h2 id="data-revolution">Data Revolution</h2>
Machine learning has risen in prominence due to the rise in data
availability, and its interconnection with computers. The high bandwidth
connection between data and computer leads to a new interaction between
us and data via the computer. It is that channel that is being mediated
by machine learning techniques.
<div class="figure">
<div id="data-science-information-flow-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//data-science/new-flow-of-information.svg" width="60%" style=" ">
</object>
</div>
<div id="data-science-information-flow-magnify" class="magnify"
onclick="magnifyFigure(&#39;data-science-information-flow&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="data-science-information-flow-caption" class="caption-frame">
<p>Figure: Large amounts of data and high interconnection bandwidth mean
that we receive much of our information about the world around us
through computers.</p>
</div>
</div>
<h2 id="supply-chain">Supply Chain</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/supply-chain.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/supply-chain.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="packhorse-bridge-burbage-brook-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//supply-chain/packhorse-bridge-burbage-brook.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="packhorse-bridge-burbage-brook-magnify" class="magnify"
onclick="magnifyFigure(&#39;packhorse-bridge-burbage-brook&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="packhorse-bridge-burbage-brook-caption" class="caption-frame">
<p>Figure: Packhorse Bridge under Burbage Edge. This packhorse route
climbs steeply out of Hathersage and heads towards Sheffield. Packhorses
were the main route for transporting goods across the Peak District. The
high cost of transport is one driver of the ‘smith’ model, where there
is a local skilled person responsible for assembling or creating goods
(e.g. a blacksmith).</p>
</div>
</div>
<p>On Sunday mornings in Sheffield, I often used to run across Packhorse
Bridge in Burbage valley. The bridge is part of an ancient network of
trails crossing the Pennines that, before Turnpike roads arrived in the
18th century, was the main way in which goods were moved. Given that the
moors around Sheffield were home to sand quarries, tin mines, lead mines
and the villages in the Derwent valley were known for nail and pin
manufacture, this wasn’t simply movement of agricultural goods, but it
was the infrastructure for industrial transport.</p>
<p>The profession of leading the horses was known as a Jagger and
leading out of the village of Hathersage is Jagger’s Lane, a trail that
headed underneath Stanage Edge and into Sheffield.</p>
<p>The movement of goods from regions of supply to areas of demand is
fundamental to our society. The physical infrastructure of supply chain
has evolved a great deal over the last 300 years.</p>
<h2 id="cromford">Cromford</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/cromford.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/cromford.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="cromford-mill-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//supply-chain/cromford-mill.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="cromford-mill-magnify" class="magnify"
onclick="magnifyFigure(&#39;cromford-mill&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="cromford-mill-caption" class="caption-frame">
<p>Figure: Richard Arkwright is regarded of the founder of the modern
factory system. Factories exploit distribution networks to centralize
production of goods. Arkwright located his factory in Cromford due to
proximity to Nottingham Weavers (his market) and availability of water
power from the tributaries of the Derwent river. When he first arrived
there was almost no transportation network. Over the following 200 years
The Cromford Canal (1790s), a Turnpike (now the A6, 1816-18) and the
High Peak Railway (now closed, 1820s) were all constructed to improve
transportation access as the factory blossomed.</p>
</div>
</div>
<p>Richard Arkwright is known as the father of the modern factory
system. In 1771 he set up a <a
href="https://en.wikipedia.org/wiki/Cromford_Mill">Mill</a> for spinning
cotton yarn in the village of Cromford, in the Derwent Valley. The
Derwent valley is relatively inaccessible. Raw cotton arrived in
Liverpool from the US and India. It needed to be transported on
packhorse across the bridleways of the Pennines. But Cromford was a good
location due to proximity to Nottingham, where weavers where consuming
the finished thread, and the availability of water power from small
tributaries of the Derwent river for Arkwright’s <a
href="https://en.wikipedia.org/wiki/Spinning_jenny">water frames</a>
which automated the production of yarn from raw cotton.</p>
<p>By 1794 the <a
href="https://en.wikipedia.org/wiki/Cromford_Canal">Cromford Canal</a>
was opened to bring coal in to Cromford and give better transport to
Nottingham. The construction of the canals was driven by the need to
improve the transport infrastructure, facilitating the movement of goods
across the UK. Canals, roads and railways were initially constructed by
the economic need for moving goods. To improve supply chain.</p>
<p>The A6 now does pass through Cromford, but at the time he moved there
there was merely a track. The High Peak Railway was opened in 1832, it
is now converted to the High Peak Trail, but it remains the highest
railway built in Britain.</p>
<p><span class="citation" data-cites="Cooper:transformation91">Cooper
(1991)</span></p>
<h2 id="containerization">Containerization</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/containerisation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/containerisation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="container-2539942_1920-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//supply-chain/container-2539942_1920.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="container-2539942_1920-magnify" class="magnify"
onclick="magnifyFigure(&#39;container-2539942_1920&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="container-2539942_1920-caption" class="caption-frame">
<p>Figure: The container is one of the major drivers of globalization,
and arguably the largest agent of social change in the last 100 years.
It reduces the cost of transportation, significantly changing the
appropriate topology of distribution networks. The container makes it
possible to ship goods halfway around the world for cheaper than it
costs to process those goods, leading to an extended distribution
topology.</p>
</div>
</div>
<p>Containerization has had a dramatic effect on global economics,
placing many people in the developing world at the end of the supply
chain.</p>
<div class="figure">
<div id="wild-alaskan-cod-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//supply-chain/wild-alaskan-cod.jpg" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//supply-chain/wild-alaskan-cod-made-in-china.jpg" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="wild-alaskan-cod-magnify" class="magnify"
onclick="magnifyFigure(&#39;wild-alaskan-cod&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="wild-alaskan-cod-caption" class="caption-frame">
<p>Figure: Wild Alaskan Cod, being solid in the Pacific Northwest, that
is a product of China. It is cheaper to ship the deep frozen fish
thousands of kilometers for processing than to process locally.</p>
</div>
</div>
<p>For example, you can buy Wild Alaskan Cod fished from Alaska,
processed in China, sold in North America. This is driven by the low
cost of transport for frozen cod vs the higher relative cost of cod
processing in the US versus China. Similarly,
<a href="https://www.telegraph.co.uk/news/uknews/1534286/12000-mile-trip-to-have-seafood-shelled.html" target="_blank">Scottish
prawns are also processed in China for sale in the UK.</a></p>
<div class="figure">
<div id="environmental-impact-of-food-by-life-cycle-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//supply-chain/environmental-impact-of-food-by-life-cycle.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="environmental-impact-of-food-by-life-cycle-magnify"
class="magnify"
onclick="magnifyFigure(&#39;environmental-impact-of-food-by-life-cycle&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="environmental-impact-of-food-by-life-cycle-caption"
class="caption-frame">
<p>Figure: The transport cost of most foods is a very small portion of
the total cost. The exception is if foods are air freighted. Source: <a
href="https://ourworldindata.org/food-choice-vs-eating-local"
class="uri">https://ourworldindata.org/food-choice-vs-eating-local</a>
by Hannah Ritche CC-BY</p>
</div>
</div>
<p>This effect on cost of transport vs cost of processing is the main
driver of the topology of the modern supply chain and the associated
effect of globalization. If transport is much cheaper than processing,
then processing will tend to agglomerate in places where processing
costs can be minimized.</p>
<p>Large scale global economic change has principally been driven by
changes in the technology that drives supply chain.</p>
<p>Supply chain is a large-scale automated decision making network. Our
aim is to make decisions not only based on our models of customer
behavior (as observed through data), but also by accounting for the
structure of our fulfilment center, and delivery network.</p>
<p>Many of the most important questions in supply chain take the form of
counterfactuals. E.g. “What would happen if we opened a manufacturing
facility in Cambridge?” A counter factual is a question that implies a
mechanistic understanding of a system. It goes beyond simple smoothness
assumptions or translation invariants. It requires a physical, or
<em>mechanistic</em> understanding of the supply chain network. For this
reason, the type of models we deploy in supply chain often involve
simulations or more mechanistic understanding of the network.</p>
<p>In supply chain Machine Learning alone is not enough, we need to
bridge between models that contain real mechanisms and models that are
entirely data driven.</p>
<p>This is challenging, because as we introduce more mechanism to the
models we use, it becomes harder to develop efficient algorithms to
match those models to data.</p>
<h2 id="for-africa">For Africa</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/supply-chain-africa.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_supply-chain/includes/supply-chain-africa.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>There is a large opportunity because infrastructures around
automation are moving from physical infrastructure towards information
infrastructures. How can African countries benefit from a modern
information infrastructure? The aim of Data Science Africa is to answer
this question, with the answers coming from the attendees.</p>
<p>Machine learning aims to replicate processes through the direct use
of data. When deployed in the domain of ‘artificial intelligence’, the
processes that it is replicating, or <em>emulating</em>, are cognitive
processes.</p>
<p>The first trick in machine learning is to convert the process itself
into a <em>mathematical function</em>. That function has a set of
parameters which control its behaviour. What we call learning is the
adaption of these parameters to change the behavior of the function. The
choice of mathematical function we use is a vital component of the
model.</p>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//health/Kapchorwa_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kapchorwa-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kapchorwa-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kapchorwa-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kapchorwa District, home district of Stephen
Kiprotich.</p>
</div>
</div>
<p>Stephen Kiprotich, the 2012 gold medal winner from the London
Olympics, comes from Kapchorwa district, in eastern Uganda, near the
border with Kenya.</p>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression
modelling. The data consists of the pace of Olympic Gold Medal Marathon
winners for the Olympics from 1896 to present. Let’s load in the data
and plot.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1896.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in that
year the Olympics was in St Louis, USA. Organizational problems and
challenges with dust kicked up by the cars following the race meant that
participants got lost, and only very few participants completed. More
recent years see more consistently quick marathons.</p>
<h2 id="polynomial-fits-to-olympic-marthon-data">Polynomial Fits to
Olympic Marthon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-polynomial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-polynomial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<p>Define the polynomial basis function.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> polynomial</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> polynomial(x, num_basis<span class="op">=</span><span class="dv">4</span>, data_limits<span class="op">=</span>[<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>]):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Polynomial basis&quot;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    centre <span class="op">=</span> data_limits[<span class="dv">0</span>]<span class="op">/</span><span class="fl">2.</span> <span class="op">+</span> data_limits[<span class="dv">1</span>]<span class="op">/</span><span class="fl">2.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    span <span class="op">=</span> data_limits[<span class="dv">1</span>] <span class="op">-</span> data_limits[<span class="dv">0</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.asarray(x, dtype<span class="op">=</span><span class="bu">float</span>) <span class="op">-</span> centre</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>z<span class="op">/</span>span   <span class="co"># scale the inputs to be within -1, 1 where polynomials are well behaved</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> np.zeros((x.shape[<span class="dv">0</span>], num_basis))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_basis):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        Phi[:, i:i<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> z<span class="op">**</span>i</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Phi</span></code></pre></div>
<p>Now we include the solution for the linear regression through
QR-decomposition.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> basis_fit(Phi, y):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Use QR decomposition to fit the basis.&quot;</span><span class="st">&quot;&quot;</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    Q, R <span class="op">=</span> np.linalg.qr(Phi)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sp.linalg.solve_triangular(R, Q.T<span class="op">@</span>y) </span></code></pre></div>
<h2 id="linear-fit">Linear Fit</h2>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">2</span>, <span class="co"># two basis functions (1 and x)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<p>Now we make some predictions for the fit.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>x_pred <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">400</span>)[:, np.newaxis]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic-marathon-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-2-caption" class="caption-frame">
<p>Figure: Fit of a 1-degree polynomial (a linear model) to the Olympic
marathon data.</p>
</div>
</div>
<h2 id="cubic-fit">Cubic Fit</h2>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">4</span>, <span class="co"># four basis: 1, x, x^2, x^3</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic-marathon-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-4-caption" class="caption-frame">
<p>Figure: Fit of a 3-degree polynomial (a cubic model) to the Olympic
marathon data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit">9th Degree Polynomial Fit</h2>
<p>Now we’ll try a 9th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">10</span>, <span class="co"># basis up to x^9</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic-marathon-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-10-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-10&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-10-caption" class="caption-frame">
<p>Figure: Fit of a 9-degree polynomial to the Olympic marathon
data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit-1">16th Degree Polynomial Fit</h2>
<p>Now we’ll try a 16th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">17</span>, <span class="co"># basis up to x^16</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic-marathon-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-17-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-17&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-17-caption" class="caption-frame">
<p>Figure: Fit of a 16-degree polynomial to the Olympic marathon
data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit-2">26th Degree Polynomial Fit</h2>
<p>Now we’ll try a 26th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">27</span>, <span class="co"># basis up to x^26</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-27-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic-marathon-polynomial-27.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-27-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-27&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-27-caption" class="caption-frame">
<p>Figure: Fit of a 26-degree polynomial to the Olympic marathon
data.</p>
</div>
</div>
<h2 id="what-does-machine-learning-do">What does Machine Learning
do?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-does-machine-learning-do.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-does-machine-learning-do.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Any process of automation allows us to scale what we do by codifying
a process in some way that makes it efficient and repeatable. Machine
learning automates by emulating human (or other actions) found in data.
Machine learning codifies in the form of a mathematical function that is
learnt by a computer. If we can create these mathematical functions in
ways in which they can interconnect, then we can also build systems.</p>
<p>Machine learning works through codifying a prediction of interest
into a mathematical function. For example, we can try and predict the
probability that a customer wants to by a jersey given knowledge of
their age, and the latitude where they live. The technique known as
logistic regression estimates the odds that someone will by a jumper as
a linear weighted sum of the features of interest.</p>
<p><span class="math display">\[ \text{odds} =
\frac{p(\text{bought})}{p(\text{not bought})} \]</span></p>
<p><span class="math display">\[ \log \text{odds}  = w_0 + w_1
\text{age} + w_2 \text{latitude}.\]</span> Here <span
class="math inline">\(w_0\)</span>, <span
class="math inline">\(w_1\)</span> and <span
class="math inline">\(w_2\)</span> are the parameters of the model. If
<span class="math inline">\(w_1\)</span> and <span
class="math inline">\(w_2\)</span> are both positive, then the log-odds
that someone will buy a jumper increase with increasing latitude and
age, so the further north you are and the older you are the more likely
you are to buy a jumper. The parameter <span
class="math inline">\(w_0\)</span> is an offset parameter and gives the
log-odds of buying a jumper at zero age and on the equator. It is likely
to be negative<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> indicating that the purchase is
odds-against. This is also a classical statistical model, and models
like logistic regression are widely used to estimate probabilities from
ad-click prediction to disease risk.</p>
<p>This is called a generalized linear model, we can also think of it as
estimating the <em>probability</em> of a purchase as a nonlinear
function of the features (age, latitude) and the parameters (the <span
class="math inline">\(w\)</span> values). The function is known as the
<em>sigmoid</em> or <a
href="https://en.wikipedia.org/wiki/Logistic_regression">logistic
function</a>, thus the name <em>logistic</em> regression.</p>
<h3 id="sigmoid-function">Sigmoid Function</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/sigmoid-function.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/sigmoid-function.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-logistic-function-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/logistic.svg" width="80%" style=" ">
</object>
</div>
<div id="the-logistic-function-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-logistic-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-logistic-function-caption" class="caption-frame">
<p>Figure: The logistic function.</p>
</div>
</div>
<p>The function has this characeristic ‘s’-shape (from where the term
sigmoid, as in sigma, comes from). It also takes the input from the
entire real line and ‘squashes’ it into an output that is between zero
and one. For this reason it is sometimes also called a ‘squashing
function’.</p>
<p>The sigmoid comes from the inverting the odds ratio, <span
class="math display">\[
\frac{\pi}{(1-\pi)}
\]</span> where <span class="math inline">\(\pi\)</span> is the
probability of a positive outcome and <span
class="math inline">\(1-\pi\)</span> is the probability of a negative
outcome</p>
<p><span class="math display">\[ p(\text{bought}) =  \sigma\left(w_0 +
w_1 \text{age} + w_2 \text{latitude}\right).\]</span></p>
<p>In the case where we have <em>features</em> to help us predict, we
sometimes denote such features as a vector, <span
class="math inline">\(\mathbf{ x}\)</span>, and we then use an inner
product between the features and the parameters, <span
class="math inline">\(\mathbf{ w}^\top \mathbf{ x}= w_1 x_1 + w_2 x_2 +
w_3 x_3 ...\)</span>, to represent the argument of the sigmoid.</p>
<p><span class="math display">\[ p(\text{bought})
=  \sigma\left(\mathbf{ w}^\top \mathbf{ x}\right).\]</span> More
generally, we aim to predict some aspect of our data, <span
class="math inline">\(y\)</span>, by relating it through a mathematical
function, <span class="math inline">\(f(\cdot)\)</span>, to the
parameters, <span class="math inline">\(\mathbf{ w}\)</span> and the
data, <span class="math inline">\(\mathbf{ x}\)</span>.</p>
<p><span class="math display">\[ y=  f\left(\mathbf{ x}, \mathbf{
w}\right).\]</span> We call <span
class="math inline">\(f(\cdot)\)</span> the <em>prediction
function</em>.</p>
<p>To obtain the fit to data, we use a separate function called the
<em>objective function</em> that gives us a mathematical representation
of the difference between our predictions and the real data.</p>
<p><span class="math display">\[E(\mathbf{ w}, \mathbf{Y},
\mathbf{X})\]</span> A commonly used examples (for example in a
regression problem) is least squares, <span
class="math display">\[E(\mathbf{ w}, \mathbf{Y}, \mathbf{X}) =
\sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i, \mathbf{
w})\right)^2.\]</span></p>
<p>If a linear prediction function is combined with the least squares
objective function, then that gives us a classical <em>linear
regression</em>, another classical statistical model. Statistics often
focusses on linear models because it makes interpretation of the model
easier. Interpretation is key in statistics because the aim is normally
to validate questions by analysis of data. Machine learning has
typically focused more on the prediction function itself and worried
less about the interpretation of parameters. In statistics, where
interpretation is typically more important than prediction, parameters
are normally denoted by <span
class="math inline">\(\boldsymbol{\beta}\)</span> instead of <span
class="math inline">\(\mathbf{ w}\)</span>.</p>
<p>A key difference between statistics and machine learning, is that
(traditionally) machine learning has focussed on predictive capability
and statistics has focussed on interpretability. That means that in a
statistics class far more emphasis will be placed on interpretation of
the parameters. In machine learning, the parameters, $, are just a means
to an end. But in statistics, when we denote the parameters by <span
class="math inline">\(\boldsymbol{\beta}\)</span>, we often use the
parameters to tell us something about the disease.</p>
<p>So we move between <span class="math display">\[ p(\text{bought})
=  \sigma\left(w_0 + w_1 \text{age} + w_2
\text{latitude}\right).\]</span></p>
<p>to denote the emphasis is on predictive power to</p>
<p><span class="math display">\[ p(\text{bought}) =  \sigma\left(\beta_0
+ \beta_1 \text{age} + \beta_2 \text{latitude}\right).\]</span></p>
<p>to denote the emphasis is on interpretation of the parameters.</p>
<p>Another effect of the focus on prediction in machine learning is that
<em>non-linear</em> approaches, which can be harder to interpret, are
more widely deployedin machine learning – they tend to improve quality
of predictions at the expense of interpretability.</p>
<h2 id="what-is-machine-learning">What is Machine Learning?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-2.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-2.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Machine learning allows us to extract knowledge from data to form a
prediction.</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>A machine learning prediction is made by combining a model with data
to form the prediction. The manner in which this is done gives us the
machine learning <em>algorithm</em>.</p>
<p>Machine learning models are <em>mathematical models</em> which make
weak assumptions about data, e.g. smoothness assumptions. By combining
these assumptions with the data, we observe we can interpolate between
data points or, occasionally, extrapolate into the future.</p>
<p>Machine learning is a technology which strongly overlaps with the
methodology of statistics. From a historical/philosophical view point,
machine learning differs from statistics in that the focus in the
machine learning community has been primarily on accuracy of prediction,
whereas the focus in statistics is typically on the interpretability of
a model and/or validating a hypothesis through data collection.</p>
<p>The rapid increase in the availability of compute and data has led to
the increased prominence of machine learning. This prominence is
surfacing in two different but overlapping domains: data science and
artificial intelligence.</p>
<h2 id="from-model-to-decision">From Model to Decision</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-end-to-end.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-end-to-end.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The real challenge, however, is end-to-end decision making. Taking
information from the environment and using it to drive decision making
to achieve goals.</p>
<h2 id="artificial-intelligence-and-data-science">Artificial
Intelligence and Data Science</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/ai-vs-data-science-2.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/ai-vs-data-science-2.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Artificial intelligence has the objective of endowing computers with
human-like intelligent capabilities. For example, understanding an image
(computer vision) or the contents of some speech (speech recognition),
the meaning of a sentence (natural language processing) or the
translation of a sentence (machine translation).</p>
<h3 id="supervised-learning-for-ai">Supervised Learning for AI</h3>
<p>The machine learning approach to artificial intelligence is to
collect and annotate a large data set from humans. The problem is
characterized by input data (e.g. a particular image) and a label
(e.g. is there a car in the image yes/no). The machine learning
algorithm fits a mathematical function (I call this the <em>prediction
function</em>) to map from the input image to the label. The parameters
of the prediction function are set by minimizing an error between the
function’s predictions and the true data. This mathematical function
that encapsulates this error is known as the <em>objective
function</em>.</p>
<p>This approach to machine learning is known as <em>supervised
learning</em>. Various approaches to supervised learning use different
prediction functions, objective functions or different optimization
algorithms to fit them.</p>
<p>For example, <em>deep learning</em> makes use of <em>neural
networks</em> to form the predictions. A neural network is a particular
type of mathematical function that allows the algorithm designer to
introduce invariances into the function.</p>
<p>An invariance is an important way of including prior understanding in
a machine learning model. For example, in an image, a car is still a car
regardless of whether it’s in the upper left or lower right corner of
the image. This is known as translation invariance. A neural network
encodes translation invariance in <em>convolutional layers</em>.
Convolutional neural networks are widely used in image recognition
tasks.</p>
<p>An alternative structure is known as a recurrent neural network
(RNN). RNNs neural networks encode temporal structure. They use auto
regressive connections in their hidden layers, they can be seen as time
series models which have non-linear auto-regressive basis functions.
They are widely used in speech recognition and machine translation.</p>
<p>Machine learning has been deployed in Speech Recognition (e.g. Alexa,
deep neural networks, convolutional neural networks for speech
recognition), in computer vision (e.g. Amazon Go, convolutional neural
networks for person recognition and pose detection).</p>
<p>The field of data science is related to AI, but philosophically
different. It arises because we are increasingly creating large amounts
of data through <em>happenstance</em> rather than active collection. In
the modern era data is laid down by almost all our activities. The
objective of data science is to extract insights from this data.</p>
<p>Classically, in the field of statistics, data analysis proceeds by
assuming that the question (or scientific hypothesis) comes before the
data is created. E.g., if I want to determine the effectiveness of a
particular drug, I perform a <em>design</em> for my data collection. I
use foundational approaches such as randomization to account for
confounders. This made a lot of sense in an era where data had to be
actively collected. The reduction in cost of data collection and storage
now means that many data sets are available which weren’t collected with
a particular question in mind. This is a challenge because bias in the
way data was acquired can corrupt the insights we derive. We can perform
randomized control trials (or A/B tests) to verify our conclusions, but
the opportunity is to use data science techniques to better guide our
question selection or even answer a question without the expense of a
full randomized control trial (referred to as A/B testing in modern
internet parlance).</p>
<h2 id="neural-networks-and-prediction-functions">Neural Networks and
Prediction Functions</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/neural-networks.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/neural-networks.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Neural networks are adaptive non-linear function models. Originally,
they were studied (by McCulloch and Pitts <span class="citation"
data-cites="McCulloch:neuron43">(McCulloch and Pitts, 1943)</span>) as
simple models for neurons, but over the last decade they have become
popular because they are a flexible approach to modelling complex data.
A particular characteristic of neural network models is that they can be
composed to form highly complex functions which encode many of our
expectations of the real world. They allow us to encode our assumptions
about how the world works.</p>
<p>We will return to composition later, but for the moment, let’s focus
on a one hidden layer neural network. We are interested in the
prediction function, so we’ll ignore the objective function (which is
often called an error function) for the moment, and just describe the
mathematical object of interest</p>
<p><span class="math display">\[
f(\mathbf{ x}) = \mathbf{W}^\top \boldsymbol{ \phi}(\mathbf{V}, \mathbf{
x})
\]</span></p>
<p>Where in this case <span class="math inline">\(f(\cdot)\)</span> is a
scalar function with vector inputs, and <span
class="math inline">\(\boldsymbol{ \phi}(\cdot)\)</span> is a vector
function with vector inputs. The dimensionality of the vector function
is known as the number of hidden units, or the number of neurons. The
elements of this vector function are known as the <em>activation</em>
function of the neural network and <span
class="math inline">\(\mathbf{V}\)</span> are the parameters of the
activation functions.</p>
<h2 id="relations-with-classical-statistics">Relations with Classical
Statistics</h2>
<p>In statistics activation functions are traditionally known as
<em>basis functions</em>. And we would think of this as a <em>linear
model</em>. It’s doesn’t make linear predictions, but it’s linear
because in statistics estimation focuses on the parameters, <span
class="math inline">\(\mathbf{W}\)</span>, not the parameters, <span
class="math inline">\(\mathbf{V}\)</span>. The linear model terminology
refers to the fact that the model is <em>linear in the parameters</em>,
but it is <em>not</em> linear in the data unless the activation
functions are chosen to be linear.</p>
<h2 id="adaptive-basis-functions">Adaptive Basis Functions</h2>
<p>The first difference in the (early) neural network literature to the
classical statistical literature is the decision to optimize these
parameters, <span class="math inline">\(\mathbf{V}\)</span>, as well as
the parameters, <span class="math inline">\(\mathbf{W}\)</span> (which
would normally be denoted in statistics by <span
class="math inline">\(\boldsymbol{\beta}\)</span>)<a href="#fn2"
class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>.</p>
<h2 id="machine-learning-1">Machine Learning</h2>
<p>The key idea in machine learning is to observe the system in
practice, and then emulate its behavior with mathematics. That leads to
a design challenge as to where to place the mathematical function. The
placement of the mathematical function leads to the different domains of
machine learning.</p>
<ol type="1">
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Reinforcement learning</li>
</ol>
<h1 id="supervised-learning">Supervised Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/supervised-learning.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/supervised-learning.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Supervised learning is one of the most widely deployed machine
learning technologies, and a particular domain of success has been
<em>classification</em>. Classification is the process of taking an
input (which might be an image) and categorizing it into one of a number
of different classes (e.g. dog or cat). This simple idea underpins a lot
of machine learning. By scanning across the image we can also determine
where the animal is in the image.</p>
<h2 id="introduction-to-classification">Introduction to
Classification</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classification is perhaps the technique most closely assocated with
machine learning. In the speech based agents, on-device classifiers are
used to determine when the wake word is used. A wake word is a word that
wakes up the device. For the Amazon Echo it is “Alexa”, for Siri it is
“Hey Siri”. Once the wake word detected with a classifier, the speech
can be uploaded to the cloud for full processing, the speech recognition
stages.</p>
<p>This isn’t just useful for intelligent agents, the UN global pulse
project on public discussion on radio also uses <a
href="https://radio.unglobalpulse.net/uganda/">wake word detection for
recording radio conversations</a>.</p>
<p>A major breakthrough in image classification came in 2012 with the
ImageNet result of <a
href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-">Alex
Krizhevsky, Ilya Sutskever and Geoff Hinton</a> from the University of
Toronto. ImageNet is a large data base of 14 million images with many
thousands of classes. The data is used in a community-wide challenge for
object categorization. Krizhevsky et al used convolutional neural
networks to outperform all previous approaches on the challenge. They
formed a company which was purchased shortly after by Google. This
challenge, known as object categorisation, was a major obstacle for
practical computer vision systems. Modern object categorization systems
are close to human performance.</p>
<p>Machine learning problems normally involve a prediction function and
an objective function. Regression is the case where the prediction
function iss over the real numbers, so the codomain of the functions,
<span class="math inline">\(f(\mathbf{X})\)</span> was the real numbers
or sometimes real vectors. The classification problem consists of
predicting whether or not a particular example is a member of a
particular class. So we may want to know if a particular image
represents a digit 6 or if a particular user will click on a given
advert. These are classification problems, and they require us to map to
<em>yes</em> or <em>no</em> answers. That makes them naturally discrete
mappings.</p>
<p>In classification we are given an input vector, <span
class="math inline">\(\mathbf{ x}\)</span>, and an associated label,
<span class="math inline">\(y\)</span> which either takes the value
<span class="math inline">\(-1\)</span> to represent <em>no</em> or
<span class="math inline">\(1\)</span> to represent <em>yes</em>.</p>
<p>In supervised learning the inputs, <span
class="math inline">\(\mathbf{ x}\)</span>, are mapped to a label, <span
class="math inline">\(y\)</span>, through a function <span
class="math inline">\(f(\cdot)\)</span> that is dependent on a set of
parameters, <span class="math inline">\(\mathbf{ w}\)</span>, <span
class="math display">\[
y= f(\mathbf{ x}; \mathbf{ w}).
\]</span> The function <span class="math inline">\(f(\cdot)\)</span> is
known as the <em>prediction function</em>. The key challenges are (1)
choosing which features, <span class="math inline">\(\mathbf{
x}\)</span>, are relevant in the prediction, (2) defining the
appropriate <em>class of function</em>, <span
class="math inline">\(f(\cdot)\)</span>, to use and (3) selecting the
right parameters, <span class="math inline">\(\mathbf{ w}\)</span>.</p>
<h2 id="classification-examples">Classification Examples</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-examples.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-examples.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li>Classifiying hand written digits from binary images (automatic zip
code reading)</li>
<li>Detecting faces in images (e.g. digital cameras).</li>
<li>Who a detected face belongs to (e.g. Facebook, DeepFace)</li>
<li>Classifying type of cancer given gene expression data.</li>
<li>Categorization of document types (different types of news article on
the internet)</li>
</ul>
<div class="figure">
<div id="the-perceptron-algorithm-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/perceptron001.svg" width="80%" style=" ">
</object>
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/perceptron044.svg" width="80%" style=" ">
</object>
</div>
<div id="the-perceptron-algorithm-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-perceptron-algorithm&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-perceptron-algorithm-caption" class="caption-frame">
<p>Figure: The perceptron algorithm.</p>
</div>
</div>
<h2 id="logistic-regression">Logistic Regression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>A logistic regression is an approach to classification which extends
the linear basis function models we’ve already explored. Rather than
modeling the output of the function directly the assumption is that we
model the <em>log-odds</em> with the basis functions.</p>
<p>The <a href="http://en.wikipedia.org/wiki/Odds">odds</a> are defined
as the ratio of the probability of a positive outcome, to the
probability of a negative outcome. If the probability of a positive
outcome is denoted by <span class="math inline">\(\pi\)</span>, then the
odds are computed as <span
class="math inline">\(\frac{\pi}{1-\pi}\)</span>. Odds are widely used
by <a href="http://en.wikipedia.org/wiki/Bookmaker">bookmakers</a> in
gambling, although a bookmakers odds won’t normalise: i.e. if you look
at the equivalent probabilities, and sum over the probability of all
outcomes the bookmakers are considering, then you won’t get one. This is
how the bookmaker makes a profit. Because a probability is always
between zero and one, the odds are always between <span
class="math inline">\(0\)</span> and <span
class="math inline">\(\infty\)</span>. If the positive outcome is
unlikely the odds are close to zero, if it is very likely then the odds
become close to infinite. Taking the logarithm of the odds maps the odds
from the positive half space to being across the entire real line. Odds
that were between 0 and 1 (where the negative outcome was more likely)
are mapped to the range between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(0\)</span>. Odds that are greater than 1 are
mapped to the range between <span class="math inline">\(0\)</span> and
<span class="math inline">\(\infty\)</span>. Considering the log odds
therefore takes a number between 0 and 1 (the probability of positive
outcome) and maps it to the entire real line. The function that does
this is known as the <a href="http://en.wikipedia.org/wiki/Logit">logit
function</a>, <span class="math inline">\(g^{-1}(p_i) =
\log\frac{p_i}{1-p_i}\)</span>. This function is known as a <em>link
function</em>.</p>
<p>For a standard regression we take, <span class="math display">\[
f(\mathbf{ x}) = \mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}),
\]</span> if we want to perform classification we perform a logistic
regression. <span class="math display">\[
\log \frac{\pi}{(1-\pi)} = \mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})
\]</span> where the odds ratio between the positive class and the
negative class is given by <span class="math display">\[
\frac{\pi}{(1-\pi)}
\]</span> The odds can never be negative, but can take any value from 0
to <span class="math inline">\(\infty\)</span>. We have defined the link
function as taking the form <span
class="math inline">\(g^{-1}(\cdot)\)</span> implying that the inverse
link function is given by <span class="math inline">\(g(\cdot)\)</span>.
Since we have defined, <span class="math display">\[
g^{-1}(\pi) =
\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})
\]</span> we can write <span class="math inline">\(\pi\)</span> in terms
of the <em>inverse link</em> function, <span
class="math inline">\(g(\cdot)\)</span> as <span class="math display">\[
\pi = g(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})).
\]</span></p>
<h2 id="basis-function">Basis Function</h2>
<p>We’ll define our prediction, objective and gradient functions below.
But before we start, we need to define a basis function for our model.
Let’s start with the linear basis.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> linear</span></code></pre></div>
<h2 id="prediction-function">Prediction Function</h2>
<p>Now we have the basis function let’s define the prediction
function.</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(w, x, basis<span class="op">=</span>linear, <span class="op">**</span>kwargs):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Generates the prediction function and the basis matrix.&quot;</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> basis(x, <span class="op">**</span>kwargs)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.dot(Phi, w)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>f)), Phi</span></code></pre></div>
<p>This inverse of the link function is known as the <a
href="http://en.wikipedia.org/wiki/Logistic_function">logistic</a> (thus
the name logistic regression) or sometimes it is called the sigmoid
function. For a particular value of the input to the link function,
<span class="math inline">\(f_i = \mathbf{ w}^\top \boldsymbol{
\phi}(\mathbf{ x}_i)\)</span> we can plot the value of the inverse link
function as below.</p>
<p>By replacing the inverse link with the sigmoid we can write <span
class="math inline">\(\pi\)</span> as a function of the input and the
parameter vector as, <span class="math display">\[
\pi(\mathbf{ x},\mathbf{ w}) = \frac{1}{1+\exp\left(-\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})\right)}.
\]</span> The process for logistic regression is as follows. Compute the
output of a standard linear basis function composition (<span
class="math inline">\(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\)</span>, as we did for linear regression) and then apply the
inverse link function, <span class="math inline">\(g(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}))\)</span>. In logistic regression this
involves <em>squashing</em> it with the logistic (or sigmoid) function.
Use this value, which now has an interpretation as a
<em>probability</em> in a Bernoulli distribution to form the likelihood.
Then we can assume conditional independence of each data point given the
parameters and develop a likelihod for the entire data set.</p>
<p>As we discussed last time, the Bernoulli likelihood is of the form,
<span class="math display">\[
P(y_i|\mathbf{ w}, \mathbf{ x}) =
\pi_i^{y_i} (1-\pi_i)^{1-y_i}
\]</span> which we can think of as clever trick for mathematically
switching between two probabilities if we were to write it as code it
would be better described as</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli(x, y, pi):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pi(x)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>pi(x)</span></code></pre></div>
<p>but writing it mathematically makes it easier to write our objective
function within a single mathematical equation.</p>
<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<p>To obtain the parameters of the model, we need to maximize the
likelihood, or minimize the objective function, normally taken to be the
negative log likelihood. With a data conditional independence assumption
the likelihood has the form, <span class="math display">\[
P(\mathbf{ y}|\mathbf{ w},
\mathbf{X}) = \prod_{i=1}^nP(y_i|\mathbf{ w}, \mathbf{ x}_i).
\]</span> which can be written as a log likelihood in the form <span
class="math display">\[
\log P(\mathbf{ y}|\mathbf{ w},
\mathbf{X}) = \sum_{i=1}^n\log P(y_i|\mathbf{ w}, \mathbf{ x}_i) =
\sum_{i=1}^n
y_i \log \pi_i + \sum_{i=1}^n(1-y_i)\log (1-\pi_i)
\]</span> and if we take the probability of positive outcome for the
<span class="math inline">\(i\)</span>th data point to be given by <span
class="math display">\[
\pi_i = g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x}_i)\right),
\]</span> where <span class="math inline">\(g(\cdot)\)</span> is the
<em>inverse</em> link function, then this leads to an objective function
of the form, <span class="math display">\[
E(\mathbf{ w}) = -  \sum_{i=1}^ny_i \log
g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\right) -
\sum_{i=1}^n(1-y_i)\log \left(1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}_i)\right)\right).
\]</span></p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(g, y):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Computes the objective function.&quot;</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    labs <span class="op">=</span> np.asarray(y, dtype<span class="op">=</span><span class="bu">float</span>).flatten()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    posind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    negind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">0</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(g[posind, :]).<span class="bu">sum</span>() <span class="op">-</span> np.log(<span class="dv">1</span><span class="op">-</span>g[negind, :]).<span class="bu">sum</span>()</span></code></pre></div>
<p>As normal, we would like to minimize this objective. This can be done
by differentiating with respect to the parameters of our prediction
function, <span class="math inline">\(\pi(\mathbf{ x};\mathbf{
w})\)</span>, for optimisation. The gradient of the likelihood with
respect to <span class="math inline">\(\pi(\mathbf{ x};\mathbf{
w})\)</span> is of the form, <span class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = -\sum_{i=1}^n
\frac{y_i}{g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{
x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i) +  \sum_{i=1}^n
\frac{1-y_i}{1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{
x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i)
\]</span> where we used the chain rule to develop the derivative in
terms of <span
class="math inline">\(\frac{\text{d}g(f_i)}{\text{d}f_i}\)</span>, which
is the gradient of the inverse link function (in our case the gradient
of the sigmoid function).</p>
<p>So the objective function now depends on the gradient of the inverse
link function, as well as the likelihood depends on the gradient of the
inverse link function, as well as the gradient of the log likelihood,
and naturally the gradient of the argument of the inverse link function
with respect to the parameters, which is simply <span
class="math inline">\(\boldsymbol{ \phi}(\mathbf{ x}_i)\)</span>.</p>
<p>The only missing term is the gradient of the inverse link function.
For the sigmoid squashing function we have, <span
class="math display">\[\begin{align*}
g(f_i) &amp;= \frac{1}{1+\exp(-f_i)}\\
&amp;=(1+\exp(-f_i))^{-1}
\end{align*}\]</span> and the gradient can be computed as <span
class="math display">\[\begin{align*}
\frac{\text{d}g(f_i)}{\text{d} f_i} &amp; =
\exp(-f_i)(1+\exp(-f_i))^{-2}\\
&amp; = \frac{1}{1+\exp(-f_i)}
\frac{\exp(-f_i)}{1+\exp(-f_i)} \\
&amp; = g(f_i) (1-g(f_i))
\end{align*}\]</span> so the full gradient can be written down as <span
class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = -\sum_{i=1}^n
y_i\left(1-g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i) +  \sum_{i=1}^n
(1-y_i)\left(g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i).
\]</span></p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient(g, Phi, y):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Generates the gradient of the parameter vector.&quot;</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    labs <span class="op">=</span> np.asarray(y, dtype<span class="op">=</span><span class="bu">float</span>).flatten()</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    posind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> <span class="op">-</span>(Phi[posind]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>g[posind])).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    negind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">0</span> )</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">+=</span> (Phi[negind]<span class="op">*</span>g[negind]).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dw[:, <span class="va">None</span>]</span></code></pre></div>
<h2 id="optimization-of-the-function">Optimization of the Function</h2>
<p>Reorganizing the gradient to find a stationary point of the function
with respect to the parameters <span class="math inline">\(\mathbf{
w}\)</span> turns out to be impossible. Optimization has to proceed by
<em>numerical methods</em>. Options include the multidimensional variant
of <a href="http://en.wikipedia.org/wiki/Newton%27s_method">Newton’s
method</a> or <a
href="http://en.wikipedia.org/wiki/Gradient_method">gradient based
optimization methods</a> like we used for optimizing matrix
factorization for the movie recommender system. We recall from matrix
factorization that, for large data, <em>stochastic gradient descent</em>
or the Robbins Munro <span class="citation"
data-cites="Robbins:stoch51">(Robbins and Monro, 1951)</span>
optimization procedure worked best for function minimization.</p>
<h1 id="nigeria-nmis-data">Nigeria NMIS Data</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>As an example data set we will use Nigerian Millennium Development
Goals Information System Health Facility <span class="citation"
data-cites="Nigeria-nmis14">(The Office of the Senior Special Assistant
to the President on the Millennium Development Goals (OSSAP-MDGs) and
Columbia University, 2014)</span>. It can be found here <a
href="https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014"
class="uri">https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014</a>.</p>
<p>Taking from the information on the site,</p>
<blockquote>
<p>The Nigeria MDG (Millennium Development Goals) Information System –
NMIS health facility data is collected by the Office of the Senior
Special Assistant to the President on the Millennium Development Goals
(OSSAP-MDGs) in partner with the Sustainable Engineering Lab at Columbia
University. A rigorous, geo-referenced baseline facility inventory
across Nigeria is created spanning from 2009 to 2011 with an additional
survey effort to increase coverage in 2014, to build Nigeria’s first
nation-wide inventory of health facility. The database includes 34,139
health facilities info in Nigeria.</p>
<p>The goal of this database is to make the data collected available to
planners, government officials, and the public, to be used to make
strategic decisions for planning relevant interventions.</p>
<p>For data inquiry, please contact Ms. Funlola Osinupebi, Performance
Monitoring &amp; Communications, Advisory Power Team, Office of the Vice
President at funlola.osinupebi@aptovp.org</p>
<p>To learn more, please visit <a
href="http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/"
class="uri">http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/</a></p>
<p>Suggested citation: Nigeria NMIS facility database (2014), the Office
of the Senior Special Assistant to the President on the Millennium
Development Goals (OSSAP-MDGs) &amp; Columbia University</p>
</blockquote>
<p>For ease of use we’ve packaged this data set in the <code>pods</code>
library</p>
<h2 id="pods">pods</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In Sheffield we created a suite of software tools for ‘Open Data
Science’. Open data science is an approach to sharing code, models and
data that should make it easier for companies, health professionals and
scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a>.</p>
<p>The software can be installed using</p>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/ods"
class="uri">https://github.com/lawrennd/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.nigeria_nmis()[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>data.head()</span></code></pre></div>
<p>Alternatively, you can access the data directly with the following
commands.</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://energydata.info/dataset/f85d1796-e7f2-4630-be84-79420174e3bd/resource/6e640a13-cab4-457b-b9e6-0336051bac27/download/healthmopupandbaselinenmisfacility.csv&#39;</span>, <span class="st">&#39;healthmopupandbaselinenmisfacility.csv&#39;</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&#39;healthmopupandbaselinenmisfacility.csv&#39;</span>)</span></code></pre></div>
<p>Once it is loaded in the data can be summarized using the
<code>describe</code> method in pandas.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>data.describe()</span></code></pre></div>
<p>We can also find out the dimensions of the dataset using the
<code>shape</code> property.</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>data.shape</span></code></pre></div>
<p>Dataframes have different functions that you can use to explore and
understand your data. In python and the Jupyter notebook it is possible
to see a list of all possible functions and attributes by typing the
name of the object followed by <code>.&lt;Tab&gt;</code> for example in
the above case if we type <code>data.&lt;Tab&gt;</code> it show the
columns available (these are attributes in pandas dataframes) such as
<code>num_nurses_fulltime</code>, and also functions, such as
<code>.describe()</code>.</p>
<p>For functions we can also see the documentation about the function by
following the name with a question mark. This will open a box with
documentation at the bottom which can be closed with the x button.</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>data.describe?</span></code></pre></div>
<div class="figure">
<div id="nigerian-health-facilities-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/nigerian-health-facilities.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="nigerian-health-facilities-magnify" class="magnify"
onclick="magnifyFigure(&#39;nigerian-health-facilities&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nigerian-health-facilities-caption" class="caption-frame">
<p>Figure: Location of the over thirty-four thousand health facilities
registered in the NMIS data across Nigeria. Each facility plotted
according to its latitude and longitude.</p>
</div>
</div>
<h2 id="nigeria-nmis-data-classification">Nigeria NMIS Data
Classification</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data-classification.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data-classification.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Our aim will be to predict whether a center has maternal health
delivery services given the attributes in the data. We will predict of
the number of nurses, the number of doctors, location etc.</p>
<p>Now we will convert this data into a form which we can use as inputs
<code>X</code>, and labels <code>y</code>.</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[<span class="op">~</span>pd.isnull(data[<span class="st">&#39;maternal_health_delivery_services&#39;</span>])]</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.dropna() <span class="co"># Remove entries with missing values</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">&#39;emergency_transport&#39;</span>,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;num_chews_fulltime&#39;</span>, </span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;phcn_electricity&#39;</span>,</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;child_health_measles_immun_calc&#39;</span>,</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;num_nurses_fulltime&#39;</span>,</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;num_doctors_fulltime&#39;</span>, </span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;improved_water_supply&#39;</span>, </span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;improved_sanitation&#39;</span>,</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;antenatal_care_yn&#39;</span>, </span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;family_planning_yn&#39;</span>,</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;malaria_treatment_artemisinin&#39;</span>, </span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;latitude&#39;</span>, </span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;longitude&#39;</span>]].copy()</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;maternal_health_delivery_services&#39;</span>]<span class="op">==</span><span class="va">True</span>  <span class="co"># set label to be whether there&#39;s a maternal health delivery service</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create series of health center types with the relevant index</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> data[<span class="st">&#39;facility_type_display&#39;</span>].<span class="bu">apply</span>(pd.Series, <span class="dv">1</span>).stack() </span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>s.index <span class="op">=</span> s.index.droplevel(<span class="op">-</span><span class="dv">1</span>) <span class="co"># to line up with df&#39;s index</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract from the series the unique list of types.</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>types <span class="op">=</span> s.unique()</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="co"># For each type extract the indices where it is present and add a column to X</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>type_names <span class="op">=</span> []</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> htype <span class="kw">in</span> types:</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> s[s<span class="op">==</span>htype].index.tolist()</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>    type_col<span class="op">=</span>htype.replace(<span class="st">&#39; &#39;</span>, <span class="st">&#39;_&#39;</span>).replace(<span class="st">&#39;/&#39;</span>,<span class="st">&#39;-&#39;</span>).lower()</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    type_names.append(type_col)</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>    X.loc[:, type_col] <span class="op">=</span> <span class="fl">0.0</span> </span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>    X.loc[index, type_col] <span class="op">=</span> <span class="fl">1.0</span></span></code></pre></div>
<p>This has given us a new data frame <code>X</code> which contains the
different facility types in different columns.</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>X.describe()</span></code></pre></div>
<h2 id="batch-gradient-descent">Batch Gradient Descent</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-gradient-descent.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-gradient-descent.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We will need to define some initial random values for our vector and
then minimize the objective by descending the gradient.</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate train and test</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.random.permutation(X.shape[<span class="dv">0</span>])</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>num_train <span class="op">=</span> np.ceil(X.shape[<span class="dv">0</span>]<span class="op">/</span><span class="dv">2</span>)r</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> indices[:num_train]</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>test_indices <span class="op">=</span> indices[num_train:]</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X.iloc[train_indices]</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y.iloc[train_indices]<span class="op">==</span><span class="va">True</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X.iloc[test_indices]</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y.iloc[test_indices]<span class="op">==</span><span class="va">True</span></span></code></pre></div>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient descent algorithm</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.random.normal(size<span class="op">=</span>(X.shape[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, <span class="dv">1</span>), scale <span class="op">=</span> <span class="fl">0.001</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>iters <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    g, Phi <span class="op">=</span> predict(w, X_train, linear)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> eta<span class="op">*</span>gradient(g, Phi, y_train) <span class="op">+</span> <span class="fl">0.001</span><span class="op">*</span>w</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> i <span class="op">%</span> <span class="dv">100</span>:</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Iter&quot;</span>, i, <span class="st">&quot;Objective&quot;</span>, objective(g, y_train))</span></code></pre></div>
<p>Let’s look at the weights and how they relate to the inputs.</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w)</span></code></pre></div>
<p>What does the magnitude of the weight vectors tell you about the
different parameters and their influence on outcome? Are the weights of
roughly the same size, if not, how might you fix this?</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>g_test, Phi_test <span class="op">=</span> predict(w, X_test, linear)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">sum</span>(g_test[y_test]<span class="op">&gt;</span><span class="fl">0.5</span>)</span></code></pre></div>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<h3 id="exercise-1">Exercise 1</h3>
<p>Now construct a stochastic gradient descent algorithm and run it on
the data. Is it faster or slower than batch gradient descent? What can
you do to improve convergence speed?</p>
<h2 id="regression">Regression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classification is the case where our prediction function gives a
discrete valued output, normally associated with a ‘class’. Regression
is an alternative approach where the aim is to predict a <em>continuous
output</em>.</p>
<p>The name is a historical accident, it would be better to call
regression ‘curve fitting’, or even split it into two parts
‘interpolation’, which is the practice of predicting a function value
between existing data, and ‘extrapolation’, which is the practice of
predicting a function value beyond the regime where we have data.</p>
<h2 id="regression-examples">Regression Examples</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-examples.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-examples.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Regression involves predicting a real value, <span
class="math inline">\(y_i\)</span>, given an input vector, <span
class="math inline">\(\mathbf{ x}_i\)</span>. For example, the Tecator
data involves predicting the quality of meat given spectral
measurements. Or in radiocarbon dating, the C14 calibration curve maps
from radiocarbon age to age measured through a back-trace of tree rings.
Regression has also been used to predict the quality of board game moves
given expert rated training data.</p>
<h2 id="supervised-learning-challenges">Supervised Learning
Challenges</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/supervised-learning-challenges.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/supervised-learning-challenges.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>There are three principal challenges in constructing a problem for
supervised learning.</p>
<ol type="1">
<li>choosing which features, <span class="math inline">\(\mathbf{
x}\)</span>, are relevant in the prediction</li>
<li>defining the appropriate <em>class of function</em>, <span
class="math inline">\(f(\cdot)\)</span>.</li>
<li>selecting the right parameters, <span class="math inline">\(\mathbf{
w}\)</span>.</li>
</ol>
<h2 id="feature-selection">Feature Selection</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/feature-selection.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/feature-selection.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Feature selection is a critical stage in the algorithm design
process. In the Olympic prediction example above we’re only using time
to predict the the pace of the runners. In practice we might also want
to use characteristics of the course: how hilly it is, what the
temperature was when the race was run. In 1904 the runners actually got
lost during the race. Should we include ‘lost’ as a feature? It would
certainly help explain the particularly slow time in 1904. The features
we select should be ones we expect to correlate with the prediction. In
statistics, these features are even called <em>predictors</em> which
highlights their role in developing the prediction function. For
Facebook newsfeed, we might use features that include how close your
friendship is with the poster, or how often you react to that poster, or
whether a photo is included in the post.</p>
<p>Sometimes we use feature selection algorithms, algorithms that
automate the process of finding the features that we need.
Classification is often used to rank search results, to decide which
adverts to serve or, at Facebook, to determine what appears at the top
of your newsfeed. In the Facebook example features might include how
many likes a post has had, whether it has an image in it, whether you
regularly interact with the friend who has posted. A good newsfeed
ranking algorithm is critical to Facebook’s success, just as good ad
serving choice is critical to Google’s success. These algorithms are in
turn highly dependent on the feature sets used. Facebook in particular
has made heavy investments in machine learning pipelines for evaluation
of the feature utility.</p>
<h2 id="class-of-function-fcdot">Class of Function, <span
class="math inline">\(f(\cdot)\)</span></h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/class-of-function.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/class-of-function.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>By class of function we mean, what are the characteristics of the
mapping between <span class="math inline">\(\mathbf{x}\)</span> and
<span class="math inline">\(y\)</span>. Often, we might choose it to be
a smooth function. Sometimes we will choose it to be a linear function.
If the prediction is a forecast, for example the demand of a particular
product, then the function would need some periodic components to
reflect seasonal or weekly effects.</p>
<h2 id="analysis-of-us-birth-rates">Analysis of US Birth Rates</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/bda-forecasting.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/bda-forecasting.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Aki Vehtari
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/dsa/./slides/diagrams//people/aki-vehtari.jpg" clip-path="url(#clip3)"/>
</svg>
</div>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="bialik-friday-the-13th-magnify" class="magnify"
onclick="magnifyFigure(&#39;bialik-friday-the-13th&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bialik-friday-the-13th-caption" class="caption-frame">
<p>Figure: This is a retrospective analysis of US births by Aki Vehtari.
The challenges of forecasting. Even with seasonal and weekly effects
removed there are significant effects on holidays, weekends, etc.</p>
</div>
</div>
<p>There’s a nice analysis of US birth rates by Gaussian processes with
additive covariances in <span class="citation"
data-cites="Gelman:bayesian13">Gelman et al. (2013)</span>. A
combination of covariance functions are used to take account of weekly
and yearly trends. The analysis is summarized on the cover of the
book.</p>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="bayesian-data-analysis-magnify" class="magnify"
onclick="magnifyFigure(&#39;bayesian-data-analysis&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bayesian-data-analysis-caption" class="caption-frame">
<p>Figure: Two different editions of Bayesian Data Analysis <span
class="citation" data-cites="Gelman:bayesian13">(Gelman et al.,
2013)</span>.</p>
</div>
</div>
<p>In the ImageNet challenge the input, <span
class="math inline">\(\mathbf{ x}\)</span>, was in the form of an image.
And the form of the prediction function was a <em>convolutional neural
network</em> (more on this later). A convolutional neural network
introduces <em>invariances</em> into the function that are particular to
image classification. An invariance is a transformation of the input
that we don’t want to affect the output. For example, a cat in an image
is still a cat no matter where it’s located in the image (translation).
The cat is also a cat regardless of how large it is (scale), or whether
it’s upside-down (rotation). Convolutional neural networks encode these
invariances: scale invariance, rotation invariance and translation
invariance; in the mathematical function.</p>
<p>Encoding invariance in the prediction function is like encoding
knowledge in the model. If we don’t specify these invariances, then the
model must learn them. This will require a lot more data to achieve the
same performance, making the model less data efficient. Note that one
invariance that is <em>not</em> encoded in a convolutional network is
invariance to camera type. As a result, practitioners need to be careful
to ensure that their training data is representative of the type of
cameras that will be used when the model is deployed.</p>
<p>In general the prediction function could be any set of parameterized
functions. In the Olympic marathon data example above we used a
polynomial fit, <span class="math display">\[
f(x) = w_0 + w_1 x+ w_2 x^2 + w_3 x^3 + w_4 x^4.
\]</span> The Olympic example is also a supervised learning challenge.
But it is a <em>regression</em> problem. A regression problem is one
where the output is a continuous value (such as the pace in the
marathon). In classification the output is constrained to be discrete.
For example, classifying whether or not an image contains a dog implies
the output is binary. An early example of a regression problem used in
machine learning was <a
href="http://lib.stat.cmu.edu/datasets/tecator">the Tecator data</a>,
where the fat, water and protein content of meat samples was predicted
as a function of the absorption of infrared light.</p>
<h2 id="class-of-function-neural-networks">Class of Function: Neural
Networks</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/class-of-function-neural-network.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/class-of-function-neural-network.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One class of function that has become popular recently is neural
network functions, in particular deep neural networks. The ImageNet
challenge uses <em>convolutional neural networks</em> which introduce a
<em>translation invariance</em> to the prediction function.</p>
<p>It’s impressive that only this additional invariance is enough to
improve performance so much, particularly when we know that rotational
invariances and scale invariances are also applicable for object
detection in images.</p>
<h1 id="deep-learning">Deep Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classical statistical models and simple machine learning models have
a great deal in common. The main difference between the fields is
philosophical. Machine learning practitioners are typically more
concerned with the quality of prediciton (e.g. measured by ROC curve)
while statisticians tend to focus more on the interpretability of the
model and the validity of any decisions drawn from that interpretation.
For example, a statistical model may be used to validate whether a large
scale intervention (such as the mass provision of mosquito nets) has had
a long term effect on disease (such as malaria). In this case one of the
covariates is likely to be the provision level of nets in a particular
region. The response variable would be the rate of malaria disease in
the region. The parmaeter, <span class="math inline">\(\beta_1\)</span>
associated with that covariate will demonstrate a positive or negative
effect which would be validated in answering the question. The focus in
statistics would be less on the accuracy of the response variable and
more on the validity of the interpretation of the effect variable, <span
class="math inline">\(\beta_1\)</span>.</p>
<p>A machine learning practitioner on the other hand would typically
denote the parameter <span class="math inline">\(w_1\)</span>, instead
of <span class="math inline">\(\beta_1\)</span> and would only be
interested in the output of the prediction function, <span
class="math inline">\(f(\cdot)\)</span> rather than the parameter
itself. The general formalism of the prediction function allows for
<em>non-linear</em> models. In machine learning, the emphasis on
prediction over interpretability means that non-linear models are often
used. The parameters, <span class="math inline">\(\mathbf{w}\)</span>,
are a means to an end (good prediction) rather than an end in themselves
(interpretable).</p>
<!-- No slide titles in this context -->
<h2 id="deepface">DeepFace</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deep-face-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-face&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-face-caption" class="caption-frame">
<p>Figure: The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.</p>
</div>
</div>
<p>The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span> consists
of layers that deal with <em>translation</em> invariances, known as
convolutional layers. These layers are followed by three
locally-connected layers and two fully-connected layers. Color
illustrates feature maps produced at each layer. The neural network
includes more than 120 million parameters, where more than 95% come from
the local and fully connected layers.</p>
<h3 id="deep-learning-as-pinball">Deep Learning as Pinball</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-pinball-caption" class="caption-frame">
<p>Figure: Deep learning models are composition of simple functions. We
can think of a pinball machine as an analogy. Each layer of pins
corresponds to one of the layers of functions in the model. Input data
is represented by the location of the ball from left to right when it is
dropped in from the top. Output class comes from the position of the
ball as it leaves the pins at the bottom.</p>
</div>
</div>
<p>Sometimes deep learning models are described as being like the brain,
or too complex to understand, but one analogy I find useful to help the
gist of these models is to think of them as being similar to early pin
ball machines.</p>
<p>In a deep neural network, we input a number (or numbers), whereas in
pinball, we input a ball.</p>
<p>Think of the location of the ball on the left-right axis as a single
number. Our simple pinball machine can only take one number at a time.
As the ball falls through the machine, each layer of pins can be thought
of as a different layer of ‘neurons’. Each layer acts to move the ball
from left to right.</p>
<p>In a pinball machine, when the ball gets to the bottom it might fall
into a hole defining a score, in a neural network, that is equivalent to
the decision: a classification of the input object.</p>
<p>An image has more than one number associated with it, so it is like
playing pinball in a <em>hyper-space</em>.</p>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-initialization-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-initialization&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-initialization-caption" class="caption-frame">
<p>Figure: At initialization, the pins, which represent the parameters
of the function, aren’t in the right place to bring the balls to the
correct decisions.</p>
</div>
</div>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-trained-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-trained&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-trained-caption" class="caption-frame">
<p>Figure: After learning the pins are now in the right place to bring
the balls to the correct decisions.</p>
</div>
</div>
<p>Learning involves moving all the pins to be in the correct position,
so that the ball ends up in the right place when it’s fallen through the
machine. But moving all these pins in hyperspace can be difficult.</p>
<p>In a hyper-space you have to put a lot of data through the machine
for to explore the positions of all the pins. Even when you feed many
millions of data points through the machine, there are likely to be
regions in the hyper-space where no ball has passed. When future test
data passes through the machine in a new route unusual things can
happen.</p>
<p><em>Adversarial examples</em> exploit this high dimensional space. If
you have access to the pinball machine, you can use gradient methods to
find a position for the ball in the hyper space where the image looks
like one thing, but will be classified as another.</p>
<p>Probabilistic methods explore more of the space by considering a
range of possible paths for the ball through the machine. This helps to
make them more data efficient and gives some robustness to adversarial
examples.</p>
<h2 id="encoding-knowledge">Encoding Knowledge</h2>
<p>Knowledge that is not encoded in the prediction function must be
learned through data. So any unspecified invariance (such as rotational
or scale invariances) must be learned through the data. This means that
learning would require a lot more data than otherwise would be necessary
and results in less data efficient algorithms.</p>
<p>The choice of predication funciton and invariances is therefore a
critical stage in designing your machine learning algorithm.
Unfortunately many invariances are non-trivial to incorporate and many
machine learning algorithms focus on simpler concepts such as linearity
or smoothness.</p>
<h2 id="parameter-estimation-objective-functions">Parameter Estimation:
Objective Functions</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/parameter-estimation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/parameter-estimation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Once we have a set of features, and the class of functions we use is
determined, we need to find the parameters of the model.</p>
<p>The parameters of the model, <span class="math inline">\(\mathbf{
w}\)</span>, are estimated by specifying an <em>objective function</em>.
The objective function specifies the quality of the match between the
prediction function and the <em>training data</em>. In supervised
learning the objective function incorporates both the input data (in the
ImageNet data the image, in the Olympic marathon data the year of the
marathon) and a <em>label</em>.</p>
<p>The label is where the term supervised learning comes from. The idea
being that a supervisor, or annotator, has already looked at the data
and given it labels. For regression problem, a typical objective
function is the <em>squared error</em>, <span class="math display">\[
E(\mathbf{ w}) = \sum_{i=1}^n(y_i - f(\mathbf{ x}_i))^2
\]</span> where the data is provided to us as a set of <span
class="math inline">\(n\)</span> inputs, <span
class="math inline">\(\mathbf{ x}_1\)</span>, <span
class="math inline">\(\mathbf{ x}_2\)</span>, <span
class="math inline">\(\mathbf{ x}_3\)</span>, <span
class="math inline">\(\dots\)</span>, <span
class="math inline">\(\mathbf{ x}_n\)</span> each one with an associated
label, <span class="math inline">\(y_1\)</span>, <span
class="math inline">\(y_2\)</span>, <span
class="math inline">\(y_3\)</span>, <span
class="math inline">\(\dots\)</span>, <span
class="math inline">\(y_n\)</span>. Sometimes the label is cheap to
acquire. For example, in Newsfeed ranking Facebook are acquiring a label
each time a user clicks on a post in their Newsfeed. Similarly, in
ad-click prediction labels are obtained whenever an advert is clicked.
More generally though, we have to employ human annotators to label the
data. For example, ImageNet, the breakthrough deep learning result was
annotated using Amazon’s Mechanical Turk. Without such large scale human
input, we would not have the breakthrough results on image
categorization we have today.</p>
<p>Some tasks are easier to annotate than others. For example, in the
Tecator data, to acquire the actual values of water, protein and fat
content in the meat samples further experiments may be required. It is
not simply a matter of human labelling. Even if the task is easy for
humans to solve there can be problems. For example, humans will
extrapolate the context of an image. A colleague mentioned once to me a
challenge where humans were labelling images as containing swimming
pools, even though none was visible, because they could infer there must
be a pool nearby, perhaps because there are kids wearing bathing suits.
But there is no swimming pool in the image for the computer to find. The
quality of any machine learning solution is very sensitive to the
quality of annotated data we have. Investing in processes and tools to
improve annotation of data is therefore priority for improving the
quality of machine learning solutions.</p>
<p>There can also be significant problems with misrepresentation in the
data set. If data isn’t collected carefully, then it can reflect biases
about the population that we don’t want our models to have. For example,
if we design a face detector using Californians may not perform well
when deployed in Kampala, Uganda.</p>
<h2 id="generalization-and-overfitting">Generalization and
Overfitting</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/generalization-and-overfitting.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/generalization-and-overfitting.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Once a supervised learning system is trained it can be placed in a
sequential pipeline to automate a process that used to be done
manually.</p>
<p>Supervised learning is one of the dominant approaches to learning.
But the cost and time associated with labeling data is a major
bottleneck for deploying machine learning systems. The process for
creating training data requires significant human intervention. For
example, internationalization of a speech recognition system would
require large speech corpora in new languages.</p>
<p>An important distinction in machine learning is the separation
between training data and test data (or production data). Training data
is the data that was used to find the model parameters. Test data (or
production data) is the data that is used with the live system. The
ability of a machine learning system to predict well on production
systems given only its training data is known as its
<em>generalization</em> ability. This is the system’s ability to predict
in areas where it hasn’t previously seen data.</p>
<h2 id="hold-out-validation-on-olympic-marathon-data">Hold Out
Validation on Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-hold-out-validation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-hold-out-validation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="olympic-val-extra-LM-polynomial-number-11-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic_val_extra_LM_polynomial_number011.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-val-extra-LM-polynomial-number-11-magnify"
class="magnify"
onclick="magnifyFigure(&#39;olympic-val-extra-LM-polynomial-number-11&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-val-extra-LM-polynomial-number-11-caption"
class="caption-frame">
<p>Figure: Olympic marathon data with validation error for
extrapolation.</p>
</div>
</div>
<h2 id="extrapolation">Extrapolation</h2>
<h2 id="interpolation">Interpolation</h2>
<div class="figure">
<div id="olympic-val-inter-LM-polynomial-number-11-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic_val_inter_LM_polynomial_number011.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-val-inter-LM-polynomial-number-11-magnify"
class="magnify"
onclick="magnifyFigure(&#39;olympic-val-inter-LM-polynomial-number-11&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-val-inter-LM-polynomial-number-11-caption"
class="caption-frame">
<p>Figure: Olympic marathon data with validation error for
interpolation.</p>
</div>
</div>
<h2 id="choice-of-validation-set">Choice of Validation Set</h2>
<h2 id="hold-out-data">Hold Out Data</h2>
<p>You have a conclusion as to which model fits best under the training
error, but how do the two models perform in terms of validation? In this
section we consider <em>hold out</em> validation. In hold out validation
we remove a portion of the training data for <em>validating</em> the
model on. The remaining data is used for fitting the model (training).
Because this is a time series prediction, it makes sense for us to hold
out data at the end of the time series. This means that we are
validating on future predictions. We will hold out data from after 1980
and fit the model to the data before 1980.</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># select indices of data to &#39;hold out&#39;</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>indices_hold_out <span class="op">=</span> np.flatnonzero(x<span class="op">&gt;</span><span class="dv">1980</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training set</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> np.delete(x, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.delete(y, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a hold out set</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>x_valid <span class="op">=</span> np.take(x, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>y_valid <span class="op">=</span> np.take(y, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<h3 id="exercise-2">Exercise 2</h3>
<p>For both the linear and quadratic models, fit the model to the data
up until 1980 and then compute the error on the held out data (from 1980
onwards). Which model performs better on the validation data?</p>
<h2 id="richer-basis-set">Richer Basis Set</h2>
<p>Now we have an approach for deciding which model to retain, we can
consider the entire family of polynomial bases, with arbitrary
degrees.</p>
<h3 id="exercise-3">Exercise 3</h3>
<p>Now we are going to build a more sophisticated form of basis
function, one that can accept arguments to its inputs (similar to those
we used in <a href="./week4.ipynb">this lab</a>). Here we will start
with a polynomial basis.</p>
<pre><code>def polynomial(x, degree, loc, scale):
    degrees =np.arange(degree+1)
    return ((x-loc)/scale)**degrees</code></pre>
<p>The basis as we’ve defined it has three arguments as well as the
input. The degree of the polynomial, the scale of the polynomial and the
offset. These arguments need to be passed to the basis functions
whenever they are called. Modify your code to pass these additional
arguments to the python function for creating the basis. Do this for
each of your functions <code>predict</code>, <code>fit</code> and
<code>objective</code>. You will find <code>*args</code> (or
<code>**kwargs</code>) useful.</p>
<p>Write code that tries to fit different models to the data with
polynomial basis. Use a maximum degree for your basis from 0 to 17. For
each polynomial store the <em>hold out validation error</em> and the
<em>training error</em>. When you have finished the computation plot the
hold out error for your models and the training error for your p. When
computing your polynomial basis use <code>offset=1956.</code> and
<code>scale=120.</code> to ensure that the data is mapped (roughly) to
the -1, 1 range.</p>
<p>Which polynomial has the minimum training error? Which polynomial has
the minimum validation error?</p>
<h2 id="bias-variance-decomposition">Bias Variance Decomposition</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/bias-variance-dilemma.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/bias-variance-dilemma.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One of Breiman’s ideas for improving predictive performance is known
as bagging <span class="citation"
data-cites="Breiman:bagging96">(<strong>Breiman:bagging96?</strong>)</span>.
The idea is to train a number of models on the data such that they
overfit (high variance). Then average the predictions of these models.
The models are trained on different bootstrap samples <span
class="citation" data-cites="Efron:bootstrap79">(Efron, 1979)</span> and
their predictions are aggregated giving us the acronym, Bagging. By
combining decision trees with bagging, we recover random forests <span
class="citation" data-cites="Breiman-forests01">(Breiman,
2001)</span>.</p>
<p>Bias and variance can also be estimated through Efron’s bootstrap
<span class="citation" data-cites="Efron:bootstrap79">(Efron,
1979)</span>, and the traditional view has been that there’s a form of
Goldilocks effect, where the best predictions are given by the model
that is ‘just right’ for the amount of data available. Not to simple,
not too complex. The idea is that bias decreases with increasing model
complexity and variance increases with increasing model complexity.
Typically plots begin with the Mummy bear on the left (too much bias)
end with the Daddy bear on the right (too much variance) and show a dip
in the middle where the Baby bear (just) right finds themselves.</p>
<p>The Daddy bear is typically positioned at the point where the model
can exactly interpolate the data. For a generalized linear model <span
class="citation" data-cites="McCullagh:gen_linear89">(McCullagh and
Nelder, 1989)</span>, this is the point at which the number of
parameters is equal to the number of data<a href="#fn3"
class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>.</p>
<p>The bias-variance decomposition <span class="citation"
data-cites="Geman:biasvariance92">(<strong>Geman:biasvariance92?</strong>)</span>
considers the expected test error for different variations of the
<em>training data</em> sampled from, <span
class="math inline">\(\mathbb{P}(\mathbf{ x}, y)\)</span> <span
class="math display">\[\begin{align*}
R(\mathbf{ w}) = &amp; \int \left(y- f^*(\mathbf{ x})\right)^2
\mathbb{P}(y, \mathbf{ x}) \text{d}y\text{d}\mathbf{ x}\\
&amp; \triangleq \mathbb{E}\left[ \left(y- f^*(\mathbf{ x})\right)^2
\right].
\end{align*}\]</span></p>
<p>This can be decomposed into two parts, <span class="math display">\[
\begin{align*}
\mathbb{E}\left[ \left(y- f(\mathbf{ x})\right)^2 \right] = &amp;
\text{bias}\left[f^*(\mathbf{ x})\right]^2  +
\text{variance}\left[f^*(\mathbf{ x})\right]  +\sigma^2,
\end{align*}
\]</span> where the bias is given by <span class="math display">\[
  \text{bias}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[f^*(\mathbf{ x})\right] - f(\mathbf{ x})
\]</span> and it summarizes error that arises from the model’s inability
to represent the underlying complexity of the data. For example, if we
were to model the marathon pace of the winning runner from the Olympics
by computing the average pace across time, then that model would exhibit
<em>bias</em> error because the reality of Olympic marathon pace is it
is changing (typically getting faster).</p>
<p>The variance term is given by <span class="math display">\[
  \text{variance}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[\left(f^*(\mathbf{ x}) - \mathbb{E}\left[f^*(\mathbf{
x})\right]\right)^2\right].
  \]</span> The variance term is often described as arising from a model
that is too complex, but we must be careful with this idea. Is the model
really too complex relative to the real world that generates the data?
The real world is a complex place, and it is rare that we are
constructing mathematical models that are more complex than the world
around us. Rather, the ‘too complex’ refers to ability to estimate the
parameters of the model given the data we have. Slight variations in the
training set cause changes in prediction.</p>
<p>Models that exhibit high variance are sometimes said to ‘overfit’ the
data whereas models that exhibit high bias are sometimes described as
‘underfitting’ the data.</p>
<h2 id="bias-vs-variance-error-plots">Bias vs Variance Error Plots</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/bias-variance-plots.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/bias-variance-plots.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Helper function for sampling data from two different classes.</p>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<p>Helper function for plotting the decision boundary of the SVM.</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>font <span class="op">=</span> {<span class="st">&#39;family&#39;</span> : <span class="st">&#39;sans&#39;</span>,</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;weight&#39;</span> : <span class="st">&#39;bold&#39;</span>,</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;size&#39;</span>   : <span class="dv">22</span>}</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>matplotlib.rc(<span class="st">&#39;font&#39;</span>, <span class="op">**</span>font)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of SVM and fit the data. </span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="fl">100.0</span>  <span class="co"># SVM regularization parameter</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>gammas <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>]</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>per_class<span class="op">=</span><span class="dv">30</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>num_samps <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set-up 2x2 grid for plotting.</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>xlim<span class="op">=</span><span class="va">None</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>ylim<span class="op">=</span><span class="va">None</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> samp <span class="kw">in</span> <span class="bu">range</span>(num_samps):</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    X, y<span class="op">=</span>create_data(per_class)</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> []</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    titles <span class="op">=</span> []</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> gamma <span class="kw">in</span> gammas:</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>        models.append(svm.SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>, gamma<span class="op">=</span>gamma, C<span class="op">=</span>C))</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>        titles.append(<span class="st">&#39;$\gamma=</span><span class="sc">{}</span><span class="st">$&#39;</span>.<span class="bu">format</span>(gamma))</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> (cl.fit(X, y) <span class="cf">for</span> cl <span class="kw">in</span> models)</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>    xlim, ylim <span class="op">=</span> decision_boundary_plot(models, X, y, </span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>                           axs<span class="op">=</span>ax, </span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>                           filename<span class="op">=</span><span class="st">&#39;bias-variance</span><span class="sc">{samp:0&gt;3}</span><span class="st">.svg&#39;</span>.<span class="bu">format</span>(samp<span class="op">=</span>samp), </span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>                           directory<span class="op">=</span><span class="st">&#39;./ml&#39;</span></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>                           titles<span class="op">=</span>titles,</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>                          xlim<span class="op">=</span>xlim,</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>                          ylim<span class="op">=</span>ylim)</span></code></pre></div>
<!---->
<div class="figure">
<div id="bias-variance-errors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/bias-variance000.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/dsa/./slides/diagrams//ml/bias-variance010.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="bias-variance-errors-magnify" class="magnify"
onclick="magnifyFigure(&#39;bias-variance-errors&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bias-variance-errors-caption" class="caption-frame">
<p>Figure: In each figure the simpler model is on the left, and the more
complex model is on the right. Each fit is done to a different version
of the data set. The simpler model is more consistent in its errors
(bias error), whereas the more complex model is varying in its errors
(variance error).</p>
</div>
</div>
<h2 id="overfitting">Overfitting</h2>
<div class="figure">
<div id="alex-ihler-overfitting-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/py8QrZPT48s?start=4m0s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="alex-ihler-overfitting-magnify" class="magnify"
onclick="magnifyFigure(&#39;alex-ihler-overfitting&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="alex-ihler-overfitting-caption" class="caption-frame">
<p>Figure: Alex Ihler discusses polynomials and overfitting.</p>
</div>
</div>
<p>We can easily develop a simple prediction function that reconstructs
the training data exactly, you can just use a look up table. But how
would the lookup table predict between the training data, where examples
haven’t been seen before? The choice of the class of prediction
functions is critical in ensuring that the model generalizes well.</p>
<p>The generalization error is normally estimated by applying the
objective function to a set of data that the model <em>wasn’t</em>
trained on, the test data. To ensure good performance we normally want a
model that gives us a low generalization error. If we weren’t sure of
the right prediction function to use, then we could try 1,000 different
prediction functions. Then we could use the one that gives us the lowest
error on the test data. But you have to be careful. Selecting a model in
this way is like a further stage of training where you are using the
test data in the training.<a href="#fn4" class="footnote-ref"
id="fnref4" role="doc-noteref"><sup>4</sup></a> So when this is done,
the data used for this is not known as test data, it is known as
<em>validation data</em>. And the associated error is the <em>validation
error</em>. Using the validation error for model selection is a standard
machine learning technique, but it can be misleading about the final
generalization error. Almost all machine learning practitioners know not
to use the test data in your training procedure, but sometimes people
forget that when validation data is used for model selection that
validation error cannot be used as an unbiased estimate of the
generalization performance.</p>
<h2 id="olympic-data-with-bayesian-polynomials">Olympic Data with
Bayesian Polynomials</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-bayesian-polynomial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-bayesian-polynomial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Five fold cross validation tests the ability of the model to
<em>interpolate</em>.</p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="figure">
<div id="olympic-blm-polynomial-number-26-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic_BLM_polynomial_number026.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-blm-polynomial-number-26-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-blm-polynomial-number-26&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-blm-polynomial-number-26-caption"
class="caption-frame">
<p>Figure: Bayesian fit with 26th degree polynomial and negative
marginal log likelihood.</p>
</div>
</div>
<h2 id="hold-out-validation">Hold Out Validation</h2>
<p>For the polynomial fit, we will now look at <em>hold out</em>
validation, where we are holding out some of the most recent points.
This tests the abilit of our model to <em>extrapolate</em>.</p>
<div class="figure">
<div id="olympic-val-blm-polynomial-number-26-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic_val_BLM_polynomial_number026.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-val-blm-polynomial-number-26-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-val-blm-polynomial-number-26&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-val-blm-polynomial-number-26-caption"
class="caption-frame">
<p>Figure: Bayesian fit with 26th degree polynomial and hold out
validation scores.</p>
</div>
</div>
<h2 id="fold-cross-validation">5-fold Cross Validation</h2>
<p>Five fold cross validation tests the ability of the model to
<em>interpolate</em>.</p>
<div class="figure">
<div id="olympic-5cv05-blm-polynomial-number-26-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/olympic_5cv05_BLM_polynomial_number026.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-5cv05-blm-polynomial-number-26-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-5cv05-blm-polynomial-number-26&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-5cv05-blm-polynomial-number-26-caption"
class="caption-frame">
<p>Figure: Bayesian fit with 26th degree polynomial and five fold cross
validation scores.</p>
</div>
</div>
<!-- Leave unsupervised and reinforcement learning in the notes -->
<h1 id="unsupervised-learning">Unsupervised Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/unsupervised-learning.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/unsupervised-learning.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="unsupervised-learning-1">Unsupervised Learning</h2>
<p>Supervised learning is when your data is provided with labels. Now we
are going to turn to a different form of learning, commonly known as
<em>unsupervised</em> learning. In unsupervised learning our data isn’t
necessarily labelled in any form, but we want models that give us a
better understanding of the data. We’ve actually seen an example of this
already with , which we introduces in the context of <em>objective
functions</em>. Now we will introduce a more probabilistic approach to
such models, specifically we are interested in <em>latent variable</em>
modelling.</p>
<p>In unsupervised learning you have data, <span
class="math inline">\(\mathbf{ x}\)</span>, but no labels <span
class="math inline">\(y\)</span>. The aim in unsupervised learning is to
extract structure from data. The type of structure you are interested in
is dependent on the broader context of the task. In supervised learning
that context is very much driven by the labels. Supervised learning
algorithms try and focus on the aspects of the data which are relevant
to predicting the labels. But in unsupervised learning there are no
labels.</p>
<h2 id="context">Context</h2>
<p>Humans can easily sort a number of objects into objects that share
similar characteristics. We easily categorize animals or vehicles. But
if the data is very large this is too slow. Even for smaller data, it
may be that it is presented in a form that is unintelligible for humans.
We are good at dealing with high dimensional data when it’s presented in
images, but if it’s presented as a series of numbers, we find it hard to
interpret. In unsupervised learning we want the computer to do the
sorting for us. For example, an e-commerce company might need an
algorithm that can go through its entire list of products and
automatically sort them into groups such that similar products are
located together.</p>
<h2 id="discrete-vs-continuous">Discrete vs Continuous</h2>
<p>Supervised learning is broadly divided into classification: i.e. wake
word classification in the Amazon Echo, and regression, e.g. shelf life
prediction for perishable goods. Similarly, unsupervised learning can be
broadly split into methods that cluster the data (i.e. provide a
discrete label) and methods that represent the data as a continuous
value.</p>
<h2 id="clustering">Clustering</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/clustering.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/clustering.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li><p>One common approach, not deeply covered in this course.</p></li>
<li><p>Associate each data point, <span class="math inline">\(\mathbf{
y}_{i, :}\)</span> with one of <span class="math inline">\(k\)</span>
different discrete groups.</p></li>
<li><p>For example:</p>
<ul>
<li>Clustering animals into discrete groups. Are animals discrete or
continuous?</li>
<li>Clustering into different different <em>political</em>
affiliations.</li>
</ul></li>
<li><p>Humans do seem to like clusters:</p>
<ul>
<li>Very useful when interacting with biologists.</li>
</ul></li>
<li><p>Subtle difference between clustering and <em>vector
quantisation</em></p></li>
<li><p>Little anecdote.</p></li>
<li><p>To my mind difference is in clustering there should be a
reduction in data density between samples.</p></li>
<li><p>This definition is not universally applied.</p></li>
<li><p>For today’s purposes we merge them:</p>
<ul>
<li>Determine how to allocate each point to a group and <em>harder</em>
total number of groups.</li>
</ul></li>
<li><p>Simple algorithm for allocating points to groups.</p></li>
<li><p><em>Require</em>: Set of <span class="math inline">\(k\)</span>
cluster centres &amp; assignment of each points to a cluster.</p></li>
</ul>
<ol type="1">
<li>Initialize cluster centres as randomly selected data points.
<ol start="2" type="1">
<li>Assign each data point to <em>nearest</em> cluster centre.</li>
<li>Update each cluster centre by setting it to the mean of assigned
data points.</li>
<li>Repeat 2 and 3 until cluster allocations do not change.</li>
</ol></li>
</ol>
<ul>
<li>This minimizes the objective <span class="math display">\[
E=\sum_{j=1}^K \sum_{i\ \text{allocated to}\ j}  \left(\mathbf{ y}_{i,
:} - \boldsymbol{ \mu}_{j, :}\right)^\top\left(\mathbf{ y}_{i, :} -
\boldsymbol{ \mu}_{j, :}\right)
\]</span> <em>i.e.</em> it minimizes thesum of Euclidean squared
distances betwen points and their associated centres.</li>
<li>The minimum is <em>not</em> guaranteed to be <em>global</em> or
<em>unique</em>.</li>
<li>This objective is a non-convex optimization problem.</li>
</ul>
<p>Clustering methods associate each data point with a different label.
Unlike in classification the label is not provided by a human annotator.
It is allocated by the computer. Clustering is quite intuitive for
humans, we do it naturally with our observations of the real world. For
example, we cluster animals into different groups. If we encounter a new
animal, we can immediately assign it to a group: bird, mammal, insect.
These are certainly labels that can be provided by humans, but they were
also originally invented by humans. With clustering we want the computer
to recreate that process of inventing the label.</p>
<p>Unsupervised learning enables computers to form similar
categorizations on data that is too large scale for us to process. When
the Greek philosopher, Plato, was thinking about ideas, he considered
the concept of the Platonic ideal. The Platonic ideal bird is the bird
that is most bird-like or the chair that is most chair-like. In some
sense, the task in clustering is to define different clusters, by
finding their Platonic ideal (known as the cluster center) and allocate
each data point to the relevant cluster center. So, allocate each animal
to the class defined by its nearest cluster center.</p>
<p>To perform clustering on a computer we need to define a notion of
either similarity or distance between the objects and their Platonic
ideal, the cluster center. We normally assume that our objects are
represented by vectors of data, <span class="math inline">\(\mathbf{
x}_i\)</span>. Similarly, we represent our cluster center for category
<span class="math inline">\(j\)</span> by a vector <span
class="math inline">\(\boldsymbol{ \mu}_j\)</span>. This vector contains
the ideal features of a bird, a chair, or whatever category <span
class="math inline">\(j\)</span> is. In clustering we can either think
in terms of similarity of the objects, or distances. We want objects
that are similar to each other to cluster together. We want objects that
are distant from each other to cluster apart.</p>
<p>This requires us to formalize our notion of similarity or distance.
Let’s focus on distances. A definition of distance between an object,
<span class="math inline">\(i\)</span>, and the cluster center of class
<span class="math inline">\(j\)</span> is a function of two vectors, the
data point, <span class="math inline">\(\mathbf{ x}_i\)</span> and the
cluster center, <span class="math inline">\(\boldsymbol{
\mu}_j\)</span>, <span class="math display">\[
d_{ij} = f(\mathbf{ x}_i, \boldsymbol{ \mu}_j).
\]</span> Our objective is then to find cluster centers that are close
to as many data points as possible. For example, we might want to
cluster customers into their different tastes. We could represent each
customer by the products they’ve purchased in the past. This could be a
binary vector <span class="math inline">\(\mathbf{ x}_i\)</span>. We can
then define a distance between the cluster center and the customer.</p>
<h3 id="squared-distance">Squared Distance</h3>
<p>A commonly used distance is the squared distance, <span
class="math display">\[
d_{ij} = (\mathbf{ x}_i - \boldsymbol{ \mu}_j)^2.
\]</span> The squared distance comes up a lot in machine learning. In
unsupervised learning it was used to measure dissimilarity between
predictions and observed data. Here its being used to measure the
dissimilarity between a cluster center and the data.</p>
<p>Once we have decided on the distance or similarity function, we can
decide a number of cluster centers, <span
class="math inline">\(K\)</span>. We find their location by allocating
each center to a sub-set of the points and minimizing the sum of the
squared errors, <span class="math display">\[
E(\mathbf{M}) = \sum_{i \in \mathbf{i}_j} (\mathbf{ x}_i - \boldsymbol{
\mu}_j)^2
\]</span> where the notation <span
class="math inline">\(\mathbf{i}_j\)</span> represents all the indices
of each data point which has been allocated to the <span
class="math inline">\(j\)</span>th cluster represented by the center
<span class="math inline">\(\boldsymbol{ \mu}_j\)</span>.</p>
<h3 id="k-means-clustering"><span class="math inline">\(k\)</span>-Means
Clustering</h3>
<p>One approach to minimizing this objective function is known as
<em><span class="math inline">\(k\)</span>-means clustering</em>. It is
simple and relatively quick to implement, but it is an initialization
sensitive algorithm. Initialization is the process of choosing an
initial set of parameters before optimization. For <span
class="math inline">\(k\)</span>-means clustering you need to choose an
initial set of centers. In <span class="math inline">\(k\)</span>-means
clustering your final set of clusters is very sensitive to the initial
choice of centers. For more technical details on <span
class="math inline">\(k\)</span>-means clustering you can watch a video
of Alex Ihler introducing the algorithm here.</p>
<h3 id="k-means-clustering-1"><span
class="math inline">\(k\)</span>-Means Clustering</h3>
<div class="figure">
<div id="kmeans-clustering-13-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/kmeans_clustering_013.svg" width="\width" style=" ">
</object>
</div>
<div id="kmeans-clustering-13-magnify" class="magnify"
onclick="magnifyFigure(&#39;kmeans-clustering-13&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kmeans-clustering-13-caption" class="caption-frame">
<p>Figure: Clustering with the <span
class="math inline">\(k\)</span>-means clustering algorithm.</p>
</div>
</div>
<div class="figure">
<div id="k-means-clustering-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/mfqmoUN-Cuw?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="k-means-clustering-magnify" class="magnify"
onclick="magnifyFigure(&#39;k-means-clustering&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="k-means-clustering-caption" class="caption-frame">
<p>Figure: <span class="math inline">\(k\)</span>-means clustering by
Alex Ihler.</p>
</div>
</div>
<h3 id="hierarchical-clustering">Hierarchical Clustering</h3>
<p>Other approaches to clustering involve forming taxonomies of the
cluster centers, like humans apply to animals, to form trees. You can
learn more about agglomerative clustering in this video from Alex
Ihler.</p>
<div class="figure">
<div id="alex-ihler-hierarchical-clustering-figure"
class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/OcoE7JlbXvY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="alex-ihler-hierarchical-clustering-magnify" class="magnify"
onclick="magnifyFigure(&#39;alex-ihler-hierarchical-clustering&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="alex-ihler-hierarchical-clustering-caption"
class="caption-frame">
<p>Figure: Hierarchical Clustering by Alex Ihler.</p>
</div>
</div>
<h3 id="phylogenetic-trees">Phylogenetic Trees</h3>
<p>Indeed, one application of machine learning techniques is performing
a hierarchical clustering based on genetic data, i.e. the actual
contents of the genome. If we do this across a number of species then we
can produce a <em>phylogeny</em>. The phylogeny aims to represent the
actual evolution of the species and some phylogenies even estimate the
timing of the common ancestor between two species<a href="#fn5"
class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.
Similar methods are used to estimate the origin of viruses like AIDS or
Bird flu which mutate very quickly. Determining the origin of viruses
can be important in containing or treating outbreaks.</p>
<h3 id="product-clustering">Product Clustering</h3>
<p>An e-commerce company could apply hierarchical clustering to all its
products. That would give a phylogeny of products. Each cluster of
products would be split into sub-clusters of products until we got down
to individual products. For example, we might expect a high level split
to be Electronics/Clothing. Of course, a challenge with these tree-like
structures is that many products belong in more than one parent cluster:
for example running shoes should be in more than one group, they are
‘sporting goods’ and they are ‘apparel’. A tree structure doesn’t allow
this allocation.</p>
<h3 id="hierarchical-clustering-challenge">Hierarchical Clustering
Challenge</h3>
<p>Our own psychological grouping capabilities are studied as a domain
of cognitive science. Researchers like Josh Tenenbaum have developed
algorithms that decompose data in more complex ways, but they can
normally only be applied to smaller data sets.</p>
<h2 id="other-clustering-approaches">Other Clustering Approaches</h2>
<ul>
<li>Spectral clustering (<span class="citation"
data-cites="Shi:normalized00">Shi and Malik (2000)</span>,<span
class="citation" data-cites="Ng:spectral02">Ng et al. (n.d.)</span>)
<ul>
<li>Allows clusters which aren’t convex hulls.</li>
</ul></li>
<li>Dirichlet process
<ul>
<li>A probabilistic formulation for a clustering algorithm that is
<em>non-parametric</em>.</li>
<li>Loosely speaking it allows infinite clusters</li>
<li>In practice useful for dealing with previously unknown species
(e.g. a “Black Swan Event”).</li>
</ul></li>
</ul>
<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/dimensionality-reduction-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/dimensionality-reduction-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Dimensionality reduction methods compress the data by replacing the
original data with a reduced number of continuous variables. One way of
thinking of these methods is to imagine a marionette.</p>
<div class="figure">
<div id="marionette-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/marionette.svg" width="40%" style=" ">
</object>
</div>
<div id="marionette-magnify" class="magnify"
onclick="magnifyFigure(&#39;marionette&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="marionette-caption" class="caption-frame">
<p>Figure: Thinking of dimensionality reduction as a marionette. We
observe the high dimensional pose of the puppet, <span
class="math inline">\(\mathbf{ x}\)</span>, but the movement of the
puppeteer’s hand, <span class="math inline">\(\mathbf{ z}\)</span>
remains hidden to us. Dimensionality reduction aims to recover those
hidden movements which generated the observations.</p>
</div>
</div>
<p>The position of each body part of a marionette could be thought of as
our data, <span class="math inline">\(\mathbf{ x}_i\)</span>. So, each
data point consists of the 3-D co-ordinates of all the different body
parts of the marionette. Let’s say there are 13 different body parts (2
each of feet, knees, hips, hands, elbows, shoulders, one head). Each
body part has an x, y, z position in Cartesian coordinates. So that’s 39
numbers associated with each observation.</p>
<p>The movement of these 39 parts is determined by the puppeteer via
strings. Let’s assume it’s a very simple puppet, with just one stick to
control it. The puppeteer can move the stick up and down, left and
right. And they can twist it. This gives three parameters in the
puppeteers control. This implies that the 39 variables we see moving are
controlled by only 3 variables. These 3 variables are often called the
hidden or <em>latent variables</em>.</p>
<p>Dimensionality reduction assumes something similar for real world
data. It assumes that the data we observe is generated from some lower
dimensional underlying process. It then seeks to recover the values
associated with this low dimensional process.</p>
<h3 id="examples-in-social-sciences">Examples in Social Sciences</h3>
<p>Dimensionality reduction techniques underpin a lot of psychological
scoring tests such as IQ tests or personality tests. An IQ test can
involve several hundred questions, potentially giving a rich, high
dimensional, characterization of some aspects of your intelligence. It
is then summarized by a single number. Similarly, the Myers-Briggs
personality test involves answering questions about preferences which
are reduced to a set of numbers reflecting personality.</p>
<p>These tests are assuming that our intelligence is implicitly
one-dimensional and that our personality is implicitly four dimensional.
Other examples include political belief which is typically represented
on a left to right scale. A one-dimensional distillation of an entire
philosophy about how a country should be run. Our own leadership
principles imply that our decisions have a fourteen-dimensional space
underlying them. Each decision could be characterized by judging to what
extent it embodies each of the principles.</p>
<p>Political belief, personality, intelligence, leadership. None of
these exist as a directly measurable quantity in the real world, rather
they are inferred based on measurables. Dimensionality reduction is the
process of allowing the computer to automatically find such underlying
dimensions. This automatically allowing us to characterize each data
point according to those explanatory variables. Each of these
characteristics can be scored, and individuals can then be turned into
vectors.</p>
<p>This doesn’t only apply to individuals, in recent years work on
language modeling has taken a similar approach to words. The <a
href="https://arxiv.org/abs/1301.3781">word2vec</a> algorithm performed
a dimensionality reduction on words, now you can take any word and map
it to a latent space where similar words exhibit similar
characteristics. A ‘personality space’ for words.</p>
<h3 id="principal-component-analysis">Principal Component Analysis</h3>
<p>Principal component analysis (PCA) is arguably the queen of
dimensionality reduction techniques. PCA was developed as an approach to
dimensionality reduction in 1930s by Hotelling as a method for the
social sciences. In Hotelling’s formulation of PCA it was assumed that
any data point, <span class="math inline">\(\mathbf{x}\)</span> could be
represented as a weighted sum of the latent factors of interest, so that
Hotelling described prediction functions (like in regression and
classification above), only the regression is now <em>multiple
output</em>. And instead of predicting a label, <span
class="math inline">\(y_i\)</span>, we now try and force the regression
to predict the observed feature vector, <span
class="math inline">\(\mathbf{ y}_i\)</span>. So, for example, on an IQ
test we would try and predict subject <span
class="math inline">\(i\)</span>’s answer to the <span
class="math inline">\(j\)</span>th question with the following function
<span class="math display">\[
y_{ij} = f_j(z_i; \mathbf{ w}).
\]</span> Here <span class="math inline">\(z_i\)</span> would be the IQ
of subject <span class="math inline">\(i\)</span> and <span
class="math inline">\(f_j(\cdot)\)</span> would be a function
representing the relationship between the subject’s IQ and their score
on the answer to question <span class="math inline">\(j\)</span>. This
function is the same for all subjects, but the subject’s IQ is assumed
to differ leading to different scores for each subject.</p>
<div class="figure">
<div id="dem-manifold-print-all-1-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/dsa/./slides/diagrams//ml/demManifoldPrint_all_1_2.svg" width="60%" style=" ">
</object>
</div>
<div id="dem-manifold-print-all-1-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-manifold-print-all-1-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-manifold-print-all-1-2-caption" class="caption-frame">
<p>Figure: Visualization of the first two principal components of an
artificial data set. The data was generated by taking an image of a
handwritten digit, 6, and rotating it 360 times, one degree each time.
The first two principal components have been extracted in the diagram.
The underlying circular shape is derived from the rotation of the data.
Each image in the data set is projected on to the location its projected
to in the latent space.</p>
</div>
</div>
<h3 id="hotellings-pca">Hotelling’s PCA</h3>
<p>In Hotelling’s formulation he assumed that the function was a linear
function. This idea is taken from a wider field known as <em>factor
analysis</em>, so Hotelling described the challenge as <span
class="math display">\[
f_j(z_i; \mathbf{ w}) = w_j z_i
\]</span> so the answer to the <span class="math inline">\(j\)</span>th
question is predicted to be a scaling of the subject’s IQ. The scale
factor is given by <span class="math inline">\(w_j\)</span>. If there
are more latent dimensions then a matrix of parameters, <span
class="math inline">\(\mathbf{W}\)</span> is used, for example if there
were two latent dimensions, we’d have <span class="math display">\[
f_j(\mathbf{z}_i; \mathbf{W}) = w_{1j} z_{1i} + w_{2j} z_{2i}
\]</span> where, if this were a personality test, then <span
class="math inline">\(z_{1i}\)</span> might represent the spectrum over
a subject’s extrovert/introvert and <span
class="math inline">\(z_{2i}\)</span> might represent where the subject
was on the rational/perceptual scale. The function would make a
prediction about the subjects answer to a particular question on the
test (e.g. preference for office job vs preference for outdoor job). In
factor analysis the parameters <span
class="math inline">\(\mathbf{W}\)</span> are known as the factor
<em>loadings</em> and in PCA they are known as the principal
components.</p>
<h3 id="parameters">Parameters</h3>
<p>Fitting the model involves finding estimates for the loadings, <span
class="math inline">\(\mathbf{W}\)</span>, and latent variables, <span
class="math inline">\(\mathbf{Z}\)</span>. There are different
approaches including least squares. The least squares approach is used,
for example, in recommender systems. In recommender systems this method
is called <em>matrix factorization</em>. The customer characteristics,
<span class="math inline">\(\mathbf{ y}_i\)</span> is the customer
rating for each different product (or item) and the latent variables can
be seen as a space of customer preferences. In the recommender system
case, the loadings matrix also has an interpretation as product
similarities.<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a> Recommender systems have a
particular characteristic in that most of the entries of the vector
<span class="math inline">\(\mathbf{ y}_i\)</span> are missing most of
the time.</p>
<p>In PCA and factor analysis the unknown latent factors are dealt with
through a probability distribution. They are each assumed to be drawn
from a zero mean, unit variance normal distribution. This leaves the
factor loadings to be estimated. For PCA the maximum likelihood solution
for the factor loadings can be shown to be given by the <em>eigenvalue
decomposition</em> of the data covariance matrix. This is
algorithmically simple and convenient, although slow to compute for very
large data sets with many features and many subjects. The eigenvalue
problem can also be derived from many other starting points: e.g. the
directions of maximum variance in the data or finding a latent space
that best preserves inter-point distances between the data, or the
optimal linear compression of the data given a linear reconstruction.
These many and varied justifications for the eigenvalue decomposition
may account for the popularity of PCA. Indeed, there is even an
interpretation for Google’s original PageRank algorithm (which computed
the <em>smallest</em> eigenvector of the internet’s linkage matrix) as
seeking the dominant principal component of the web.<a href="#fn7"
class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Characterizing users according to past buying behavior and combining
this with characteristics about products, is key to making good
recommendations and returning useful search results. Further advances
can be made if we understand the context of a particular session. For
example, if a user is buying Christmas presents and searches for a
dress, then it could be the case that the user is willing to spend a
little more on the dress than in normal circumstances. Characterizing
these effects requires more data and more complex algorithms. However,
in domains such a search we are normally constrained by the speed with
which we need to return results. Accounting for each of these factors
while returning results with acceptable latency is a particular
challenge.</p>
<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/reinforcement-learning.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/reinforcement-learning.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The final domain of learning we will review is known as reinforcement
learning. The domain of reinforcement learning is one that many
researchers seem to believe is offering a route to <em>general
intelligence</em>. The idea of general intelligence is to develop
algorithms that are adaptable to many different circumstances.
Supervised learning algorithms are designed to resolve particular
challenges. Data is annotated with those challenges in mind.
Unsupervised attempts to build representations without any context. But
normally the algorithm designer has an understanding of what the broader
objective is and designs the algorithms accordingly (for example,
characterizing users). In reinforcement learning some context is given,
in the form of a reward, but the reward is normally delayed. There may
have been many actions that affected the outcome, but which actions had
a positive effect and which a negative effect?</p>
<h2 id="reward">“Reward”</h2>
<ul>
<li><p>In reinforcement learning some context is given, in the form of a
reward. But it is often <em>delayed</em></p></li>
<li><p>Credit allocation problem: many actions that affected the
outcome, but which actions had a positive effect and which a negative
effect?</p></li>
</ul>
<p>One issue for many companies is that the best way of testing the
customer experience, A/B testing, prioritizes short term reward. The
internet is currently being driven by short term rewards which make it
distracting in the short term, but perhaps less useful in the long term.
Click-bait is an example, but there are more subtle effects. The success
of Facebook is driven by its ability to draw us in when likely we should
be doing something else. This is driven by large scale A/B testing.</p>
<p>One open question is how to drive non-visual interfaces through
equivalents to A/B testing. Speech interfaces, such as those used in
intelligent agents, are less amenable to A/B testing when determining
the quality of the interface. Improving interaction with them is
therefore less exact science than the visual interface. Data efficient
reinforcement learning methods are likely to be key to improving these
agent’s ability to interact with the user and understand intent.
However, they are not yet mature enough to be deployed in this
application.</p>
<h2 id="game-play">Game Play</h2>
<p>An area where reinforcement learning methods have been deployed with
high profile success is game play. In game play the reward is delayed to
the end of the game, and it comes in the form of victory or defeat. A
significant advantage of game play as an application area is that,
through simulation of the game, it is possible to generate as much data
as is required to solve the problem. For this reason, many of the recent
advances in reinforcement learning have occurred with methods that are
not data efficient.</p>
<p>The company DeepMind is set up around reinforcement learning as an
approach to general intelligence. All their most well-known achievements
are centered around artificial intelligence in game play. In
reinforcement learning a decision made at any given time have a
downstream effect on the result. Whether the effect if beneficial or not
is unknown until a future moment.</p>
<p>We can think of reinforcement learning as providing a label, but the
label is associated with a series of data involving a number of
decisions taken. Each decision was taken given the understanding of game
play at any given moment. Understanding which of these decisions was
important in victory or defeat is a hard problem.</p>
<p>In machine learning the process of understanding which decisions were
beneficial and which were detrimental is known as the credit allocation
problem. You wish to reward decisions that led to success to encourage
them, but punish decisions that lead to failure.</p>
<p>Broadly speaking, DeepMind uses an approach to Machine Learning where
there are two mathematical functions at work. One determines the action
to be taken at any given moment, the other estimates the quality of the
board position at any given time. These are respectively known as the
<em>policy network</em> and the <em>value network</em>.<a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>
DeepMind made use of convolutional neural networks for both these
models.</p>
<h2 id="alphago">AlphaGo</h2>
<p>The ancient Chinese game of Go was considered a challenge for
artificial intelligence for two reasons. Firstly, the game tree has a
very high branching factor. The game tree is a discrete representation
of the game. Every node in the game tree is associated with a board
position. You can move through the game tree by making legal a move on
the board to change the position. In Go, there are so many legal moves
that the game tree increases exponentially. This challenge in Go was
addressed by using stochastic game tree search. Rather than exploring
the game tree exhaustively they explored it randomly.</p>
<p>Secondly, evaluating the quality of any given board position was
deemed to be very hard.<a href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a> The value function determines for
each player whether they are winning or losing. Skilled Go players can
assess a board position, but they do it by instinct, by intuition. Just
as early AI researchers struggled to give rules for detecting cancer, it
is challenging to give rules to assess a Go board. The machine learning
approach that AlphaGo took is to train a value function network to make
this assessment.</p>
<p>The approach that DeepMind took to conquering Go is a
<em>model-free</em> approach known as <em>Q-learning</em>.<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a> The model-free approach refers to
the fact that they don’t directly include a model of how the world
evolves in the reinforcement learning algorithm. They make extensive use
of the game tree, but they don’t model how it evolves. They do model the
expected reward of each position in the game tree (the value function)
but that is not the same as modeling how the game will proceed.</p>
<h2 id="reinforcement-learning-and-classical-control">Reinforcement
Learning and Classical Control</h2>
<p>An alternative approach to reinforcement learning is to use a
prediction function to suggest how the world will evolve in response to
your actions. To predict how the game tree will evolve. You can then use
this prediction to indirectly infer the expected reward associated with
any action. This is known as <em>model-based</em> reinforcement
learning.</p>
<p>This model-based approach is also closer to a control system. A
classical control system is one where you give the system a set point.
For example, a thermostat in the house. You set the temperature and the
boiler switches off when it reaches it. Optimal control is about getting
the house to the right temperature as quickly as possible. Classical
control is widely used in robotic control and flight control.</p>
<p>One interesting crossover between classical control and machine
learning arises because classical optimal control can be seen as a form
of model-based reinforcement learning. One where the reward is recovered
when the set point is reached. In control engineering the prediction
function is known as the <em>transfer function</em>. The process of
fitting the transfer function in control is known as <em>system
identification</em>.</p>
<p>There is some exciting work emerging at the interface between the
areas of control and reinforcement learning. Results at this interface
could be very important for improving the quality of robotic and drone
control.</p>
<h2 id="optimization-methods">Optimization Methods</h2>
<p>As we implied above, reinforcement learning can also used to improve
user experience. In that case the reward is gained when the user buys a
product from us. This makes it closely allied to the area of
optimization. Optimization of our user interfaces can be seen as a
reinforcement learning task, but more commonly it is thought about
separately in the domains of <em>Bayesian optimization</em> or
<em>bandit learning</em>.</p>
<p>We use optimization in machine learning to find the parameters of our
models. We can do that because we have a mathematical representation of
our objective function as a direct function of the parameters.</p>
<p>Examples in this form of optimization include, what is the best user
interface for presenting adverts? What is the best design for a front
wing for an F1 racing car? Which product should I return top of the list
in response to this user’s search?</p>
<p>Bayesian optimization arises when we can’t directly relate the
parameters in the system of interest to our objective through a
mathematical function. For example, what is the mathematical function
that relates a user’s experience to the probability that they will buy a
product?</p>
<h2 id="bayesian-optimization">Bayesian Optimization</h2>
<p>One approach to these problems is to use machine learning methods to
develop a <em>surrogate model</em> for the optimization task. The
surrogate model is a prediction function that attempts to recreate the
process we are finding hard to model. We try to simultaneously fit the
surrogate model and optimize the process.</p>
<h2 id="surrogate-models">Surrogate Models</h2>
<p>Bayesian optimization methods use a <em>surrogate model</em>
(normally a specific form of regression model). They use this to predict
how the real system will perform. The surrogate model makes a prediction
(with an estimate of the uncertainty) of what the response will be to
any given input. Parameters to test are chosen by considering this
prediction. Similar to reinforcement learning, this can be viewed as a
<em>model-based</em> approach because the surrogate model can be seen as
a model of the real world. In bandit methods strategies are determined
without turning to a model to motivate them. They are <em>model
free</em> methods.</p>
<h2 id="model-based-and-model-free-performance">Model-Based and Model
Free: Performance</h2>
<p>Because of their different philosophies, if a class of prediction
functions is chosen, then a model-based approach might have better
average case performance. At least in terms of <em>data efficiency</em>.
A model free approach may well have better worst-case performance
though, because it makes less assumptions about the nature of the data.
To put it another way, making assumptions about the data is helpful if
they are right: and if the model is sensible they’ll be right on
average. However, it is unhelpful if the model is wrong. Indeed, it
could be actively damaging. Since we can’t usually guarantee the model
is absolutely right, the worst-case performance of a model-based
approach would be poor.</p>
<p>We have introduced a range of machine learning approaches by focusing
on their use of mathematical functions to replace manually coded systems
of rules. The important characteristic of machine learning is that the
form of these functions, as dictated by their parameters, is determined
by acquiring data from the real world.</p>
<h2 id="deployment">Deployment</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deployment.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deployment.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The methods we have introduced are roughly speaking introduced in
order of difficulty of deployment. While supervised learning is more
involved in terms of collection of data, it is the most straightforward
method to deploy once that data is recovered. For this reason, a major
focus with supervised learning should always be on maintaining data
quality, increasing the efficiency and accountability<a href="#fn11"
class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>
of the data collection pipeline and the quality of features used.</p>
<p>You can also check my blog post on <a
href="http://inverseprobability.com/2017/01/12/data-readiness-levels">Data
Readiness Levels</a>. and my blog post on <a
href="http://inverseprobability.com/2018/11/05/the-3ds-of-machine-learning-systems-design">The
3Ds of Machine Learning Systems Design</a>..</p>
<h2 id="where-to-deploy">Where to Deploy?</h2>
<p>In relation to what AI can and can’t do today Andrew Ng is quoted as
saying:</p>
<blockquote>
<p>If a typical person can do a mental task with less than one second of
thought, we can probably automate it using AI either now or in the near
future.<a href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a> Andrew Ng</p>
</blockquote>
<h2 id="is-this-right">Is this Right?</h2>
<p>I would broadly agree with this quote but only in the context of
supervised learning. If a human expert takes around that amount of time,
then it’s also likely we can acquire the data necessary to build a
supervised learning algorithm that can emulate that human’s
response.</p>
<p>The picture with regard to unsupervised learning and reinforcement
learning is more clouded.</p>
<p>One observation is that for <em>supervised</em> learning we seem to
be moving beyond the era where very deep machine learning expertise is
required to deploy methods. A solid understanding of machine learning
(say to Masters level) is certainly required, but the quality of the
final result is likely more dependent on domain expertise and the
quality of the data and the information processing pipeline. This seems
part of a wider trend where some of the big successes in machine
learning are moving rapidly from the domain of science to that of
engineering.<a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a></p>
<p>You can check my blog post on <a
href="http://inverseprobability.com/2016/11/29/new-directions-in-kernels-and-gaussian-processes">New
Directions in Kernels and Gaussian Processes</a>..</p>
<p>So if we can only emulate tasks that humans take around a second to
do, how are we managing to deliver on self driving cars? The answer is
that we are constructing engineered systems from sub-components, each of
which is a machine learning subsystem. But they are tied together as a
component based system in line with our traditional engineering
approach. This has an advantage that each component in the system can be
verified before its inclusion. This is important for debugging and
safety. But in practice we can expect these systems to be very brittle.
A human adapts the way in which they drive the car across their
lifetime. A human can react to other road users. In extreme situations,
such as a car jacking, a human can set to one side normal patterns of
behavior, and purposely crash their car to draw attention to the
situation.</p>
<p>Supervised machine learning solutions are normally trained offline.
They do not adapt when deployed because this makes them less verifiable.
But this compounds the brittleness of our solutions. By deploying our
solutions we actually change the environment in which they operate.
Therefore, it’s important that they can be quickly updated to reflect
changing circumstances. This updating happens offline. For a complex
mechanical system, such as a delivery drone, extensive testing of the
system may be required when any component is updated. It is therefore
imperative that these data processing pipelines are well documented so
that they can be redeployed on demand.</p>
<p>In practice there can be challenges with the false dichotomy between
reproducibility and performance. It is likely that most of our data
scientists are caring less about their ability to redeploy their
pipelines and only about their ability to produce an algorithm that
achieves a particular performance. A key question is how reproducible is
that process? There is a <em>false</em> dichotomy because ensuring
reproducibility will typically improve performance as it will make it
easier to run a rigorous set of explorative experiments. A worry is
that, currently, we do not have a way to quantify the scale of this
potential problem within companies.</p>
<h2 id="model-choice">Model Choice</h2>
<p>Common to all machine learning methods is the initial choice of
useful classes of functions. The deep learning revolution is associated
with a particular class of mathematical functions that is proving very
successful in what were seen to be challenging domains: speech, vision,
language. This has meant that significant advances in problems that have
been seen as hard have occurred in artificial intelligence.</p>
<!-- Machine learning solutions When we deploy our solutions in the real world, we find that the situation is more complex. ThereAnother potential problem with our rush to supervised learning solutions is the false dichotomy between reproducibility and performance. Across Amazon we are using data science to design solutions which are deployed into production.  -->
<!-- It also requires more expertise on the machine learning side to develop and deploy solutions in un, and requires more expertise.  -->
<!-- such as avoiding a crash, to deliberately ram into another vehicle -->
<!-- To deliver complex solutions, like self driving cars, many sub-components from a  -->
<!-- Domain expertise becomWith regard to deIn particular, we are moving beyond the era where there is a short -->
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Breiman-forests01" class="csl-entry" role="listitem">
Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32. <a
href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>
</div>
<div id="ref-Cooper:transformation91" class="csl-entry" role="listitem">
Cooper, B., 1991. Transformation of a valley: Derbyshire derwent.
Scarthin Books.
</div>
<div id="ref-Efron:bootstrap79" class="csl-entry" role="listitem">
Efron, B., 1979. Bootstrap methods: Another look at the jackkife. Annals
of Statistics 7, 1–26.
</div>
<div id="ref-Gelman:bayesian13" class="csl-entry" role="listitem">
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin,
D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.
</div>
<div id="ref-Gething:hmis06" class="csl-entry" role="listitem">
Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,
Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data
from health management information systems in <span>A</span>frica using
space–time geostatistics. PLoS Medicine 3. <a
href="https://doi.org/10.1371/journal.pmed.0030271">https://doi.org/10.1371/journal.pmed.0030271</a>
</div>
<div id="ref-Lawrence:dsa15" class="csl-entry" role="listitem">
Lawrence, N.D., 2015. <a
href="https://www.theguardian.com/media-network/2015/aug/25/africa-benefit-data-science-information">How
<span>A</span>frica can benefit from the data revolution</a>.
</div>
<div id="ref-McCullagh:gen_linear89" class="csl-entry" role="listitem">
McCullagh, P., Nelder, J.A., 1989. Generalized linear models, 2nd ed.
Chapman; Hall.
</div>
<div id="ref-McCulloch:neuron43" class="csl-entry" role="listitem">
McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas
immanent in nervous activity. Bulletin of Mathematical Biophysics 5,
115–133. <a
href="https://doi.org/10.1007/BF02478259">https://doi.org/10.1007/BF02478259</a>
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Ng:spectral02" class="csl-entry" role="listitem">
Ng, A.Y., Jordan, M.I., Weiss, Y., n.d. On spectral clustering: Analysis
and an algorithm.
</div>
<div id="ref-Robbins:stoch51" class="csl-entry" role="listitem">
Robbins, H., Monro, S., 1951. A stochastic approximation method. Annals
of Mathematical Statistics 22, 400–407.
</div>
<div id="ref-Shi:normalized00" class="csl-entry" role="listitem">
Shi, J., Malik, J., 2000. Normalized cuts and image segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelligence 22, 888–905.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Nigeria-nmis14" class="csl-entry" role="listitem">
The Office of the Senior Special Assistant to the President on the
Millennium Development Goals (OSSAP-MDGs), Columbia University, 2014.
Nigeria <span>NMIS</span> facility database.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The logarithm of a number less than one is negative, for
a number greater than one the logarithm is positive. So if odds are
greater than evens (odds-on) the log-odds are positive, if the odds are
less than evens (odds-against) the log-odds will be negative.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In classical statistics we often interpret these
parameters, <span class="math inline">\(\beta\)</span>, whereas in
machine learning we are normally more interested in the result of the
prediction, and less in the prediction. Although this is changing with
more need for accountability. In honour of this I normally use <span
class="math inline">\(\boldsymbol{\beta}\)</span> when I care about the
value of these parameters, and <span class="math inline">\(\mathbf{
w}\)</span> when I care more about the quality of the prediction.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Assuming we are ignoring parameters in the link function
and the distribution function.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Using the test data in your training procedure is a
major error in any machine learning procedure. It is extremely dangerous
as it gives a misleading assessment of the model performance. The <a
href="http://inverseprobability.com/2015/06/04/baidu-on-imagenet">Baidu
ImageNet scandal</a> was an example of a team competing in the ImageNet
challenge which did this. The team had announced via the publication
pre-print server Arxiv that they had a world-leading performance on the
ImageNet challenge. This was reported in the mainstream media. Two weeks
later the challenge organizers revealed that the team had created
multiple accounts for checking their test performance more times than
was permitted by the challenge rules. This was then reported as “AI’s
first doping scandal”. The team lead was fired by Baidu.<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>These models are quite a lot more complex than the
simple clustering we describe here. They represent a common ancestor
through a cluster center that is then allowed to evolve over time
through a mutation rate. The time of separation between different
species is estimated via these mutation rates.<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>One way of thinking about this is to flip the model on
its side. Instead of thinking about the <span
class="math inline">\(i\)</span>th subject and the <span
class="math inline">\(j\)</span>th characteristic. Assume that each
product is the subject. So, the <span class="math inline">\(j\)</span>th
item is thought of as the subject, and each item’s characteristic is
given by the rating from a particular user. In this case symmetries in
the model show that the matrix <span
class="math inline">\(\mathbf{W}\)</span> can now be seen as a matrix of
<em>latent variables</em> and the matrix <span
class="math inline">\(\mathbf{Z}\)</span> can be seen as <em>factor
loadings</em>. So, you can think of the method as simultaneously doing a
dimensionality reduction on the products and the users. Recommender
systems also use other approaches, some of them based on similarity
measures. In a similarity measure-based recommender system the rating
prediction is given by looking for similar products in the user profile
and scoring the new product with a score that is a weighted sum of those
products.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The interpretation requires you to think of the web as a
series of web pages in a high dimensional space where distances between
web pages are computed by moving along the links (in either direction).
The PageRank is the one-dimensional space that best preserves those
distances in the sense of an L1 norm. The interpretation works because
the smallest eigenvalue of the linkage matrix is the <em>largest</em>
eigenvalue of the inverse of the linkage matrix. The inverse linkage
matrix (which would be impossible to compute) embeds similarities
between pages according to how far apart they are via a random walk
along the linkage matrix.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>The approach was described early on in the history of
machine learning by Chris Watkins, during his PhD thesis in the 1980s.
It is known as Q-learning. It’s recent success in the games domain is
driven by the use of deep learning for the policy and value functions as
well as the use of fast compute to generate and process very large
quantities of data. In its standard form it is not seen as a very
data-efficient approach.<a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The situation in chess is much easier, firstly the
number of possible moves at any time is about an order of magnitude
lower, meaning the game tree doesn’t grow as quickly. Secondly, in
chess, there are well defined value functions. For example, a value
function could be based on adding together the points that are
associated with each piece.<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>The approach was described early on in the history of
machine learning by Chris Watkins, during his PhD thesis in the 1980s.
It is known as Q-learning. It’s recent success in the games domain is
driven by the use of deep learning for the policy and value functions as
well as the use of fast compute to generate and process very large
quantities of data. In its standard form it is not seen as a very
data-efficient approach.<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>To try and better embody the state of data readiness in
organizations I’ve been proposing “Data Readiness Levels”. More needs to
be done in this area to improve the efficiency of the data science
pipeline.<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>The quote can be found in the Harvard Business Review
Article <a
href="https://hbr.org/2016/11/what-artificial-intelligence-can-and-cant-do-right-now">“What
Artificial Intelligence Can and Can’t Do Right Now”</a>.<a
href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>This trend was very clear at the moment, <a
href="%7B%7Bsite.baseurl%20%7D%7D/">I spoke about it</a> at a recent
Dagstuhl workshop on new directions for kernel methods and Gaussian
processes.<a href="#fnref13" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

