{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Processes\n",
    "==================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2020-11-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: Classical machine learning and statistical approaches to\n",
    "learning, such as neural networks and linear regression, assume a\n",
    "parametric form for functions. Gaussian process models are an\n",
    "alternative approach that assumes a probabilistic prior over functions.\n",
    "This brings benefits, in that uncertainty of function estimation is\n",
    "sustained throughout inference, and some challenges: algorithms for\n",
    "fitting Gaussian processes tend to be more complex than parametric\n",
    "models. In this sessions I will introduce Gaussian processes and explain\n",
    "why sustaining uncertainty is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\uniformDist}[3]{\\mathcal{U}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\uniformSamp}[2]{\\mathcal{U}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "-----\n",
    "\n",
    "First we download some libraries and files to support the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pods\n",
    "----\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on github:\n",
    "<a href=\"https://github.com/sods/ods\" class=\"uri\">https://github.com/sods/ods</a>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/rasmussen-williams-book.jpg\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>A key reference for Gaussian process models remains the\n",
    "excellent book “Gaussian Processes for Machine Learning” (Rasmussen and\n",
    "Williams (2006)). The book is also\n",
    "<a href=\"http://www.gaussianprocess.org/gpml/\" target=\"_blank\" >freely\n",
    "available online</a>.</i>\n",
    "\n",
    "Rasmussen and Williams (2006) is still one of the most important\n",
    "references on Gaussian process models. It is [available freely\n",
    "online](http://www.gaussianprocess.org/gpml/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A First Course in Machine Learning\n",
    "----------------------------------\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/mlai/a-first-course-in-machine-learning.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>The main course text is “A First Course in Machine Learning”\n",
    "by Rogers and Girolami (2011).</i>\n",
    "\n",
    "<!--include{_gp/includes/what-is-a-gp.md}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Prediction of Malaria Incidence in Uganda\n",
    "--------------------------------------------------\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Martin Mubangizi\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/martin-mubangizi.png\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Ricardo Andrade Pacheco\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/ricardo-andrade-pacheco.png\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip2\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "John Quinn\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/john-quinn.jpg\" clip-path=\"url(#clip2)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "As an example of using Gaussian process models within the full pipeline\n",
    "from data to decsion, we’ll consider the prediction of Malaria incidence\n",
    "in Uganda. For the purposes of this study malaria reports come in two\n",
    "forms, HMIS reports from health centres and Sentinel data, which is\n",
    "curated by the WHO. There are limited sentinel sites and many HMIS\n",
    "sites.\n",
    "\n",
    "The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in\n",
    "collaboration with John Quinn and Martin Mubangizi (Andrade-Pacheco et\n",
    "al., 2014; Mubangizi et al., 2014). John and Martin were initally from\n",
    "the AI-DEV group from the University of Makerere in Kampala and more\n",
    "latterly they were based at UN Global Pulse in Kampala.\n",
    "\n",
    "Malaria data is spatial data. Uganda is split into districts, and health\n",
    "reports can be found for each district. This suggests that models such\n",
    "as conditional random fields could be used for spatial modelling, but\n",
    "there are two complexities with this. First of all, occasionally\n",
    "districts split into two. Secondly, sentinel sites are a specific\n",
    "location within a district, such as Nagongera which is a sentinel site\n",
    "based in the Tororo district.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/uganda-districts-2006.png\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>Ugandan districs. Data SRTM/NASA from\n",
    "<a href=\"https://dds.cr.usgs.gov/srtm/version2_1\" class=\"uri\">https://dds.cr.usgs.gov/srtm/version2_1</a>.</i>\n",
    "\n",
    "<span style=\"text-align:right\">(Andrade-Pacheco et al., 2014; Mubangizi\n",
    "et al., 2014)</span>\n",
    "\n",
    "The common standard for collecting health data on the African continent\n",
    "is from the Health management information systems (HMIS). However, this\n",
    "data suffers from missing values (Gething et al., 2006) and diagnosis of\n",
    "diseases like typhoid and malaria may be confounded.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Tororo_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Tororo district, where the sentinel site, Nagongera, is\n",
    "located.</i>\n",
    "\n",
    "[World Health Organization Sentinel Surveillance\n",
    "systems](https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/)\n",
    "are set up “when high-quality data are needed about a particular disease\n",
    "that cannot be obtained through a passive system”. Several sentinel\n",
    "sites give accurate assessment of malaria disease levels in Uganda,\n",
    "including a site in Nagongera.\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/sentinel_nagongera.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Sentinel and HMIS data along with rainfall and temperature\n",
    "for the Nagongera sentinel station in the Tororo district.</i>\n",
    "\n",
    "In collaboration with the AI Research Group at Makerere we chose to\n",
    "investigate whether Gaussian process models could be used to assimilate\n",
    "information from these two different sources of disease informaton.\n",
    "Further, we were interested in whether local information on rainfall and\n",
    "temperature could be used to improve malaria estimates.\n",
    "\n",
    "The aim of the project was to use WHO Sentinel sites, alongside rainfall\n",
    "and temperature, to improve predictions from HMIS data of levels of\n",
    "malaria.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Mubende_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Mubende District.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/mubende.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Prediction of malaria incidence in Mubende.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The project arose out of the Gaussian process summer school\n",
    "held at Makerere in Kampala in 2013. The school led, in turn, to the\n",
    "Data Science Africa initiative.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Warning Systems\n",
    "---------------------\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Kabarole_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Kabarole district in Uganda.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/kabarole.gif\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Estimate of the current disease situation in the Kabarole\n",
    "district over time. Estimate is constructed with a Gaussian process with\n",
    "an additive covariance funciton.</i>\n",
    "\n",
    "Health monitoring system for the Kabarole district. Here we have fitted\n",
    "the reports with a Gaussian process with an additive covariance\n",
    "function. It has two components, one is a long time scale component (in\n",
    "red above) the other is a short time scale component (in blue).\n",
    "\n",
    "Monitoring proceeds by considering two aspects of the curve. Is the blue\n",
    "line (the short term report signal) above the red (which represents the\n",
    "long term trend? If so we have higher than expected reports. If this is\n",
    "the case *and* the gradient is still positive (i.e. reports are going\n",
    "up) we encode this with a *red* color. If it is the case and the\n",
    "gradient of the blue line is negative (i.e. reports are going down) we\n",
    "encode this with an *amber* color. Conversely, if the blue line is below\n",
    "the red *and* decreasing, we color *green*. On the other hand if it is\n",
    "below red but increasing, we color *yellow*.\n",
    "\n",
    "This gives us an early warning system for disease. Red is a bad\n",
    "situation getting worse, amber is bad, but improving. Green is good and\n",
    "getting better and yellow good but degrading.\n",
    "\n",
    "Finally, there is a gray region which represents when the scale of the\n",
    "effect is small.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/monitor.gif\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>The map of Ugandan districts with an overview of the Malaria\n",
    "situation in each district.</i>\n",
    "\n",
    "These colors can now be observed directly on a spatial map of the\n",
    "districts to give an immediate impression of the current status of the\n",
    "disease across the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overdetermined System\n",
    "---------------------\n",
    "\n",
    "The challenge with a linear model is that it has two unknowns, $m$, and\n",
    "$c$. Observing data allows us to write down a system of simultaneous\n",
    "linear equations. So, for example if we observe two data points, the\n",
    "first with the input value, $x_1 = 1$ and the output value, $y_1 =3$ and\n",
    "a second data point, $x= 3$, $y=1$, then we can write two simultaneous\n",
    "linear equations of the form.\n",
    "\n",
    "point 1: $x= 1$, $y=3$ $$3 = m + c$$ point 2: $x= 3$, $y=1$\n",
    "$$1 = 3m + c$$\n",
    "\n",
    "The solution to these two simultaneous equations can be represented\n",
    "graphically as\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/over_determined_system003.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The solution of two linear equations represented as the fit\n",
    "of a straight line through two data</i>\n",
    "\n",
    "The challenge comes when a third data point is observed and it doesn’t\n",
    "naturally fit on the straight line.\n",
    "\n",
    "point 3: $x= 2$, $y=2.5$ $$2.5 = 2m + c$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/over_determined_system004.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A third observation of data is inconsistent with the solution\n",
    "dictated by the first two observations</i>\n",
    "\n",
    "Now there are three candidate lines, each consistent with our data.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/over_determined_system007.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Three solutions to the problem, each consistent with two\n",
    "points of the three observations</i>\n",
    "\n",
    "This is known as an *overdetermined* system because there are more data\n",
    "than we need to determine our parameters. The problem arises because the\n",
    "model is a simplification of the real world, and the data we observe is\n",
    "therefore inconsistent with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.over_determined_system(diagrams='./ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('over_determined_system{samp:0>3}.svg',\n",
    "                            directory='./ml', \n",
    "                            samp=IntSlider(1,1,7,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution was proposed by Pierre-Simon Laplace. His idea was to\n",
    "accept that the model was an incomplete representation of the real\n",
    "world, and the manner in which it was incomplete is *unknown*. His idea\n",
    "was that such unknowns could be dealt with through probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pierre-Simon Laplace\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/Pierre-Simon_Laplace.png\" style=\"width:30%\">\n",
    "\n",
    "Figure: <i>Pierre-Simon Laplace 1749-1827.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famously, Laplace considered the idea of a deterministic Universe, one\n",
    "in which the model is *known*, or as the below translation refers to it,\n",
    "“an intelligence which could comprehend all the forces by which nature\n",
    "is animated”. He speculates on an “intelligence” that can submit this\n",
    "vast data to analysis and propsoses that such an entity would be able to\n",
    "predict the future.\n",
    "\n",
    "> Given for one instant an intelligence which could comprehend all the\n",
    "> forces by which nature is animated and the respective situation of the\n",
    "> beings who compose it—an intelligence sufficiently vast to submit\n",
    "> these data to analysis—it would embrace in the same formulate the\n",
    "> movements of the greatest bodies of the universe and those of the\n",
    "> lightest atom; for it, nothing would be uncertain and the future, as\n",
    "> the past, would be present in its eyes.\n",
    "\n",
    "This notion is known as *Laplace’s demon* or *Laplace’s superman*.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/laplacesDeterminismEnglish.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Laplace’s determinsim in English translation.</i>\n",
    "\n",
    "Unfortunately, most analyses of his ideas stop at that point, whereas\n",
    "his real point is that such a notion is unreachable. Not so much\n",
    "*superman* as *strawman*. Just three pages later in the “Philosophical\n",
    "Essay on Probabilities” (Laplace, 1814), Laplace goes on to observe:\n",
    "\n",
    "> The curve described by a simple molecule of air or vapor is regulated\n",
    "> in a manner just as certain as the planetary orbits; the only\n",
    "> difference between them is that which comes from our ignorance.\n",
    ">\n",
    "> Probability is relative, in part to this ignorance, in part to our\n",
    "> knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/philosophicaless00lapliala.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>To Laplace, determinism is a strawman. Ignorance of mechanism\n",
    "and data leads to uncertainty which should be dealt with through\n",
    "probability.</i>\n",
    "\n",
    "In other words, we can never make use of the idealistic deterministic\n",
    "Universe due to our ignorance about the world, Laplace’s suggestion, and\n",
    "focus in this essay is that we turn to probability to deal with this\n",
    "uncertainty. This is also our inspiration for using probability in\n",
    "machine learning.\n",
    "\n",
    "The “forces by which nature is animated” is our *model*, the “situation\n",
    "of beings that compose it” is our *data* and the “intelligence\n",
    "sufficiently vast enough to submit these data to analysis” is our\n",
    "compute. The fly in the ointment is our *ignorance* about these aspects.\n",
    "And *probability* is the tool we use to incorporate this ignorance\n",
    "leading to uncertainty or *doubt* in our predictions.\n",
    "\n",
    "Laplace’s concept was that the reason that the data doesn’t match up to\n",
    "the model is because of unconsidered factors, and that these might be\n",
    "well represented through probability densities. He tackles the challenge\n",
    "of the unknown factors by adding a variable, $\\epsilon$, that represents\n",
    "the unknown. In modern parlance we would call this a *latent* variable.\n",
    "But in the context Laplace uses it, the variable is so common that it\n",
    "has other names such as a “slack” variable or the *noise* in the system.\n",
    "\n",
    "point 1: $x= 1$, $y=3$ $$\n",
    "3 = m + c + \\epsilon_1\n",
    "$$ point 2: $x= 3$, $y=1$ $$\n",
    "1 = 3m + c + \\epsilon_2\n",
    "$$ point 3: $x= 2$, $y=2.5$ $$\n",
    "2.5 = 2m + c + \\epsilon_3\n",
    "$$\n",
    "\n",
    "Laplace’s trick has converted the *overdetermined* system into an\n",
    "*underdetermined* system. He has now added three variables,\n",
    "$\\{\\epsilon_i\\}_{i=1}^3$, which represent the unknown corruptions of the\n",
    "real world. Laplace’s idea is that we should represent that unknown\n",
    "corruption with a *probability distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Probabilistic Process\n",
    "-----------------------\n",
    "\n",
    "However, it was left to an admirer of Gauss to develop a practical\n",
    "probability density for that purpose. It was Carl Friederich Gauss who\n",
    "suggested that the *Gaussian* density (which at the time was unnamed!)\n",
    "should be used to represent this error.\n",
    "\n",
    "The result is a *noisy* function, a function which has a deterministic\n",
    "part, and a stochastic part. This type of function is sometimes known as\n",
    "a probabilistic or stochastic process, to distinguish it from a\n",
    "deterministic process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Important Gaussian Properties\n",
    "---------------------------------\n",
    "\n",
    "The Gaussian density has many important properties, but for the moment\n",
    "we’ll review two of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of Gaussians\n",
    "----------------\n",
    "\n",
    "If we assume that a variable, $y_i$, is sampled from a Gaussian density,\n",
    "\n",
    "$$y_i \\sim \\mathcal{N}\\left(\\mu_i,\\sigma_i^2\\right)$$\n",
    "\n",
    "Then we can show that the sum of a set of variables, each drawn\n",
    "independently from such a density is also distributed as Gaussian. The\n",
    "mean of the resulting density is the sum of the means, and the variance\n",
    "is the sum of the variances,\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n\\mu_i,\\sum_{i=1}^n\\sigma_i^2\\right)\n",
    "$$\n",
    "\n",
    "Since we are very familiar with the Gaussian density and its properties,\n",
    "it is not immediately apparent how unusual this is. Most random\n",
    "variables, when you add them together, change the family of density they\n",
    "are drawn from. For example, the Gaussian is exceptional in this regard.\n",
    "Indeed, other random variables, if they are independently drawn and\n",
    "summed together tend to a Gaussian density. That is the [*central limit\n",
    "theorem*](https://en.wikipedia.org/wiki/Central_limit_theorem) which is\n",
    "a major justification for the use of a Gaussian density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling a Gaussian\n",
    "------------------\n",
    "\n",
    "Less unusual is the *scaling* property of a Gaussian density. If a\n",
    "variable, $y$, is sampled from a Gaussian density,\n",
    "\n",
    "$$y\\sim \\mathcal{N}\\left(\\mu,\\sigma^2\\right)$$ and we choose to scale\n",
    "that variable by a *deterministic* value, $w$, then the *scaled\n",
    "variable* is distributed as\n",
    "\n",
    "$$wy\\sim \\mathcal{N}\\left(w\\mu,w^2 \\sigma^2\\right).$$ Unlike the summing\n",
    "properties, where adding two or more random variables independently\n",
    "sampled from a family of densitites typically brings the summed variable\n",
    "*outside* that family, scaling many densities leaves the distribution of\n",
    "that variable in the same *family* of densities. Indeed, many densities\n",
    "include a *scale* parameter (e.g. the [Gamma\n",
    "density](https://en.wikipedia.org/wiki/Gamma_distribution)) which is\n",
    "purely for this purpose. In the Gaussian the standard deviation,\n",
    "$\\sigma$, is the scale parameter. To see why this makes sense, let’s\n",
    "consider, $$z \\sim \\mathcal{N}\\left(0,1\\right),$$ then if we scale by\n",
    "$\\sigma$ so we have, $y=\\sigma z$, we can write,\n",
    "$$y=\\sigma z \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)$$\n",
    "\n",
    "Let’s first of all review the properties of the multivariate Gaussian\n",
    "distribution that make linear Gaussian models easier to deal with. We’ll\n",
    "return to the, perhaps surprising, result on the parameters within the\n",
    "nonlinearity, $\\boldsymbol{ \\theta}$, shortly.\n",
    "\n",
    "To work with linear Gaussian models, to find the marginal likelihood all\n",
    "you need to know is the following rules. If $$\n",
    "\\mathbf{ y}= \\mathbf{W}\\mathbf{ x}+ \\boldsymbol{ \\epsilon},\n",
    "$$ where $\\mathbf{ y}$, $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$ are\n",
    "vectors and we assume that $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$\n",
    "are drawn from multivariate Gaussians, $$\n",
    "\\begin{align}\n",
    "\\mathbf{ x}& \\sim \\mathcal{N}\\left(\\boldsymbol{ \\mu},\\mathbf{C}\\right)\\\\\n",
    "\\boldsymbol{ \\epsilon}& \\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{ \\Sigma}\\right)\n",
    "\\end{align}\n",
    "$$ then we know that $\\mathbf{ y}$ is also drawn from a multivariate\n",
    "Gaussian with, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{W}\\boldsymbol{ \\mu},\\mathbf{W}\\mathbf{C}\\mathbf{W}^\\top + \\boldsymbol{ \\Sigma}\\right).\n",
    "$$\n",
    "\n",
    "With appropriately defined covariance, $\\boldsymbol{ \\Sigma}$, this is\n",
    "actually the marginal likelihood for Factor Analysis, or Probabilistic\n",
    "Principal Component Analysis (Tipping and Bishop, 1999), because we\n",
    "integrated out the inputs (or *latent* variables they would be called in\n",
    "that case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace’s Idea\n",
    "--------------\n",
    "\n",
    "Laplace had the idea to augment the observations by noise, that is\n",
    "equivalent to considering a probability density whose mean is given by\n",
    "the *prediction function*\n",
    "$$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "This is known as *stochastic process*. It is a function that is\n",
    "corrupted by noise. Laplace didn’t suggest the Gaussian density for that\n",
    "purpose, that was an innovation from Carl Friederich Gauss, which is\n",
    "what gives the Gaussian density its name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Height as a Function of Weight\n",
    "------------------------------\n",
    "\n",
    "In the standard Gaussian, parametized by mean and variance.\n",
    "\n",
    "Make the mean a linear function of an *input*.\n",
    "\n",
    "This leads to a regression model. $$\n",
    "\\begin{align*}\n",
    "  y_i=&f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "         \\epsilon_i \\sim & \\mathcal{N}\\left(0,\\sigma^2\\right).\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Likelihood for Multivariate Regression\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Input Solution with Linear Algebra\n",
    "===========================================\n",
    "\n",
    "You’ve now seen how slow it can be to perform a coordinate ascent on a\n",
    "system. Another approach to solving the system (which is not always\n",
    "possible, particularly in *non-linear* systems) is to go direct to the\n",
    "minimum. To do this we need to introduce *linear algebra*. We will\n",
    "represent all our errors and functions in the form of linear algebra. As\n",
    "we mentioned above, linear algebra is just a shorthand for performing\n",
    "lots of multiplications and additions simultaneously. What does it have\n",
    "to do with our system then? Well the first thing to note is that the\n",
    "linear function we were trying to fit has the following form: $$\n",
    "f(x) = mx + c\n",
    "$$ the classical form for a straight line. From a linear algebraic\n",
    "perspective we are looking for multiplications and additions. We are\n",
    "also looking to separate our parameters from our data. The data is the\n",
    "*givens* remember, in French the word is données literally translated\n",
    "means *givens* that’s great, because we don’t need to change the data,\n",
    "what we need to change are the parameters (or variables) of the model.\n",
    "In this function the data comes in through $x$, and the parameters are\n",
    "$m$ and $c$.\n",
    "\n",
    "What we’d like to create is a vector of parameters and a vector of data.\n",
    "Then we could represent the system with vectors that represent the data,\n",
    "and vectors that represent the parameters.\n",
    "\n",
    "We look to turn the multiplications and additions into a linear\n",
    "algebraic form, we have one multiplication ($m\\times c$) and one\n",
    "addition ($mx + c$). But we can turn this into a inner product by\n",
    "writing it in the following way, $$\n",
    "f(x) = m \\times x +\n",
    "c \\times 1,\n",
    "$$ in other words we’ve extracted the unit value, from the offset, $c$.\n",
    "We can think of this unit value like an extra item of data, because it\n",
    "is always given to us, and it is always set to 1 (unlike regular data,\n",
    "which is likely to vary!). We can therefore write each input data\n",
    "location, $\\mathbf{ x}$, as a vector $$\n",
    "\\mathbf{ x}= \\begin{bmatrix} 1\\\\ x\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now we choose to also turn our parameters into a vector. The parameter\n",
    "vector will be defined to contain $$\n",
    "\\mathbf{ w}= \\begin{bmatrix} c \\\\ m\\end{bmatrix}\n",
    "$$ because if we now take the inner product between these to vectors we\n",
    "recover $$\n",
    "\\mathbf{ x}\\cdot\\mathbf{ w}= 1 \\times c + x \\times m = mx + c\n",
    "$$ In `numpy` we can define this vector as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vector w\n",
    "w = np.zeros(shape=(2, 1))\n",
    "w[0] = m\n",
    "w[1] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the equivalence between original operation and an\n",
    "operation in vector space. Whilst the notation here isn’t a lot shorter,\n",
    "the beauty is that we will be able to add as many features as we like\n",
    "and still keep the seame representation. In general, we are now moving\n",
    "to a system where each of our predictions is given by an inner product.\n",
    "When we want to represent a linear product in linear algebra, we tend to\n",
    "do it with the transpose operation, so since we have\n",
    "$\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{a}^\\top\\mathbf{b}$ we can write $$\n",
    "f(\\mathbf{ x}_i) = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
    "$$ Where we’ve assumed that each data point, $\\mathbf{ x}_i$, is now\n",
    "written by appending a 1 onto the original vector $$\n",
    "\\mathbf{ x}_i = \\begin{bmatrix} \n",
    "1 \\\\\n",
    "x_i\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Matrix\n",
    "=============\n",
    "\n",
    "We can do this for the entire data set to form a [*design\n",
    "matrix*](http://en.wikipedia.org/wiki/Design_matrix) $\\mathbf{X}$,\n",
    "\n",
    "$$\\mathbf{X}\n",
    "= \\begin{bmatrix} \n",
    "\\mathbf{ x}_1^\\top \\\\\\ \n",
    "\\mathbf{ x}_2^\\top \\\\\\ \n",
    "\\vdots \\\\\\\n",
    "\\mathbf{ x}_n^\\top\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\\\n",
    "1 & x_2 \\\\\\\n",
    "\\vdots\n",
    "& \\vdots \\\\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "which in `numpy` can be done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Objective with Linear Algebra\n",
    "-----------------------------------------\n",
    "\n",
    "When we think of the objective function, we can think of it as the\n",
    "errors where the error is defined in a similar way to what it was in\n",
    "Legendre’s day $y_i - f(\\mathbf{ x}_i)$, in statistics these errors are\n",
    "also sometimes called\n",
    "[*residuals*](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics).\n",
    "So we can think as the objective and the prediction function as two\n",
    "separate parts, first we have, $$\n",
    "E(\\mathbf{ w}) = \\sum_{i=1}^n(y_i - f(\\mathbf{ x}_i; \\mathbf{ w}))^2,\n",
    "$$ where we’ve made the function $f(\\cdot)$’s dependence on the\n",
    "parameters $\\mathbf{ w}$ explicit in this equation. Then we have the\n",
    "definition of the function itself, $$\n",
    "f(\\mathbf{ x}_i; \\mathbf{ w}) = \\mathbf{ x}_i^\\top \\mathbf{ w}.\n",
    "$$ Let’s look again at these two equations and see if we can identify\n",
    "any inner products. The first equation is a sum of squares, which is\n",
    "promising. Any sum of squares can be represented by an inner product, $$\n",
    "a = \\sum_{i=1}^{k} b^2_i = \\mathbf{b}^\\top\\mathbf{b},\n",
    "$$ so if we wish to represent $E(\\mathbf{ w})$ in this way, all we need\n",
    "to do is convert the sum operator to an inner product. We can get a\n",
    "vector from that sum operator by placing both $y_i$ and\n",
    "$f(\\mathbf{ x}_i; \\mathbf{ w})$ into vectors, which we do by defining $$\n",
    "\\mathbf{ y}= \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
    "$$ and defining $$\n",
    "\\mathbf{ f}(\\mathbf{ x}_1; \\mathbf{ w}) = \\begin{bmatrix}f(\\mathbf{ x}_1; \\mathbf{ w})\\\\ f(\\mathbf{ x}_2; \\mathbf{ w})\\\\ \\vdots \\\\ f(\\mathbf{ x}_n; \\mathbf{ w})\\end{bmatrix}.\n",
    "$$ The second of these is actually a vector-valued function. This term\n",
    "may appear intimidating, but the idea is straightforward. A vector\n",
    "valued function is simply a vector whose elements are themselves defined\n",
    "as *functions*, i.e. it is a vector of functions, rather than a vector\n",
    "of scalars. The idea is so straightforward, that we are going to ignore\n",
    "it for the moment, and barely use it in the derivation. But it will\n",
    "reappear later when we introduce *basis functions*. So we will, for the\n",
    "moment, ignore the dependence of $\\mathbf{ f}$ on $\\mathbf{ w}$ and\n",
    "$\\mathbf{X}$ and simply summarise it by a vector of numbers $$\n",
    "\\mathbf{ f}= \\begin{bmatrix}f_1\\\\f_2\\\\\n",
    "\\vdots \\\\ f_n\\end{bmatrix}.\n",
    "$$ This allows us to write our objective in the folowing, linear\n",
    "algebraic form, $$\n",
    "E(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
    "$$ from the rules of inner products. But what of our matrix $\\mathbf{X}$\n",
    "of input data? At this point, we need to dust off [*matrix-vector\n",
    "multiplication*](http://en.wikipedia.org/wiki/Matrix_multiplication).\n",
    "Matrix multiplication is simply a convenient way of performing many\n",
    "inner products together, and it’s exactly what we need to summarise the\n",
    "operation $$\n",
    "f_i = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
    "$$ This operation tells us that each element of the vector $\\mathbf{ f}$\n",
    "(our vector valued function) is given by an inner product between\n",
    "$\\mathbf{ x}_i$ and $\\mathbf{ w}$. In other words it is a series of\n",
    "inner products. Let’s look at the definition of matrix multiplication,\n",
    "it takes the form $$\n",
    "\\mathbf{c} = \\mathbf{B}\\mathbf{a}\n",
    "$$ where $\\mathbf{c}$ might be a $k$ dimensional vector (which we can\n",
    "intepret as a $k\\times 1$ dimensional matrix), and $\\mathbf{B}$ is a\n",
    "$k\\times k$ dimensional matrix and $\\mathbf{a}$ is a $k$ dimensional\n",
    "vector ($k\\times 1$ dimensional matrix).\n",
    "\n",
    "The result of this multiplication is of the form $$\n",
    "\\begin{bmatrix}c_1\\\\c_2 \\\\ \\vdots \\\\\n",
    "a_k\\end{bmatrix} = \n",
    "\\begin{bmatrix} b_{1,1} & b_{1, 2} & \\dots & b_{1, k} \\\\\n",
    "b_{2, 1} & b_{2, 2} & \\dots & b_{2, k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k, 1} & b_{k, 2} & \\dots & b_{k, k} \\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2 \\\\\n",
    "\\vdots\\\\ c_k\\end{bmatrix} = \\begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 + \\dots +\n",
    "b_{1, k}a_k\\\\\n",
    "b_{2, 1}a_1 + b_{2, 2}a_2 + \\dots + b_{2, k}a_k \\\\ \n",
    "\\vdots\\\\\n",
    "b_{k, 1}a_1 + b_{k, 2}a_2 + \\dots + b_{k, k}a_k\\end{bmatrix}\n",
    "$$ so we see that each element of the result, $\\mathbf{a}$ is simply the\n",
    "inner product between each *row* of $\\mathbf{B}$ and the vector\n",
    "$\\mathbf{c}$. Because we have defined each element of $\\mathbf{ f}$ to\n",
    "be given by the inner product between each *row* of the design matrix\n",
    "and the vector $\\mathbf{ w}$ we now can write the full operation in one\n",
    "matrix multiplication, $$\n",
    "\\mathbf{ f}= \\mathbf{X}\\mathbf{ w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = X@w # The @ sign performs matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this result with our objective function, $$\n",
    "E(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
    "$$ we find we have defined the *model* with two equations. One equation\n",
    "tells us the form of our predictive function and how it depends on its\n",
    "parameters, the other tells us the form of our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (y-f)\n",
    "E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product.\n",
    "print(\"Error function is:\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "\n",
    "The prediction for our movie recommender system had the form $$\n",
    "f_{i,j} = \\mathbf{u}_i^\\top \\mathbf{v}_j\n",
    "$$ and the objective function was then $$\n",
    "E = \\sum_{i,j} s_{i,j}(y_{i,j} - f_{i, j})^2\n",
    "$$ Try writing this down in matrix and vector form. How many of the\n",
    "terms can you do? For each variable and parameter carefully think about\n",
    "whether it should be represented as a matrix or vector. Do as many of\n",
    "the terms as you can. Use $\\LaTeX$ to give your answers and give the\n",
    "*dimensions* of any matrices you create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "### Exercise 0 Answer\n",
    "\n",
    "Write your answer to Exercise 0 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Optimisation\n",
    "======================\n",
    "\n",
    "Our *model* has now been defined with two equations, the prediction\n",
    "function and the objective function. Next we will use multivariate\n",
    "calculus to define an *algorithm* to fit the model. The separation\n",
    "between model and algorithm is important and is often overlooked. Our\n",
    "model contains a function that shows how it will be used for prediction,\n",
    "and a function that describes the objective function we need to optimise\n",
    "to obtain a good set of parameters.\n",
    "\n",
    "The model linear regression model we have described is still the same as\n",
    "the one we fitted above with a coordinate ascent algorithm. We have only\n",
    "played with the notation to obtain the same model in a matrix and vector\n",
    "notation. However, we will now fit this model with a different\n",
    "algorithm, one that is much faster. It is such a widely used algorithm\n",
    "that from the end user’s perspective it doesn’t even look like an\n",
    "algorithm, it just appears to be a single operation (or function).\n",
    "However, underneath the computer calls an algorithm to find the\n",
    "solution. Further, the algorithm we obtain is very widely used, and\n",
    "because of this it turns out to be highly optimised.\n",
    "\n",
    "Once again we are going to try and find the stationary points of our\n",
    "objective by finding the *stationary points*. However, the stationary\n",
    "points of a multivariate function, are a little bit more complext to\n",
    "find. Once again we need to find the point at which the derivative is\n",
    "zero, but now we need to use *multivariate calculus* to find it. This\n",
    "involves learning a few additional rules of differentiation (that allow\n",
    "you to do the derivatives of a function with respect to vector), but in\n",
    "the end it makes things quite a bit easier. We define vectorial\n",
    "derivatives as follows, $$\n",
    "\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}} =\n",
    "\\begin{bmatrix}\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_1}\\\\\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_2}\\end{bmatrix}.\n",
    "$$ where $\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_1}$ is the [partial\n",
    "derivative](http://en.wikipedia.org/wiki/Partial_derivative) of the\n",
    "error function with respect to $w_1$.\n",
    "\n",
    "Differentiation through multiplications and additions is relatively\n",
    "straightforward, and since linear algebra is just multiplication and\n",
    "addition, then its rules of diffentiation are quite straightforward too,\n",
    "but slightly more complex than regular derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Derivatives\n",
    "------------------------\n",
    "\n",
    "We will need two rules of multivariate or *matrix* differentiation. The\n",
    "first is diffentiation of an inner product. By remembering that the\n",
    "inner product is made up of multiplication and addition, we can hope\n",
    "that its derivative is quite straightforward, and so it proves to be. We\n",
    "can start by thinking about the definition of the inner product, $$\n",
    "\\mathbf{a}^\\top\\mathbf{z} = \\sum_{i} a_i\n",
    "z_i,\n",
    "$$ which if we were to take the derivative with respect to $z_k$ would\n",
    "simply return the gradient of the one term in the sum for which the\n",
    "derivative was non zero, that of $a_k$, so we know that $$\n",
    "\\frac{\\text{d}}{\\text{d}z_k} \\mathbf{a}^\\top \\mathbf{z} = a_k\n",
    "$$ and by our definition of multivariate derivatives we can simply stack\n",
    "all the partial derivatives of this form in a vector to obtain the\n",
    "result that $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
    "\\mathbf{a}^\\top \\mathbf{z} = \\mathbf{a}.\n",
    "$$ The second rule that’s required is differentiation of a ‘matrix\n",
    "quadratic’. A scalar quadratic in $z$ with coefficient $c$ has the form\n",
    "$cz^2$. If $\\mathbf{z}$ is a $k\\times 1$ vector and $\\mathbf{C}$ is a\n",
    "$k \\times k$ *matrix* of coefficients then the matrix quadratic form is\n",
    "written as $\\mathbf{z}^\\top \\mathbf{C}\\mathbf{z}$, which is itself a\n",
    "*scalar* quantity, but it is a function of a *vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Dimensions in Matrix Multiplications\n",
    "\n",
    "There’s a trick for telling that it’s a scalar result. When you are\n",
    "doing maths with matrices, it’s always worth pausing to perform a quick\n",
    "sanity check on the dimensions. Matrix multplication only works when the\n",
    "dimensions match. To be precise, the ‘inner’ dimension of the matrix\n",
    "must match. What is the inner dimension. If we multiply two matrices\n",
    "$\\mathbf{A}$ and $\\mathbf{B}$, the first of which has $k$ rows and\n",
    "$\\ell$ columns and the second of which has $p$ rows and $q$ columns,\n",
    "then we can check whether the multiplication works by writing the\n",
    "dimensionalities next to each other, $$\n",
    "\\mathbf{A} \\mathbf{B} \\rightarrow (k \\times\n",
    "\\underbrace{\\ell)(p}_\\text{inner dimensions} \\times q) \\rightarrow (k\\times q).\n",
    "$$ The inner dimensions are the two inside dimensions, $\\ell$ and $p$.\n",
    "The multiplication will only work if $\\ell=p$. The result of the\n",
    "multiplication will then be a $k\\times q$ matrix: this dimensionality\n",
    "comes from the ‘outer dimensions’. Note that matrix multiplication is\n",
    "not [*commutative*](http://en.wikipedia.org/wiki/Commutative_property).\n",
    "And if you change the order of the multiplication, $$\n",
    "\\mathbf{B} \\mathbf{A} \\rightarrow (\\ell \\times \\underbrace{k)(q}_\\text{inner dimensions} \\times p) \\rightarrow (\\ell \\times p).\n",
    "$$ firstly it may no longer even work, because now the condition is that\n",
    "$k=q$, and secondly the result could be of a different dimensionality.\n",
    "An exception is if the matrices are square matrices (e.g. same number of\n",
    "rows as columns) and they are both *symmetric*. A symmetric matrix is\n",
    "one for which $\\mathbf{A}=\\mathbf{A}^\\top$, or equivalently,\n",
    "$a_{i,j} = a_{j,i}$ for all $i$ and $j$.\n",
    "\n",
    "You will need to get used to working with matrices and vectors applying\n",
    "and developing new machine learning techniques. You should have come\n",
    "across them before, but you may not have used them as extensively as we\n",
    "will now do in this course. You should get used to using this trick to\n",
    "check your work and ensure you know what the dimension of an output\n",
    "matrix should be. For our matrix quadratic form, it turns out that we\n",
    "can see it as a special type of inner product. $$\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z} \\rightarrow (1\\times\n",
    "\\underbrace{k) (k}_\\text{inner dimensions}\\times k) (k\\times 1) \\rightarrow\n",
    "\\mathbf{b}^\\top\\mathbf{z}\n",
    "$$ where $\\mathbf{b} = \\mathbf{C}\\mathbf{z}$ so therefore the result is\n",
    "a scalar, $$\n",
    "\\mathbf{b}^\\top\\mathbf{z} \\rightarrow\n",
    "(1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times 1) \\rightarrow\n",
    "(1\\times 1)\n",
    "$$ where a $(1\\times 1)$ matrix is recognised as a scalar.\n",
    "\n",
    "This implies that we should be able to differentiate this form, and\n",
    "indeed the rule for its differentiation is slightly more complex than\n",
    "the inner product, but still quite simple, $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= \\mathbf{C}\\mathbf{z} + \\mathbf{C}^\\top\n",
    "\\mathbf{z}.\n",
    "$$ Note that in the special case where $\\mathbf{C}$ is symmetric then we\n",
    "have $\\mathbf{C} = \\mathbf{C}^\\top$ and the derivative simplifies to $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}=\n",
    "2\\mathbf{C}\\mathbf{z}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "Differentiate the Objective\n",
    "---------------------------\n",
    "\n",
    "First, we need to compute the full objective by substituting our\n",
    "prediction function into the objective function to obtain the objective\n",
    "in terms of $\\mathbf{ w}$. Doing this we obtain $$\n",
    "E(\\mathbf{ w})= (\\mathbf{ y}- \\mathbf{X}\\mathbf{ w})^\\top (\\mathbf{ y}- \\mathbf{X}\\mathbf{ w}).\n",
    "$$ We now need to differentiate this *quadratic form* to find the\n",
    "minimum. We differentiate with respect to the *vector* $\\mathbf{ w}$.\n",
    "But before we do that, we’ll expand the brackets in the quadratic form\n",
    "to obtain a series of scalar terms. The rules for bracket expansion\n",
    "across the vectors are similar to those for the scalar system giving, $$\n",
    "(\\mathbf{a} - \\mathbf{b})^\\top\n",
    "(\\mathbf{c} - \\mathbf{d}) = \\mathbf{a}^\\top \\mathbf{c} - \\mathbf{a}^\\top\n",
    "\\mathbf{d} - \\mathbf{b}^\\top \\mathbf{c} + \\mathbf{b}^\\top \\mathbf{d}\n",
    "$$ which substituting for $\\mathbf{a} = \\mathbf{c} = \\mathbf{ y}$ and\n",
    "$\\mathbf{b}=\\mathbf{d} = \\mathbf{X}\\mathbf{ w}$ gives $$\n",
    "E(\\mathbf{ w})=\n",
    "\\mathbf{ y}^\\top\\mathbf{ y}- 2\\mathbf{ y}^\\top\\mathbf{X}\\mathbf{ w}+\n",
    "\\mathbf{ w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w}\n",
    "$$ where we used the fact that\n",
    "$\\mathbf{ y}^\\top\\mathbf{X}\\mathbf{ w}=\\mathbf{ w}^\\top\\mathbf{X}^\\top\\mathbf{ y}$.\n",
    "Now we can use our rules of differentiation to compute the derivative of\n",
    "this form, which is, $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{ w}}E(\\mathbf{ w})=- 2\\mathbf{X}^\\top \\mathbf{ y}+\n",
    "2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w},\n",
    "$$ where we have exploited the fact that $\\mathbf{X}^\\top\\mathbf{X}$ is\n",
    "symmetric to obtain this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "\n",
    "Use the equivalence between our vector and our matrix formulations of\n",
    "linear regression, alongside our definition of vector derivates, to\n",
    "match the gradients we’ve computed directly for\n",
    "$\\frac{\\text{d}E(c, m)}{\\text{d}c}$ and\n",
    "$\\frac{\\text{d}E(c, m)}{\\text{d}m}$ to those for\n",
    "$\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0 Answer\n",
    "\n",
    "Write your answer to Exercise 0 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Equation for Global Optimum\n",
    "==================================\n",
    "\n",
    "Once again, we need to find the minimum of our objective function. Using\n",
    "our likelihood for multiple input regression we can now minimize for our\n",
    "parameter vector $\\mathbf{ w}$. Firstly, just as in the single input\n",
    "case, we seek stationary points by find parameter vectors that solve for\n",
    "when the gradients are zero, $$\n",
    "\\mathbf{0}=- 2\\mathbf{X}^\\top\n",
    "\\mathbf{ y}+ 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w},\n",
    "$$ where $\\mathbf{0}$ is a *vector* of zeros. Rearranging this equation\n",
    "we find the solution to be $$\n",
    "\\mathbf{ w}= \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top\n",
    "\\mathbf{ y}\n",
    "$$ where $\\mathbf{A}^{-1}$ denotes [*matrix\n",
    "inverse*](http://en.wikipedia.org/wiki/Invertible_matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the Multivariate System\n",
    "-------------------------------\n",
    "\n",
    "The solution for $\\mathbf{ w}$ is given in terms of a matrix inverse,\n",
    "but computation of a matrix inverse requires, in itself, an algorithm to\n",
    "resolve it. You’ll know this if you had to invert, by hand, a\n",
    "$3\\times 3$ matrix in high school. From a numerical stability\n",
    "perspective, it is also best not to compute the matrix inverse directly,\n",
    "but rather to ask the computer to *solve* the system of linear equations\n",
    "given by\n",
    "$$\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w}= \\mathbf{X}^\\top\\mathbf{ y}$$ for\n",
    "$\\mathbf{ w}$. This can be done in `numpy` using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can obtain the solution using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.solve(X.T@X, X.T@y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map it back to the liner regression and plot the fit as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = w[1]; c=w[0]\n",
    "f_test = m*x_test + c\n",
    "print(m)\n",
    "print(c)\n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Linear Regression\n",
    "------------------------------\n",
    "\n",
    "A major advantage of the new system is that we can build a linear\n",
    "regression on a multivariate system. The matrix calculus didn’t specify\n",
    "what the length of the vector $\\mathbf{ x}$ should be, or equivalently\n",
    "the size of the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Body Count Data\n",
    "---------------------\n",
    "\n",
    "Let’s consider the movie body count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.movie_body_count()\n",
    "movies = data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s remind ourselves of the features we’ve been provided with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join(movies.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a design matrix based on the numeric features: year,\n",
    "Body\\_Count, Length\\_Minutes in an effort to predict the rating. We\n",
    "build the design matrix as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation to Single Input System\n",
    "-------------------------------\n",
    "\n",
    "Bias as an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features = ['Year', 'Body_Count', 'Length_Minutes']\n",
    "X = movies[select_features]\n",
    "X['Eins'] = 1 # add a column for the offset\n",
    "y = movies[['IMDB_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s perform a linear regression. But this time, we will create a\n",
    "pandas data frame for the result so we can store it in a form that we\n",
    "can visualise easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.DataFrame(data=np.linalg.solve(X.T@X, X.T@y),  # solve linear regression here\n",
    "                 index = X.columns,  # columns of X become rows of w\n",
    "                 columns=['regression_coefficient']) # the column of X is the value of regression coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the residuals to see how good our estimates are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y - X@w).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows our model *hasn’t* yet done a great job of representation,\n",
    "because the spread of values is large. We can check what the rating is\n",
    "dominated by in terms of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have to be a little careful about interpretation because our\n",
    "input values live on different scales, however it looks like we are\n",
    "dominated by the bias, with a small negative effect for later films (but\n",
    "bear in mind the years are large, so this effect is probably larger than\n",
    "it looks) and a positive effect for length. So it looks like long\n",
    "earlier films generally do better, but the residuals are so high that we\n",
    "probably haven’t modelled the system very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underdetermined System\n",
    "======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.under_determined_system(diagrams='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the situation where you have more parameters than data in\n",
    "your simultaneous equation? This is known as an *underdetermined*\n",
    "system. In fact this set up is in some sense *easier* to solve, because\n",
    "we don’t need to think about introducing a slack variable (although it\n",
    "might make a lot of sense from a *modelling* perspective to do so).\n",
    "\n",
    "The way Laplace proposed resolving an overdetermined system, was to\n",
    "introduce slack variables, $\\epsilon_i$, which needed to be estimated\n",
    "for each point. The slack variable represented the difference between\n",
    "our actual prediction and the true observation. This is known as the\n",
    "*residual*. By introducing the slack variable we now have an additional\n",
    "$n$ variables to estimate, one for each data point, $\\{\\epsilon_i\\}$.\n",
    "This actually turns the overdetermined system into an underdetermined\n",
    "system. Introduction of $n$ variables, plus the original $m$ and $c$\n",
    "gives us $n+2$ parameters to be estimated from $n$ observations, which\n",
    "actually makes the system *underdetermined*. However, we then made a\n",
    "probabilistic assumption about the slack variables, we assumed that the\n",
    "slack variables were distributed according to a probability density. And\n",
    "for the moment we have been assuming that density was the Gaussian,\n",
    "$$\\epsilon_i \\sim \\mathcal{N}\\left(0,\\sigma^2\\right),$$ with zero mean\n",
    "and variance $\\sigma^2$.\n",
    "\n",
    "The follow up question is whether we can do the same thing with the\n",
    "parameters. If we have two parameters and only one unknown can we place\n",
    "a probability distribution over the parameters, as we did with the slack\n",
    "variables? The answer is yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underdetermined System\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('under_determined_system{samp:0>3}.svg', \n",
    "                            directory='./ml', samp=IntSlider(0, 0, 10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/under_determined_system009.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>An underdetermined system can be fit by considering\n",
    "uncertainty. Multiple solutions are consistent with one specified\n",
    "point.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Dimensional Gaussian\n",
    "------------------------\n",
    "\n",
    "Consider the distribution of height (in meters) of an adult male human\n",
    "population. We will approximate the marginal density of heights as a\n",
    "Gaussian density with mean given by $1.7\\text{m}$ and a standard\n",
    "deviation of $0.15\\text{m}$, implying a variance of $\\sigma^2=0.0225$,\n",
    "$$\n",
    "  p(h) \\sim \\mathcal{N}\\left(1.7,0.0225\\right).\n",
    "  $$ Similarly, we assume that weights of the population are distributed\n",
    "a Gaussian density with a mean of $75 \\text{kg}$ and a standard\n",
    "deviation of $6 kg$ (implying a variance of 36), $$\n",
    "  p(w) \\sim \\mathcal{N}\\left(75,36\\right).\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.height_weight(diagrams='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/height_weight_gaussian.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian distributions for height and weight.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independence Assumption\n",
    "-----------------------\n",
    "\n",
    "First of all, we make an independence assumption, we assume that height\n",
    "and weight are independent. The definition of probabilistic independence\n",
    "is that the joint density, $p(w, h)$, factorizes into its marginal\n",
    "densities, $$\n",
    "  p(w, h) = p(w)p(h).\n",
    "  $$ Given this assumption we can sample from the joint distribution by\n",
    "independently sampling weights and heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.independent_height_weight(num_samps=8, \n",
    "                               diagrams='./ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('independent_height_weight{fig:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            fig=IntSlider(0, 0, 7, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/independent_height_weight007.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples from independent Gaussian variables that might\n",
    "represent heights and weights.</i>\n",
    "\n",
    "In reality height and weight are *not* independent. Taller people tend\n",
    "on average to be heavier, and heavier people are likely to be taller.\n",
    "This is reflected by the *body mass index*. A ratio suggested by one of\n",
    "the fathers of statistics, Adolphe Quetelet. Quetelet was interested in\n",
    "the notion of the *average man* and collected various statistics about\n",
    "people. He defined the BMI to be, $$\n",
    "\\text{BMI} = \\frac{w}{h^2}\n",
    "$$To deal with this dependence we now introduce the notion of\n",
    "*correlation* to the multivariate Gaussian density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Two Dimensional Variables\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.correlated_height_weight(num_samps=8, \n",
    "                              diagrams='./ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('correlated_height_weight{fig:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            fig=IntSlider(0, 0, 7, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/correlated_height_weight007.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples from *correlated* Gaussian variables that might\n",
    "represent heights and weights.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent Gaussians\n",
    "---------------------\n",
    "\n",
    "$$\n",
    "p(w, h) = p(w)p(h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w, h) = \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{(w-\\mu_1)^2}{\\sigma_1^2} + \\frac{(h-\\mu_2)^2}{\\sigma_2^2}\\right)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w, h) = \\frac{1}{\\sqrt{2\\pi\\sigma_1^22\\pi\\sigma_2^2}} \\exp\\left(-\\frac{1}{2}\\left(\\begin{bmatrix}w \\\\ h\\end{bmatrix} - \\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\\right)^\\top\\begin{bmatrix}\\sigma_1^2& 0\\\\0&\\sigma_2^2\\end{bmatrix}^{-1}\\left(\\begin{bmatrix}w \\\\ h\\end{bmatrix} - \\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\\right)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{ y}) = \\frac{1}{\\det{2\\pi \\mathbf{D}}^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{ y}- \\boldsymbol{ \\mu})^\\top\\mathbf{D}^{-1}(\\mathbf{ y}- \\boldsymbol{ \\mu})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlated Gaussian\n",
    "-------------------\n",
    "\n",
    "Form correlated from original by rotating the data space using matrix\n",
    "$\\mathbf{R}$.\n",
    "\n",
    "$$\n",
    "p(\\mathbf{ y}) = \\frac{1}{\\det{2\\pi\\mathbf{D}}^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{ y}- \\boldsymbol{ \\mu})^\\top\\mathbf{D}^{-1}(\\mathbf{ y}- \\boldsymbol{ \\mu})\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{ y}) = \\frac{1}{\\det{2\\pi\\mathbf{D}}^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{R}^\\top\\mathbf{ y}- \\mathbf{R}^\\top\\boldsymbol{ \\mu})^\\top\\mathbf{D}^{-1}(\\mathbf{R}^\\top\\mathbf{ y}- \\mathbf{R}^\\top\\boldsymbol{ \\mu})\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{ y}) = \\frac{1}{\\det{2\\pi\\mathbf{D}}^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{ y}- \\boldsymbol{ \\mu})^\\top\\mathbf{R}\\mathbf{D}^{-1}\\mathbf{R}^\\top(\\mathbf{ y}- \\boldsymbol{ \\mu})\\right)\n",
    "$$ this gives a covariance matrix: $$\n",
    "\\mathbf{C}^{-1} = \\mathbf{R}\\mathbf{D}^{-1} \\mathbf{R}^\\top\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{ y}) = \\frac{1}{\\det{2\\pi\\mathbf{C}}^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{ y}- \\boldsymbol{ \\mu})^\\top\\mathbf{C}^{-1} (\\mathbf{ y}- \\boldsymbol{ \\mu})\\right)\n",
    "$$ this gives a covariance matrix: $$\n",
    "\\mathbf{C}= \\mathbf{R}\\mathbf{D} \\mathbf{R}^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "Here’s the idea, instead of working directly on the original input\n",
    "space, $\\mathbf{ x}$, we build models in a new space,\n",
    "$\\boldsymbol{ \\phi}(\\mathbf{ x})$ where $\\boldsymbol{ \\phi}(\\cdot)$ is a\n",
    "*vector-valued* function that is defined on the space $\\mathbf{ x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Basis\n",
    "---------------\n",
    "\n",
    "Remember, that a *vector-valued function* is just a vector that contains\n",
    "functions instead of values. Here’s an example for a one dimensional\n",
    "input space, $x$, being projected to a *quadratic* basis. First we\n",
    "consider each basis function in turn, we can think of the elements of\n",
    "our vector as being indexed so that we have $$\n",
    "\\begin{align*}\n",
    "\\phi_1(x) & = 1, \\\\\n",
    "\\phi_2(x) & = x, \\\\\n",
    "\\phi_3(x) & = x^2.\n",
    "\\end{align*}\n",
    "$$ Now we can consider them together by placing them in a vector, $$\n",
    "\\boldsymbol{ \\phi}(x) = \\begin{bmatrix} 1\\\\ x \\\\ x^2\\end{bmatrix}.\n",
    "$$ For the vector-valued function, we have simply collected the\n",
    "different functions together in the same vector making them notationally\n",
    "easier to deal with in our mathematics.\n",
    "\n",
    "When we consider the vector-valued function for each data point, then we\n",
    "place all the data into a matrix. The result is a matrix valued\n",
    "function, $$\n",
    "\\boldsymbol{ \\Phi}(\\mathbf{ x}) = \n",
    "\\begin{bmatrix} 1 & x_1 &\n",
    "x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_n & x_n^2\n",
    "\\end{bmatrix}\n",
    "$$ where we are still in the one dimensional input setting so\n",
    "$\\mathbf{ x}$ here represents a vector of our inputs with $n$ elements.\n",
    "\n",
    "Let’s try constructing such a matrix for a set of inputs. First of all,\n",
    "we create a function that returns the matrix valued function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x, **kwargs):\n",
    "    \"\"\"Take in a vector of input values and return the design matrix associated \n",
    "    with the basis functions.\"\"\"\n",
    "    return np.hstack([np.ones((x.shape[0], 1)), x, x**2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Quadratic Basis\n",
    "--------------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = {\\color{red}{w_0}}   + {\\color{magenta}{w_1 x}} + {\\color{blue}{w_2 x^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [0, -0.7],\n",
    "      [0.75, -0.2]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = x$',\n",
    "       '$\\phi(x) = x^2$']\n",
    "\n",
    "plot.basis(quadratic, x_min=-1.3, x_max=1.3, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/quadratic_basis002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The set of functions which are combined to form a *quadratic*\n",
    "basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('quadratic_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in an $n\\times 1$ dimensional vector and returns an\n",
    "$n\\times 3$ dimensional *design matrix* containing the basis functions.\n",
    "We can plot those basis functions against there input as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's generate some inputs\n",
    "n = 100\n",
    "x = np.zeros((n, 1))  # create a data set of zeros\n",
    "x[:, 0] = np.linspace(-1, 1, n) # fill it with values between -1 and 1\n",
    "\n",
    "Phi = quadratic(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.set_ylim([-1.2, 1.2]) # set y limits to ensure basis functions show.\n",
    "ax.plot(x[:,0], Phi[:, 0], 'r-', label = '$\\phi=1$', linewidth=3)\n",
    "ax.plot(x[:,0], Phi[:, 1], 'g-', label = '$\\phi=x$', linewidth=3)\n",
    "ax.plot(x[:,0], Phi[:, 2], 'b-', label = '$\\phi=x^2$', linewidth=3)\n",
    "ax.legend(loc='lower right')\n",
    "_ = ax.set_title('Quadratic Basis Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function we observe is then made up of a sum of these\n",
    "functions. This is the reason for the name basis. The term *basis* means\n",
    "‘the underlying support or foundation for an idea, argument, or\n",
    "process’, and in this context they form the underlying support for our\n",
    "prediction function. Our prediction function can only be composed of a\n",
    "weighted linear sum of our basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Functions\n",
    "-------------------\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/quadratic_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Functions constructed by weighted sum of the components of a\n",
    "quadratic basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('quadratic_function{num_function:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_function=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified Linear Units\n",
    "----------------------\n",
    "\n",
    "The rectified linear unit is a basis function that emerged out of the\n",
    "deep learning community. Rectified linear units are popular in the\n",
    "current generation of multilayer perceptron models, or deep networks.\n",
    "These basis functions start flat, and then become linear functions at a\n",
    "certain threshold. $$\n",
    "\\phi_j(x) = xH(v_j x+ v_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s relu mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [-1, -0.5],\n",
    "      [-0.33, 0.2],\n",
    "      [0.33, -0.5],\n",
    "      [1, 0.2]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = xH(x+1.0)$',\n",
    "       '$\\phi(x) = xH(x+0.33)$',\n",
    "       '$\\phi(x) = xH(x-0.33)$',\n",
    "       '$\\phi(x) = xH(x-1.0)$']\n",
    "plot.basis(mlai.relu, x_min=-2.0, x_max=2.0, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml',\n",
    "           num_basis=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/relu_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The set of functions which are combined to form a rectified\n",
    "linear unit basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('fourier_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.relu, num_basis=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Relu Basis\n",
    "---------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = \\color{red}{w_0}   + \\color{magenta}{w_1 xH(x+1.0) } + \\color{blue}{w_2 xH(x+0.33) } + \\color{green}{w_3 xH(x-0.33)} +  \\color{cyan}{w_4 xH(x-1.0)}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/relu_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A rectified linear unit basis is made up of different\n",
    "rectified linear unit functions centered at different points.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('relu_function{func_num:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            func_num=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Processes\n",
    "------------------\n",
    "\n",
    "Models where we model the entire joint distribution of our training\n",
    "data, $p(\\mathbf{ y}, \\mathbf{X})$ are sometimes described as\n",
    "*generative models*. Because we can use sampling to generate data sets\n",
    "that represent all our assumptions. However, as we discussed in the\n",
    "sessions on and , this can be a bad idea, because if our assumptions are\n",
    "wrong then we can make poor predictions. We can try to make more complex\n",
    "assumptions about data to alleviate the problem, but then this typically\n",
    "leads to challenges for tractable application of the sum and rules of\n",
    "probability that are needed to compute the relevant marginal and\n",
    "conditional densities. If we know the form of the question we wish to\n",
    "answer then we typically try and represent that directly, through\n",
    "$p(\\mathbf{ y}|\\mathbf{X})$. In practice, we also have been making\n",
    "assumptions of conditional independence given the model parameters, $$\n",
    "p(\\mathbf{ y}|\\mathbf{X}, \\mathbf{ w}) =\n",
    "\\prod_{i=1}^{n} p(y_i | \\mathbf{ x}_i, \\mathbf{ w})\n",
    "$$ Gaussian processes are *not* normally considered to be *generative\n",
    "models*, but we will be much more interested in the principles of\n",
    "conditioning in Gaussian processes because we will use conditioning to\n",
    "make predictions between our test and training data. We will avoid the\n",
    "data conditional indpendence assumption in favour of a richer assumption\n",
    "about the data, in a Gaussian process we assume data is *jointly\n",
    "Gaussian* with a particular mean and covariance, $$\n",
    "\\mathbf{ y}|\\mathbf{X}\\sim \\mathcal{N}\\left(\\mathbf{m}(\\mathbf{X}),\\mathbf{K}(\\mathbf{X})\\right),\n",
    "$$ where the conditioning is on the inputs $\\mathbf{X}$ which are used\n",
    "for computing the mean and covariance. For this reason they are known as\n",
    "mean and covariance functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model Overview\n",
    "---------------------\n",
    "\n",
    "However, we are focussing on what happens in models which are non-linear\n",
    "in the inputs, whereas the above would be *linear* in the inputs. To\n",
    "consider these, we introduce a matrix, called the design matrix. We set\n",
    "each activation function computed at each data point to be $$\n",
    "\\phi_{i,j} = \\phi(\\mathbf{ w}^{(1)}_{j}, \\mathbf{ x}_{i})\n",
    "$$ and define the matrix of activations (known as the *design matrix* in\n",
    "statistics) to be, $$\n",
    "\\boldsymbol{ \\Phi}= \n",
    "\\begin{bmatrix}\n",
    "\\phi_{1, 1} & \\phi_{1, 2} & \\dots & \\phi_{1, h} \\\\\n",
    "\\phi_{1, 2} & \\phi_{1, 2} & \\dots & \\phi_{1, n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_{n, 1} & \\phi_{n, 2} & \\dots & \\phi_{n, h}\n",
    "\\end{bmatrix}.\n",
    "$$ By convention this matrix always has $n$ rows and $h$ columns, now if\n",
    "we define the vector of all noise corruptions,\n",
    "$\\boldsymbol{ \\epsilon}= \\left[\\epsilon_1, \\dots \\epsilon_n\\right]^\\top$.\n",
    "\n",
    "If we define the prior distribution over the vector $\\mathbf{ w}$ to be\n",
    "Gaussian, $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\mathbf{I}\\right),\n",
    "$$ then we can use rules of multivariate Gaussians to see that, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}\\right).\n",
    "$$\n",
    "\n",
    "In other words, our training data is distributed as a multivariate\n",
    "Gaussian, with zero mean and a covariance given by $$\n",
    "\\mathbf{K}= \\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "This is an $n\\times n$ size matrix. Its elements are in the form of a\n",
    "function. The maths shows that any element, index by $i$ and $j$, is a\n",
    "function *only* of inputs associated with data points $i$ and $j$,\n",
    "$\\mathbf{ y}_i$, $\\mathbf{ y}_j$.\n",
    "$k_{i,j} = k\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right)$\n",
    "\n",
    "If we look at the portion of this function associated only with\n",
    "$f(\\cdot)$, i.e. we remove the noise, then we can write down the\n",
    "covariance associated with our neural network, $$\n",
    "k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) = \\alpha \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_i\\right)^\\top \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_j\\right)\n",
    "$$ so the elements of the covariance or *kernel* matrix are formed by\n",
    "inner products of the rows of the *design matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process\n",
    "----------------\n",
    "\n",
    "This is the essence of a Gaussian process. Instead of making assumptions\n",
    "about our density over each data point, $y_i$ as i.i.d. we make a joint\n",
    "Gaussian assumption over our data. The covariance matrix is now a\n",
    "function of both the parameters of the activation function,\n",
    "$\\mathbf{V}$, and the input variables, $\\mathbf{X}$. This comes about\n",
    "through integrating out the parameters of the model, $\\mathbf{ w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "We can basically put anything inside the basis functions, and many\n",
    "people do. These can be deep kernels (Cho and Saul, 2009) or we can\n",
    "learn the parameters of a convolutional neural network inside there.\n",
    "\n",
    "Viewing a neural network in this way is also what allows us to beform\n",
    "sensible *batch* normalizations (Ioffe and Szegedy, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Basis Functions\n",
    "----------------------\n",
    "\n",
    "Another type of basis is sometimes known as a ‘radial basis’ because the\n",
    "effect basis functions are constructed on ‘centres’ and the effect of\n",
    "each basis function decreases as the radial distance from each centre\n",
    "increases.\n",
    "\n",
    "$$\n",
    "\\phi_j(x) = \\exp\\left(-\\frac{(x-\\mu_j)^2}{\\ell^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s radial mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "loc = [[-1.25, -0.4],\n",
    "       [0., 1.25],\n",
    "       [1.25, -0.4]]\n",
    "text = ['$\\phi_1(x) = e^{-(x + 1)^2}$',\n",
    "        '$\\phi_2(x) = e^{-2x^2}$', \n",
    "        '$\\phi_3(x) = e^{-2(x-1)^2}$']\n",
    "plot.basis(mlai.radial, x_min=-2, x_max=2, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/radial_basis002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The set of functions which are combined to form the radial\n",
    "basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('radial_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.radial, num_basis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Radial Basis\n",
    "-----------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = \\color{red}{w_1 e^{-2(x+1)^2}}  + \\color{magenta}{w_2e^{-2x^2}} + \\color{blue}{w_3 e^{-2(x-1)^2}}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/radial_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A radial basis is made up of different locally effective\n",
    "functions centered at different points.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('radial_function{func_num:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            func_num=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal Likelihood\n",
    "-------------------\n",
    "\n",
    "To understand the Gaussian process we’re going to build on our\n",
    "understanding of the marginal likelihood for Bayesian regression. In the\n",
    "session on we sampled directly from the weight vector, $\\mathbf{ w}$ and\n",
    "applied it to the basis matrix $\\boldsymbol{ \\Phi}$ to obtain a sample\n",
    "from the prior and a sample from the posterior. It is often helpful to\n",
    "think of modeling techniques as *generative* models. To give some\n",
    "thought as to what the process for obtaining data from the model is.\n",
    "From the perspective of Gaussian processes, we want to start by thinking\n",
    "of basis function models, where the parameters are sampled from a prior,\n",
    "but move to thinking about sampling from the marginal likelihood\n",
    "directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from the Prior\n",
    "-----------------------\n",
    "\n",
    "The first thing we’ll do is to set up the parameters of the model, these\n",
    "include the parameters of the prior, the parameters of the basis\n",
    "functions and the noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prior variance on w\n",
    "alpha = 4.\n",
    "# set the order of the polynomial basis set\n",
    "degree = 5\n",
    "# set the noise variance\n",
    "sigma2 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the variance, we can sample from the prior distribution to\n",
    "see what form we are imposing on the functions *a priori*.\n",
    "\n",
    "Let’s now compute a range of values to make predictions at, spanning the\n",
    "*new* space of inputs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, degree, loc, scale):\n",
    "    degrees = np.arange(degree+1)\n",
    "    return ((x-loc)/scale)**degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let’s build the basis matrices. First we load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 1950.\n",
    "scale = 100.\n",
    "num_data = x.shape[0]\n",
    "num_pred_data = 100 # how many points to use for plotting predictions\n",
    "x_pred = np.linspace(1880, 2030, num_pred_data)[:, np.newaxis] # input locations for predictions\n",
    "Phi_pred = polynomial(x_pred, degree=degree, loc=loc, scale=scale)\n",
    "Phi = polynomial(x, degree=degree, loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Space View\n",
    "-----------------\n",
    "\n",
    "To generate typical functional predictions from the model, we need a set\n",
    "of model parameters. We assume that the parameters are drawn\n",
    "independently from a Gaussian density, $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\mathbf{I}\\right),\n",
    "$$ then we can combine this with the definition of our prediction\n",
    "function $f(\\mathbf{ x})$, $$\n",
    "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x}).\n",
    "$$ We can now sample from the prior density to obtain a vector\n",
    "$\\mathbf{ w}$ using the function `np.random.normal` and combine these\n",
    "parameters with our basis to create some samples of what\n",
    "$f(\\mathbf{ x})$ looks like,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "K = degree+1\n",
    "for i in range(num_samples):\n",
    "    z_vec = np.random.normal(size=(K, 1))\n",
    "    w_sample = z_vec*np.sqrt(alpha)\n",
    "    f_sample = Phi_pred@w_sample\n",
    "    plt.plot(x_pred, f_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Space View\n",
    "-------------------\n",
    "\n",
    "The process we have used to generate the samples is a two stage process.\n",
    "To obtain each function, we first generated a sample from the prior, $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha \\mathbf{I}\\right)\n",
    "$$ then if we compose our basis matrix, $\\boldsymbol{ \\Phi}$ from the\n",
    "basis functions associated with each row then we get, $$\n",
    "\\boldsymbol{ \\Phi}= \\begin{bmatrix}\\boldsymbol{ \\phi}(\\mathbf{ x}_1) \\\\ \\vdots \\\\\n",
    "\\boldsymbol{ \\phi}(\\mathbf{ x}_n)\\end{bmatrix}\n",
    "$$ then we can write down the vector of function values, as evaluated at\n",
    "$$\n",
    "\\mathbf{ f}= \\begin{bmatrix} f_1\n",
    "\\\\ \\vdots f_n\\end{bmatrix}\n",
    "$$ in the form $$\n",
    "\\mathbf{ f}= \\boldsymbol{ \\Phi}\\mathbf{ w}.\n",
    "$$\n",
    "\n",
    "Now we can use standard properties of multivariate Gaussians to write\n",
    "down the probability density that is implied over $\\mathbf{ f}$. In\n",
    "particular we know that if $\\mathbf{ w}$ is sampled from a multivariate\n",
    "normal (or multivariate Gaussian) with covariance $\\alpha \\mathbf{I}$\n",
    "and zero mean, then assuming that $\\boldsymbol{ \\Phi}$ is a\n",
    "deterministic matrix (i.e. it is not sampled from a probability density)\n",
    "then the vector $\\mathbf{ f}$ will also be distributed according to a\n",
    "zero mean multivariate normal as follows, $$\n",
    "\\mathbf{ f}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top\\right).\n",
    "$$\n",
    "\n",
    "The question now is, what happens if we sample $\\mathbf{ f}$ directly\n",
    "from this density, rather than first sampling $\\mathbf{ w}$ and then\n",
    "multiplying by $\\boldsymbol{ \\Phi}$. Let’s try this. First of all we\n",
    "define the covariance as $$\n",
    "\\mathbf{K}= \\alpha\n",
    "\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = alpha*Phi_pred@Phi_pred.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `np.random.multivariate_normal` command for sampling\n",
    "from a multivariate normal with covariance given by $\\mathbf{K}$ and\n",
    "zero mean,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "for i in range(10):\n",
    "    f_sample = np.random.multivariate_normal(mean=np.zeros(x_pred.size), cov=K)\n",
    "    ax.plot(x_pred.flatten(), f_sample.flatten(), linewidth=2)\n",
    "    \n",
    "mlai.write_figure('gp-sample-basis-function.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gp-sample-basis-function.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples directly from the covariance function implied by the\n",
    "basis function based covariance,\n",
    "$\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top$.</i>\n",
    "\n",
    "The samples appear very similar to those which we obtained indirectly.\n",
    "That is no surprise because they are effectively drawn from the same\n",
    "mutivariate normal density. However, when sampling $\\mathbf{ f}$\n",
    "directly we created the covariance for $\\mathbf{ f}$. We can visualise\n",
    "the form of this covaraince in an image in python with a colorbar to\n",
    "show scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "im = ax.imshow(K, interpolation='none')\n",
    "fig.colorbar(im)\n",
    "\n",
    "mlai.write_figure('basis-covariance-function.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/basis-covariance-function.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Covariance of the function implied by the basis set\n",
    "$\\alpha\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top$.</i>\n",
    "\n",
    "This image is the covariance expressed between different points on the\n",
    "function. In regression we normally also add independent Gaussian noise\n",
    "to obtain our observations $\\mathbf{ y}$, $$\n",
    "\\mathbf{ y}= \\mathbf{ f}+ \\boldsymbol{\\epsilon}\n",
    "$$ where the noise is sampled from an independent Gaussian distribution\n",
    "with variance $\\sigma^2$, $$\n",
    "\\epsilon \\sim \\mathcal{N}\\left(\\mathbf{0},\\sigma^2\\mathbf{I}\\right).\n",
    "$$ we can use properties of Gaussian variables, i.e. the fact that sum\n",
    "of two Gaussian variables is also Gaussian, and that it’s covariance is\n",
    "given by the sum of the two covariances, whilst the mean is given by the\n",
    "sum of the means, to write down the marginal likelihood, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top +\\sigma^2\\mathbf{I}\\right).\n",
    "$$ Sampling directly from this density gives us the noise corrupted\n",
    "functions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = alpha*Phi_pred@Phi_pred.T + sigma2*np.eye(x_pred.size)\n",
    "for i in range(10):\n",
    "    y_sample = np.random.multivariate_normal(mean=np.zeros(x_pred.size), cov=K)\n",
    "    ax.plot(x_pred.flatten(), y_sample.flatten())\n",
    "    \n",
    "mlai.write_figure('gp-sample-basis-function-plus-noise.svg', \n",
    "                  './kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gp-sample-basis-function-plus-noise.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples directly from the covariance function implied by the\n",
    "noise corrupted basis function based covariance,\n",
    "$\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}$.</i>\n",
    "\n",
    "where the effect of our noise term is to roughen the sampled functions,\n",
    "we can also increase the variance of the noise to see a different\n",
    "effect,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = 1.\n",
    "K = alpha*Phi_pred@Phi_pred.T + sigma2*np.eye(x_pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "for i in range(10):\n",
    "    y_sample = np.random.multivariate_normal(mean=np.zeros(x_pred.size), cov=K)\n",
    "    plt.plot(x_pred.flatten(), y_sample.flatten())\n",
    "    \n",
    "mlai.write_figure('gp-sample-basis-function-plus-large-noise.svg', \n",
    "                  './kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gp-sample-basis-function-plus-large-noise.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples directly from the covariance function implied by the\n",
    "noise corrupted basis function based covariance,\n",
    "$\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\mathbf{I}$.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-degenerate Gaussian Processes\n",
    "---------------------------------\n",
    "\n",
    "The process described above is degenerate. The covariance function is of\n",
    "rank at most $h$ and since the theoretical amount of data could always\n",
    "increase $n\\rightarrow \\infty$, the covariance function is not full\n",
    "rank. This means as we increase the amount of data to infinity, there\n",
    "will come a point where we can’t normalize the process because the\n",
    "multivariate Gaussian has the form, $$\n",
    "\\mathcal{N}\\left(\\mathbf{ f}|\\mathbf{0},\\mathbf{K}\\right) = \\frac{1}{\\left(2\\pi\\right)^{\\frac{n}{2}}\\det{\\mathbf{K}}^\\frac{1}{2}} \\exp\\left(-\\frac{\\mathbf{ f}^\\top\\mathbf{K}\\mathbf{ f}}{2}\\right)\n",
    "$$ and a non-degenerate kernel matrix leads to $\\det{\\mathbf{K}} = 0$\n",
    "defeating the normalization (it’s equivalent to finding a projection in\n",
    "the high dimensional Gaussian where the variance of the the resulting\n",
    "univariate Gaussian is zero, i.e. there is a null space on the\n",
    "covariance, or alternatively you can imagine there are one or more\n",
    "directions where the Gaussian has become the delta function).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip3\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Radford Neal\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/radford-neal.jpg\" clip-path=\"url(#clip3)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "In the machine learning field, it was Radford Neal (Neal, 1994) that\n",
    "realized the potential of the next step. In his 1994 thesis, he was\n",
    "considering Bayesian neural networks, of the type we described above,\n",
    "and in considered what would happen if you took the number of hidden\n",
    "nodes, or neurons, to infinity, i.e. $h\\rightarrow \\infty$.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/neal-infinite-priors.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Page 37 of [Radford Neal’s 1994\n",
    "thesis](http://www.cs.toronto.edu/~radford/ftp/thesis.pdf)</i>\n",
    "\n",
    "In loose terms, what Radford considers is what happens to the elements\n",
    "of the covariance function, $$\n",
    "  \\begin{align*}\n",
    "  k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) & = \\alpha \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_i\\right)^\\top \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_j\\right)\\\\\n",
    "  & = \\alpha \\sum_k \\phi\\left(\\mathbf{ w}^{(1)}_k, \\mathbf{ x}_i\\right) \\phi\\left(\\mathbf{ w}^{(1)}_k, \\mathbf{ x}_j\\right)\n",
    "  \\end{align*}\n",
    "  $$ if instead of considering a finite number you sample infinitely\n",
    "many of these activation functions, sampling parameters from a prior\n",
    "density, $p(\\mathbf{ v})$, for each one, $$\n",
    "k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) = \\alpha \\int \\phi\\left(\\mathbf{ w}^{(1)}, \\mathbf{ x}_i\\right) \\phi\\left(\\mathbf{ w}^{(1)}, \\mathbf{ x}_j\\right) p(\\mathbf{ w}^{(1)}) \\text{d}\\mathbf{ w}^{(1)}\n",
    "$$ And that’s not *only* for Gaussian $p(\\mathbf{ v})$. In fact this\n",
    "result holds for a range of activations, and a range of prior densities\n",
    "because of the *central limit theorem*.\n",
    "\n",
    "To write it in the form of a probabilistic program, as long as the\n",
    "distribution for $\\phi_i$ implied by this short probabilistic program,\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\mathbf{ v}& \\sim p(\\cdot)\\\\\n",
    "  \\phi_i & = \\phi\\left(\\mathbf{ v}, \\mathbf{ x}_i\\right), \n",
    "  \\end{align*}\n",
    "  $$ has finite variance, then the result of taking the number of hidden\n",
    "units to infinity, with appropriate scaling, is also a Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "---------------\n",
    "\n",
    "To understand this argument in more detail, I highly recommend reading\n",
    "chapter 2 of Neal’s thesis (Neal, 1994), which remains easy to read and\n",
    "clear today. Indeed, for readers interested in Bayesian neural networks,\n",
    "both Raford Neal’s and David MacKay’s PhD thesis (MacKay, 1992) remain\n",
    "essential reading. Both theses embody a clarity of thought, and an\n",
    "ability to weave together threads from different fields that was the\n",
    "business of machine learning in the 1990s. Radford and David were also\n",
    "pioneers in making their software widely available and publishing\n",
    "material on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process\n",
    "----------------\n",
    "\n",
    "In our we sampled from the prior over paraemters. Through the properties\n",
    "of multivariate Gaussian densities this prior over parameters implies a\n",
    "particular density for our data observations, $\\mathbf{ y}$. In this\n",
    "session we sampled directly from this distribution for our data,\n",
    "avoiding the intermediate weight-space representation. This is the\n",
    "approach taken by *Gaussian processes*. In a Gaussian process you\n",
    "specify the *covariance function* directly, rather than *implicitly*\n",
    "through a basis matrix and a prior over parameters. Gaussian processes\n",
    "have the advantage that they can be *nonparametric*, which in simple\n",
    "terms means that they can have *infinite* basis functions. In the\n",
    "lectures we introduced the *exponentiated quadratic* covariance, also\n",
    "known as the RBF or the Gaussian or the squared exponential covariance\n",
    "function. This covariance function is specified by $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left( -\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime\\right\\Vert^2}{2\\ell^2}\\right),\n",
    "$$ where $\\left\\Vert\\mathbf{ x}- \\mathbf{ x}^\\prime\\right\\Vert^2$ is the\n",
    "squared distance between the two input vectors $$\n",
    "\\left\\Vert\\mathbf{ x}- \\mathbf{ x}^\\prime\\right\\Vert^2 = (\\mathbf{ x}- \\mathbf{ x}^\\prime)^\\top (\\mathbf{ x}- \\mathbf{ x}^\\prime) \n",
    "$$ Let’s build a covariance matrix based on this function. First we\n",
    "define the form of the covariance function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to compute *directly* the covariance for $\\mathbf{ f}$\n",
    "at the points given by `x_pred`. Let’s define a new function `K()` which\n",
    "does this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can image the resulting covariance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov, variance=1., lengthscale=10.)\n",
    "K = kernel.K(x_pred, x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the covariance between the points we can use the `imshow`\n",
    "function in matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "im = ax.imshow(K, interpolation='none')\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can sample functions from the marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize(8, 5))\n",
    "for i in range(10):\n",
    "    y_sample = np.random.multivariate_normal(mean=np.zeros(x_pred.size), cov=K)\n",
    "    ax.plot(x_pred.flatten(), y_sample.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "**Moving Parameters** Have a play with the parameters for this\n",
    "covariance function (the lengthscale and the variance) and see what\n",
    "effects the parameters have on the types of functions you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Inference by Rejection Sampling\n",
    "----------------------------------------\n",
    "\n",
    "One view of Bayesian inference is to assume we are given a mechanism for\n",
    "generating samples, where we assume that mechanism is representing on\n",
    "accurate view on the way we believe the world works.\n",
    "\n",
    "This mechanism is known as our *prior* belief.\n",
    "\n",
    "We combine our prior belief with our observations of the real world by\n",
    "discarding all those samples that are inconsistent with our prior. The\n",
    "*likelihood* defines mathematically what we mean by inconsistent with\n",
    "the prior. The higher the noise level in the likelihood, the looser the\n",
    "notion of consistent.\n",
    "\n",
    "The samples that remain are considered to be samples from the\n",
    "*posterior*.\n",
    "\n",
    "This approach to Bayesian inference is closely related to two sampling\n",
    "techniques known as *rejection sampling* and *importance sampling*. It\n",
    "is realized in practice in an approach known as *approximate Bayesian\n",
    "computation* (ABC) or likelihood-free inference.\n",
    "\n",
    "In practice, the algorithm is often too slow to be practical, because\n",
    "most samples will be inconsistent with the data and as a result the\n",
    "mechanism has to be operated many times to obtain a few posterior\n",
    "samples.\n",
    "\n",
    "However, in the Gaussian process case, when the likelihood also assumes\n",
    "Gaussian noise, we can operate this mechanism mathematically, and obtain\n",
    "the posterior density *analytically*. This is the benefit of Gaussian\n",
    "processes.\n",
    "\n",
    "First we will load in two python functions for computing the covariance\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     lengthscale=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sample from a multivariate normal density (a multivariate\n",
    "Gaussian), using the covariance function as the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rejection_samples(kernel=kernel, \n",
    "    diagrams='./gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('gp_rejection_sample{sample:0>3}.png', \n",
    "                            directory='./gp', \n",
    "                            sample=IntSlider(1,1,5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample003.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample004.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample005.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>One view of Bayesian inference is we have a machine for\n",
    "generating samples (the *prior*), and we discard all samples\n",
    "inconsistent with our data, leaving the samples of interest (the\n",
    "*posterior*). This is a rejection sampling view of Bayesian inference.\n",
    "The Gaussian process allows us to do this analytically by multiplying\n",
    "the *prior* by the *likelihood*.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process\n",
    "----------------\n",
    "\n",
    "The Gaussian process perspective takes the marginal likelihood of the\n",
    "data to be a joint Gaussian density with a covariance given by\n",
    "$\\mathbf{K}$. So the model likelihood is of the form, $$\n",
    "p(\\mathbf{ y}|\\mathbf{X}) =\n",
    "\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\mathbf{K}|^{\\frac{1}{2}}}\n",
    "\\exp\\left(-\\frac{1}{2}\\mathbf{ y}^\\top \\left(\\mathbf{K}+\\sigma^2\n",
    "\\mathbf{I}\\right)^{-1}\\mathbf{ y}\\right)\n",
    "$$ where the input data, $\\mathbf{X}$, influences the density through\n",
    "the covariance matrix, $\\mathbf{K}$ whose elements are computed through\n",
    "the covariance function, $k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$.\n",
    "\n",
    "This means that the negative log likelihood (the objective function) is\n",
    "given by, $$\n",
    "E(\\boldsymbol{\\theta}) = \\frac{1}{2} \\log |\\mathbf{K}|\n",
    "+ \\frac{1}{2} \\mathbf{ y}^\\top \\left(\\mathbf{K}+\n",
    "\\sigma^2\\mathbf{I}\\right)^{-1}\\mathbf{ y}\n",
    "$$ where the *parameters* of the model are also embedded in the\n",
    "covariance function, they include the parameters of the kernel (such as\n",
    "lengthscale and variance), and the noise variance, $\\sigma^2$. Let’s\n",
    "create a class in python for storing these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s GP mlai.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions\n",
    "------------------\n",
    "\n",
    "We now have a probability density that represents functions. How do we\n",
    "make predictions with this density? The density is known as a process\n",
    "because it is *consistent*. By consistency, here, we mean that the model\n",
    "makes predictions for $\\mathbf{ f}$ that are unaffected by future values\n",
    "of $\\mathbf{ f}^*$ that are currently unobserved (such as test points).\n",
    "If we think of $\\mathbf{ f}^*$ as test points, we can still write down a\n",
    "joint probability density over the training observations, $\\mathbf{ f}$\n",
    "and the test observations, $\\mathbf{ f}^*$. This joint probability\n",
    "density will be Gaussian, with a covariance matrix given by our\n",
    "covariance function, $k(\\mathbf{ x}_i, \\mathbf{ x}_j)$. $$\n",
    "\\begin{bmatrix}\\mathbf{ f}\\\\ \\mathbf{ f}^*\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix} \\mathbf{K}& \\mathbf{K}_\\ast \\\\\n",
    "\\mathbf{K}_\\ast^\\top & \\mathbf{K}_{\\ast,\\ast}\\end{bmatrix}\\right)\n",
    "$$ where here $\\mathbf{K}$ is the covariance computed between all the\n",
    "training points, $\\mathbf{K}_\\ast$ is the covariance matrix computed\n",
    "between the training points and the test points and\n",
    "$\\mathbf{K}_{\\ast,\\ast}$ is the covariance matrix computed betwen all\n",
    "the tests points and themselves. To be clear, let’s compute these now\n",
    "for our example, using `x` and `y` for the training data (although `y`\n",
    "doesn’t enter the covariance) and `x_pred` as the test locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set covariance function parameters\n",
    "variance = 16.0\n",
    "lengthscale = 8\n",
    "# set noise variance\n",
    "sigma2 = 0.05\n",
    "\n",
    "kernel = Kernel(eq_cov, variance=variance, lengthscale=lengthscale)\n",
    "K = kernel.K(x, x)\n",
    "K_star = kernel.K(x, x_pred)\n",
    "K_starstar = kernel.K(x_pred, x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this structure to visualise the covariance between test data\n",
    "and training data. This structure is how information is passed between\n",
    "test and training data. Unlike the maximum likelihood formalisms we’ve\n",
    "been considering so far, the structure expresses *correlation* between\n",
    "our different data points. However, just like the we now have a *joint\n",
    "density* between some variables of interest. In particular we have the\n",
    "joint density over $p(\\mathbf{ f}, \\mathbf{ f}^*)$. The joint density is\n",
    "*Gaussian* and *zero mean*. It is specified entirely by the *covariance\n",
    "matrix*, $\\mathbf{K}$. That covariance matrix is, in turn, defined by a\n",
    "covariance function. Now we will visualise the form of that covariance\n",
    "in the form of the matrix, $$\n",
    "\\begin{bmatrix} \\mathbf{K}& \\mathbf{K}_\\ast \\\\ \\mathbf{K}_\\ast^\\top\n",
    "& \\mathbf{K}_{\\ast,\\ast}\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "im = ax.imshow(np.vstack([np.hstack([K, K_star]), np.hstack([K_star.T, K_starstar])]), interpolation='none')\n",
    "# Add lines for separating training and test data\n",
    "ax.axvline(x.shape[0]-1, color='w')\n",
    "ax.axhline(x.shape[0]-1, color='w')\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four blocks to this color plot. The upper left block is the\n",
    "covariance of the training data with itself, $\\mathbf{K}$. We see some\n",
    "structure here due to the missing data from the first and second world\n",
    "wars. Alongside this covariance (to the right and below) we see the\n",
    "cross covariance between the training and the test data ($\\mathbf{K}_*$\n",
    "and $\\mathbf{K}_*^\\top$). This is giving us the covariation between our\n",
    "training and our test data. Finally the lower right block The banded\n",
    "structure we now observe is because some of the training points are near\n",
    "to some of the test points. This is how we obtain ‘communication’\n",
    "between our training data and our test data. If there is no structure in\n",
    "$\\mathbf{K}_*$ then our belief about the test data simply matches our\n",
    "prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Across Two Points with GPs\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling a Function from a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(0, 0, 8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample001.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_2$ along with the\n",
    "conditional distribution of $f_2$ given $f_1$</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density of $f_1$ and $f_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(9, 9, 12, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample012.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_2$ along with the\n",
    "conditional distribution of $f_2$ given $f_1$</i>\n",
    "\n",
    "-   The single contour of the Gaussian density represents the\n",
    "    <font color=\"red\">joint distribution, $p(f_1, f_2)$</font>\n",
    "\n",
    ". . .\n",
    "\n",
    "-   We observe that <font color=\"green\">$f_1=?$</font>\n",
    "\n",
    ". . .\n",
    "\n",
    "-   Conditional density: <font color=\"red\">$p(f_2|f_1=?)$</font>\n",
    "\n",
    "-   Prediction of $f_2$ from $f_1$ requires *conditional density*.\n",
    "\n",
    "-   Conditional density is *also* Gaussian. $$\n",
    "    p(f_2|f_1) = {\\mathcal{N}\\left(f_2|\\frac{k_{1, 2}}{k_{1, 1}}f_1,k_{2, 2} - \\frac{k_{1,2}^2}{k_{1,1}}\\right)}\n",
    "    $$ where covariance of joint density is given by $$\n",
    "    \\mathbf{K}= \\begin{bmatrix} k_{1, 1} & k_{1, 2}\\\\ k_{2, 1} & k_{2, 2}\\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density of $f_1$ and $f_8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(13, 13, 17, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample013.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sample from the joint Gaussian model, points indexed by 1 and\n",
    "8 highlighted.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of $f_{8}$ from $f_{1}$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample017.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_8$ along with the\n",
    "conditional distribution of $f_8$ given $f_1$</i>\n",
    "\n",
    "-   The single contour of the Gaussian density represents the\n",
    "    <font color=\"blue\">joint distribution, $p(f_1, f_8)$</font>\n",
    "\n",
    ". . .\n",
    "\n",
    "-   We observe a value for <font color=\"green\">$f_1=-?$</font>\n",
    "\n",
    ". . .\n",
    "\n",
    "-   Conditional density: <font color=\"red\">$p(f_5|f_1=?)$</font>.\n",
    "\n",
    "-   Prediction of $\\mathbf{ f}_*$ from $\\mathbf{ f}$ requires\n",
    "    multivariate *conditional density*.\n",
    "\n",
    "-   Multivariate conditional density is *also* Gaussian. <large> $$\n",
    "    p(\\mathbf{ f}_*|\\mathbf{ f}) = {\\mathcal{N}\\left(\\mathbf{ f}_*|\\mathbf{K}_{*,\\mathbf{ f}}\\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{ f},\\mathbf{K}_{*,*}-\\mathbf{K}_{*,\\mathbf{ f}} \\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{K}_{\\mathbf{ f},*}\\right)}\n",
    "    $$ </large>\n",
    "\n",
    "-   Here covariance of joint density is given by $$\n",
    "    \\mathbf{K}= \\begin{bmatrix} \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}} & \\mathbf{K}_{*, \\mathbf{ f}}\\\\ \\mathbf{K}_{\\mathbf{ f}, *} & \\mathbf{K}_{*, *}\\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "-   Prediction of $\\mathbf{ f}_*$ from $\\mathbf{ f}$ requires\n",
    "    multivariate *conditional density*.\n",
    "\n",
    "-   Multivariate conditional density is *also* Gaussian. <large> $$\n",
    "    p(\\mathbf{ f}_*|\\mathbf{ f}) = {\\mathcal{N}\\left(\\mathbf{ f}_*|\\boldsymbol{ \\mu},\\boldsymbol{ \\Sigma}\\right)}\n",
    "    $$ $$\n",
    "    \\boldsymbol{ \\mu}= \\mathbf{K}_{*,\\mathbf{ f}}\\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{ f}\n",
    "    $$ $$\n",
    "    \\boldsymbol{ \\Sigma}= \\mathbf{K}_{*,*}-\\mathbf{K}_{*,\\mathbf{ f}} \\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{K}_{\\mathbf{ f},*}\n",
    "    $$ </large>\n",
    "\n",
    "-   Here covariance of joint density is given by $$\n",
    "    \\mathbf{K}= \\begin{bmatrix} \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}} & \\mathbf{K}_{*, \\mathbf{ f}}\\\\ \\mathbf{K}_{\\mathbf{ f}, *} & \\mathbf{K}_{*, *}\\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Importance of the Covariance Function\n",
    "-----------------------------------------\n",
    "\n",
    "The covariance function encapsulates our assumptions about the data. The\n",
    "equations for the distribution of the prediction function, given the\n",
    "training observations, are highly sensitive to the covariation between\n",
    "the test locations and the training locations as expressed by the matrix\n",
    "$\\mathbf{K}_*$. We defined a matrix $\\mathbf{A}$ which allowed us to\n",
    "express our conditional mean in the form, $$\n",
    "\\boldsymbol{ \\mu}_f= \\mathbf{A}^\\top \\mathbf{ y},\n",
    "$$ where $\\mathbf{ y}$ were our *training observations*. In other words\n",
    "our mean predictions are always a linear weighted combination of our\n",
    "*training data*. The weights are given by computing the covariation\n",
    "between the training and the test data ($\\mathbf{K}_*$) and scaling it\n",
    "by the inverse covariance of the training data observations,\n",
    "$\\left[\\mathbf{K}+ \\sigma^2 \\mathbf{I}\\right]^{-1}$. This inverse is the\n",
    "main computational object that needs to be resolved for a Gaussian\n",
    "process. It has a computational burden which is $O(n^3)$ and a storage\n",
    "burden which is $O(n^2)$. This makes working with Gaussian processes\n",
    "computationally intensive for the situation where $n>10,000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('ewJ3AxKclOg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Introduction to Gaussian processes given by Neil Lawrence at\n",
    "the 2014 Gaussian process Winter School at the University of\n",
    "Sheffield.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the Numerics\n",
    "----------------------\n",
    "\n",
    "In practice we shouldn’t be using matrix inverse directly to solve the\n",
    "GP system. One more stable way is to compute the *Cholesky\n",
    "decomposition* of the kernel matrix. The log determinant of the\n",
    "covariance can also be derived from the Cholesky decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s update_inverse mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.update_inverse = update_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capacity Control\n",
    "----------------\n",
    "\n",
    "Gaussian processes are sometimes seen as part of a wider family of\n",
    "methods known as kernel methods. Kernel methods are also based around\n",
    "covariance functions, but in the field they are known as Mercer kernels.\n",
    "Mercer kernels have interpretations as inner products in potentially\n",
    "infinite dimensional Hilbert spaces. This interpretation arises because,\n",
    "if we take $\\alpha=1$, then the kernel can be expressed as $$\n",
    "\\mathbf{K}= \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top \n",
    "$$ which imples the elements of the kernel are given by, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\boldsymbol{ \\phi}(\\mathbf{ x})^\\top \\boldsymbol{ \\phi}(\\mathbf{ x}^\\prime).\n",
    "$$ So we see that the kernel function is developed from an inner product\n",
    "between the basis functions. Mercer’s theorem tells us that any valid\n",
    "*positive definite function* can be expressed as this inner product but\n",
    "with the caveat that the inner product could be *infinite length*. This\n",
    "idea has been used quite widely to *kernelize* algorithms that depend on\n",
    "inner products. The kernel functions are equivalent to covariance\n",
    "functions and they are parameterized accordingly. In the kernel modeling\n",
    "community it is generally accepted that kernel parameter estimation is a\n",
    "difficult problem and the normal solution is to cross validate to obtain\n",
    "parameters. This can cause difficulties when a large number of kernel\n",
    "parameters need to be estimated. In Gaussian process modelling kernel\n",
    "parameter estimation (in the simplest case proceeds) by maximum\n",
    "likelihood. This involves taking gradients of the likelihood with\n",
    "respect to the parameters of the covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients of the Likelihood\n",
    "---------------------------\n",
    "\n",
    "The easiest conceptual way to obtain the gradients is a two step\n",
    "process. The first step involves taking the gradient of the likelihood\n",
    "with respect to the covariance function, the second step involves\n",
    "considering the gradient of the covariance function with respect to its\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Process Scale\n",
    "---------------------\n",
    "\n",
    "In general we won’t be able to find parameters of the covariance\n",
    "function through fixed point equations, we will need to do gradient\n",
    "based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capacity Control and Data Fit\n",
    "-----------------------------\n",
    "\n",
    "The objective function can be decomposed into two terms, a capacity\n",
    "control term, and a data fit term. The capacity control term is the log\n",
    "determinant of the covariance. The data fit term is the matrix inner\n",
    "product between the data and the inverse covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateObject(rotationMatrix, handle):\n",
    "for i = 1:prod(size(handle))\n",
    "    type = get(handle(i), 'type');\n",
    "    if strcmp(type, 'text'):\n",
    "        xy = get(handle(i), 'position');\n",
    "        xy(1:2) = rotationMatrix*xy(1:2)';\n",
    "        set(handle(i), 'position', xy);\n",
    "    else:\n",
    "        xd = get(handle(i), 'xdata');\n",
    "        yd = get(handle(i), 'ydata');\n",
    "        new = rotationMatrix*[xd(:)'; yd(:)'];\n",
    "        set(handle(i), 'xdata', new(1, :));\n",
    "        set(handle(i), 'ydata', new(2, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Covariance Parameters\n",
    "------------------------------\n",
    "\n",
    "Can we determine covariance parameters from the data?\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}\\color{blue}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}\\color{red}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=&\\color{blue}{-\\frac{1}{2}\\log\\det{\\mathbf{K}}}\\color{red}{-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}} \\\\ &-\\frac{n}{2}\\log2\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{ \\theta}) = \\color{blue}{\\frac{1}{2}\\log\\det{\\mathbf{K}}} + \\color{red}{\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      clf\n",
    "      lambda1 = 3;\n",
    "      lambda2 = 1;\n",
    "      t = linspace(-pi, pi, 200);\n",
    "      R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "      xy = R*[lambda1*sin(t); lambda2*cos(t)];\n",
    "      line(xy(1, :), xy(2, :), 'linewidth', 3, 'color', blackColor);\n",
    "      axis off, axis equal\n",
    "      a = arrow([0 lambda1*R(1, 1)], [0 lambda1*R(2, 1)]);\n",
    "      set(a, 'linewidth', 3, 'color', blueColor);\n",
    "      a = arrow([0 lambda2*R(1, 2)], [0 lambda2*R(2, 2)]);\n",
    "      set(a, 'linewidth', 3, 'color', blueColor);\n",
    "      xlim = get(gca, 'xlim');\n",
    "      xspan = xlim(2) - xlim(1);\n",
    "      ylim = get(gca, 'ylim');\n",
    "      yspan = ylim(2) - ylim(1);\n",
    "      text(lambda1*0.5*R(1, 1)-0.05*xspan, lambda1*0.5*R(2, 1)-yspan*0.05, '$\\eigenvalue_1$')\n",
    "      text(lambda2*0.5*R(1, 2)-0.05*xspan, lambda2*0.5*R(2, 2)-yspan*0.05, '$\\eigenvalue_2$')\n",
    "      fileName = 'gpOptimiseEigen';\n",
    "      printLatexPlot(fileName, directory, 0.45*textWidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capacity Control through the Determinant\n",
    "----------------------------------------\n",
    "\n",
    "The parameters are *inside* the covariance function (matrix).\n",
    "$$k_{i, j} = k(\\mathbf{ x}_i, \\mathbf{ x}_j; \\boldsymbol{ \\theta})$$\n",
    "\n",
    "$$\\mathbf{K}= \\mathbf{R}\\boldsymbol{ \\Lambda}^2 \\mathbf{R}^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpoptimizePlot1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimize-eigen.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "$\\boldsymbol{ \\Lambda}$ represents distance on axes. $\\mathbf{R}$ gives\n",
    "rotation.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "-   $\\boldsymbol{ \\Lambda}$ is *diagonal*,\n",
    "    $\\mathbf{R}^\\top\\mathbf{R}= \\mathbf{I}$.\n",
    "-   Useful representation since\n",
    "    $\\det{\\mathbf{K}} = \\det{\\boldsymbol{ \\Lambda}^2} = \\det{\\boldsymbol{ \\Lambda}}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = './gp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_capacity(rotate_angle=np.pi/4, lambda1 = 0.5, lambda2 = 0.3, diagrams = './gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise-determinant009.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The determinant of the covariance is dependent only on the\n",
    "eigenvalues. It represents the ‘footprint’ of the Gaussian.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf\n",
    "    includeText = [];\n",
    "    counter = 0;\n",
    "    plotWidth = 0.6*textWidth;\n",
    "    lambda1 = 3;\n",
    "    lambda2 = 1;\n",
    "    t = linspace(-pi, pi, 200);\n",
    "    R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)];\n",
    "    contourHand = line(xy(1, :), xy(2, :), 'color', blackColor);\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)]*2;\n",
    "    lim = [-1 1]*max([lambda1 lambda2])*2.2;\n",
    "    set(gca, 'xlim', lim, 'ylim', lim)\n",
    "    axis equal\n",
    "\n",
    "\n",
    "    contourHand = [contourHand line(xy(1, :), xy(2, :), 'color', blackColor)];\n",
    "    set(contourHand, 'linewidth', 2, 'color', redColor)\n",
    "    arrowHand = arrow([0 lambda1], [0 0]);\n",
    "    arrowHand = [arrowHand arrow([0 0], [0 lambda2])];\n",
    "    set(arrowHand, 'linewidth', 3, 'color', blackColor);\n",
    "    xlim = get(gca, 'xlim');\n",
    "    xspan = xlim(2) - xlim(1);\n",
    "    ylim = get(gca, 'ylim');\n",
    "    yspan = ylim(2) - ylim(1);\n",
    "    eigLabel = text(lambda1*0.5, -yspan*0.05, '$\\eigenvalue_1$', 'horizontalalignment', 'center');\n",
    "    eigLabel = [eigLabel text(-0.05*xspan, lambda2*0.5, '$\\eigenvalue_2$', 'horizontalalignment', 'center')];\n",
    "    xlabel('$\\dataScalar_1$')\n",
    "    ylabel('$\\dataScalar_2$')\n",
    "    \n",
    "    box off\n",
    "    xlim = get(gca, 'xlim');\n",
    "    ylim = get(gca, 'ylim');\n",
    "    line([xlim(1) xlim(1)], ylim, 'color', blackColor)\n",
    "    line(xlim, [ylim(1) ylim(1)], 'color', blackColor)\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    y = [1.2 1.4];\n",
    "    dataHand = line(y(1), y(2), 'marker', 'x', 'markersize', markerSize, 'linewidth', markerWidth, 'color', blackColor);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    \n",
    "    rotateObject(rotationMatrix, arrowHand);\n",
    "    rotateObject(rotationMatrix, contourHand);\n",
    "    rotateObject(rotationMatrix, eigLabel);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "    \n",
    "    printLatexText(includeText, 'gpOptimiseQuadraticIncludeText.tex', directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/diagrams/gp-optimise-quadratic002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The data fit term of the Gaussian process is a quadratic loss\n",
    "centered around zero. This has eliptical contours, the principal axes of\n",
    "which are given by the covariance matrix.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Data Fit\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Fit Term\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import gp_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(125)\n",
    "diagrams = './gp'\n",
    "\n",
    "black_color=[0., 0., 0.]\n",
    "red_color=[1., 0., 0.]\n",
    "blue_color=[0., 0., 1.]\n",
    "magenta_color=[1., 0., 1.]\n",
    "fontsize=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lim = [-2.2, 2.2]\n",
    "y_ticks = [-2, -1, 0, 1, 2]\n",
    "x_lim = [-2, 2]\n",
    "x_ticks = [-2, -1, 0, 1, 2]\n",
    "err_y_lim = [-12, 20]\n",
    "\n",
    "linewidth=3\n",
    "markersize=15\n",
    "markertype='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 6)[:, np.newaxis]\n",
    "xtest = np.linspace(x_lim[0], x_lim[1], 200)[:, np.newaxis]\n",
    "\n",
    "# True data\n",
    "true_kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "true_kern.rbf.lengthscale = 1.0\n",
    "true_kern.white.variance = 0.01\n",
    "K = true_kern.K(x) \n",
    "y = np.random.multivariate_normal(np.zeros((6,)), K, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted model\n",
    "kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "kern.rbf.lengthscale = 1.0\n",
    "kern.white.variance = 0.01\n",
    "\n",
    "lengthscales = np.asarray([0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 4, 8, 16, 100])\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=plot.one_figsize)    \n",
    "fig2, ax2 = plt.subplots(figsize=plot.one_figsize)    \n",
    "line = ax2.semilogx(np.NaN, np.NaN, 'x-', \n",
    "                    color=black_color)\n",
    "ax.set_ylim(err_y_lim)\n",
    "ax.set_xlim([0.025, 32])\n",
    "ax.grid(True)\n",
    "ax.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "ax.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "\n",
    "err = np.zeros_like(lengthscales)\n",
    "err_log_det = np.zeros_like(lengthscales)\n",
    "err_fit = np.zeros_like(lengthscales)\n",
    "\n",
    "counter = 0\n",
    "for i, ls in enumerate(lengthscales):\n",
    "        kern.rbf.lengthscale=ls\n",
    "        K = kern.K(x) \n",
    "        invK, L, Li, log_det_K = GPy.util.linalg.pdinv(K)\n",
    "        err[i] = 0.5*(log_det_K + np.dot(np.dot(y.T,invK),y))\n",
    "        err_log_det[i] = 0.5*log_det_K\n",
    "        err_fit[i] = 0.5*np.dot(np.dot(y.T,invK), y)\n",
    "        Kx = kern.K(x, xtest)\n",
    "        ypred_mean = np.dot(np.dot(Kx.T, invK), y)\n",
    "        ypred_var = kern.Kdiag(xtest) - np.sum((np.dot(Kx.T,invK))*Kx.T, 1)\n",
    "        ypred_sd = np.sqrt(ypred_var)\n",
    "        ax1.clear()\n",
    "        _ = gp_tutorial.gpplot(xtest.flatten(),\n",
    "                               ypred_mean.flatten(),\n",
    "                               ypred_mean.flatten()-2*ypred_sd.flatten(),\n",
    "                               ypred_mean.flatten()+2*ypred_sd.flatten(), \n",
    "                               ax=ax1)\n",
    "        x_lim = ax1.get_xlim()\n",
    "        ax1.set_ylabel('$f(x)$', fontsize=fontsize)\n",
    "        ax1.set_xlabel('$x$', fontsize=fontsize)\n",
    "\n",
    "        p = ax1.plot(x, y, markertype, color=black_color, markersize=markersize, linewidth=linewidth)\n",
    "        ax1.set_ylim(y_lim)\n",
    "        ax1.set_xlim(x_lim)                                    \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        #ax.set(box=False)\n",
    "           \n",
    "        ax1.plot([x_lim[0], x_lim[0]], y_lim, color=black_color)\n",
    "        ax1.plot(x_lim, [y_lim[0], y_lim[0]], color=black_color)\n",
    "\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig1,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "\n",
    "        ax2.clear()\n",
    "        t = ax2.semilogx(lengthscales[0:i+1], err[0:i+1], 'x-', \n",
    "                        color=magenta_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t2 = ax2.semilogx(lengthscales[0:i+1], err_log_det[0:i+1], 'x-', \n",
    "                         color=blue_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t3 = ax2.semilogx(lengthscales[0:i+1], err_fit[0:i+1], 'x-', \n",
    "                         color=red_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        ax2.set_ylim(err_y_lim)\n",
    "        ax2.set_xlim([0.025, 32])\n",
    "        ax2.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "        ax2.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax2.set_ylabel('negative log likelihood', fontsize=fontsize)\n",
    "        ax2.set_xlabel('length scale, $\\ell$', fontsize=fontsize)\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig2,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "        #ax.set_box(False)\n",
    "        xlim = ax2.get_xlim()\n",
    "        ax2.plot([xlim[0], xlim[0]], err_y_lim, color=black_color)\n",
    "        ax2.plot(xlim, [err_y_lim[0], err_y_lim[0]], color=black_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise006.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise010.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise016.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise021.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Variation in the data fit term, the capacity term and the\n",
    "negative log likelihood for different lengthscales.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiated Quadratic Covariance\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\exp\\left(-\\frac{\\ltwoNorm{\\inputVector-\\inputVector^\\prime}^2}{2\\lengthScale^2}\\right)',\n",
    "                     lengthscale=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_func(kernel=kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponentiated quadratic covariance, also known as the Gaussian\n",
    "covariance or the RBF covariance and the squared exponential. Covariance\n",
    "between two points is related to the negative exponential of the squared\n",
    "distnace between those points. This covariance function can be derived\n",
    "in a few different ways: as the infinite limit of a radial basis\n",
    "function neural network, as diffusion in the heat equation, as a\n",
    "Gaussian filter in *Fourier space* or as the composition as a series of\n",
    "linear filters applied to a base function.\n",
    "\n",
    "The covariance takes the following form, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)\n",
    "$$ where $\\ell$ is the *length scale* or *time scale* of the process and\n",
    "$\\alpha$ represents the overall process variance.\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/eq_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/eq_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPSS: Gaussian Process Summer School\n",
    "------------------------------------\n",
    "\n",
    "If you’re interested in finding out more about Gaussian processes, you\n",
    "can attend the Gaussian process summer school, or view the lectures and\n",
    "material on line. Details of the school, future events and past events\n",
    "can be found at the website\n",
    "<a href=\"http://gpss.cc\" class=\"uri\">http://gpss.cc</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy: A Gaussian Process Framework in Python\n",
    "-------------------------------------------\n",
    "\n",
    "Gaussian processes are a flexible tool for non-parametric analysis with\n",
    "uncertainty. The GPy software was started in Sheffield to provide a easy\n",
    "to use interface to GPs. One which allowed the user to focus on the\n",
    "modelling rather than the mathematics.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gpy.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>GPy is a BSD licensed software code base for implementing\n",
    "Gaussian process models in Python. It is designed for teaching and\n",
    "modelling. We welcome contributions which can be made through the Github\n",
    "repository\n",
    "<a href=\"https://github.com/SheffieldML/GPy\" class=\"uri\">https://github.com/SheffieldML/GPy</a></i>\n",
    "\n",
    "GPy is a BSD licensed software code base for implementing Gaussian\n",
    "process models in python. This allows GPs to be combined with a wide\n",
    "variety of software libraries.\n",
    "\n",
    "The software itself is available on\n",
    "[GitHub](https://github.com/SheffieldML/GPy) and the team welcomes\n",
    "contributions.\n",
    "\n",
    "The aim for GPy is to be a probabilistic-style programming language,\n",
    "i.e. you specify the model rather than the algorithm. As well as a large\n",
    "range of covariance functions the software allows for non-Gaussian\n",
    "likelihoods, multivariate outputs, dimensionality reduction and\n",
    "approximations for larger data sets.\n",
    "\n",
    "The documentation for GPy can be found\n",
    "[here](https://gpy.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy Tutorial\n",
    "------------\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip4\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "James Hensman\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/james-hensman.png\" clip-path=\"url(#clip4)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip5\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Nicolas Durrande\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/nicolas-durrande2.jpg\" clip-path=\"url(#clip5)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "This GPy tutorial is based on material we share in the Gaussian process\n",
    "summer school for teaching these models\n",
    "<a href=\"https://gpss.cc\" class=\"uri\">https://gpss.cc</a>. It contains\n",
    "material from various members and former members of the Sheffield\n",
    "machine learning group, but particular mention should be made of\n",
    "[Nicolas\n",
    "Durrande](https://sites.google.com/site/nicolasdurrandehomepage/) and\n",
    "[James Hensman](https://jameshensman.github.io/), see\n",
    "<a href=\"http://gpss.cc/gpss17/labs/GPSS_Lab1_2017.ipynb\" class=\"uri\">http://gpss.cc/gpss17/labs/GPSS_Lab1_2017.ipynb</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a feel for the sofware we’ll start by creating an exponentiated\n",
    "quadratic covariance function, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}- \\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right),\n",
    "$$ where the length scale is $\\ell$ and the variance is $\\alpha$.\n",
    "\n",
    "To set this up in GPy we create a kernel in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1\n",
    "alpha = 1.0\n",
    "lengthscale = 2.0\n",
    "kern = GPy.kern.RBF(input_dim=input_dim, variance=alpha, lengthscale=lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That builds a kernel object for us. The kernel can be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or because it’s one dimensional, you can also plot the kernel as a\n",
    "function of its inputs (while the other is fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "kern.plot(ax=ax)\n",
    "mlai.write_figure('gpy-eq-covariance.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-covariance.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function as plotted by\n",
    "the `GPy.kern.plot` command.</i>\n",
    "\n",
    "You can set the lengthscale of the covariance to different values and\n",
    "plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(input_dim=input_dim)     # By default, the parameters are set to 1.\n",
    "lengthscales = np.asarray([0.2,0.5,1.,2.,4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "for lengthscale in lengthscales:\n",
    "    kern.lengthscale=lengthscale\n",
    "    kern.plot(ax=ax)\n",
    "\n",
    "ax.legend(lengthscales)\n",
    "mlai.write_figure('gpy-eq-covariance-lengthscales.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-covariance-lengthscales.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function plotted for\n",
    "different lengthscales by `GPy.kern.plot` command.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Functions in GPy\n",
    "---------------------------\n",
    "\n",
    "Many covariance functions are already implemented in GPy. Instead of\n",
    "rbf, try constructing and plotting the following covariance functions:\n",
    "`exponential`, `Matern32`, `Matern52`, `Brownian`, `linear`, `bias`,\n",
    "`rbfcos`, `periodic_Matern32`, etc. Some of these covariance functions,\n",
    "such as `rbfcos`, are not parametrized by a variance and a lengthscale.\n",
    "Furthermore, not all kernels are stationary (i.e., they can’t all be\n",
    "written as\n",
    "$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = f(\\mathbf{ x}-\\mathbf{ x}^\\prime)$,\n",
    "see for example the Brownian covariance function). For plotting so it\n",
    "may be interesting to change the value of the fixed input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "brownian_kern = GPy.kern.Brownian(input_dim=1)\n",
    "inputs = np.array([2., 4.])\n",
    "for x in inputs:\n",
    "    brownian_kern.plot(x,plot_limits=[0,5], ax=ax)\n",
    "ax.legend(inputs)\n",
    "ax.set_ylim(-0.1,5.1)\n",
    "\n",
    "mlai.write_figure('gpy-brownian-covariance-lengthscales.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Covariance Functions in GPy\n",
    "-------------------------------------\n",
    "\n",
    "In GPy you can easily combine covariance functions you have created\n",
    "using the sum and product operators, `+` and `*`. So, for example, if we\n",
    "wish to combine an exponentiated quadratic covariance with a Matern 5/2\n",
    "then we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, variance=1., lengthscale=2.)\n",
    "kern2 = GPy.kern.Matern52(1, variance=2., lengthscale=4.)\n",
    "kern = kern1 + kern2\n",
    "display(kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "kern.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('gpy-eq-plus-matern52-covariance.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-plus-matern52-covariance.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A combination of the exponentiated quadratic covariance plus\n",
    "the Matern $5/2$ covariance.</i>\n",
    "\n",
    "Or if we wanted to multiply them we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, variance=1., lengthscale=2.)\n",
    "kern2 = GPy.kern.Matern52(1, variance=2., lengthscale=4.)\n",
    "kern = kern1 * kern2\n",
    "display(kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "kern.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('gpy-eq-times-matern52-covariance.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-times-matern52-covariance.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A combination of the exponentiated quadratic covariance\n",
    "multiplied by the Matern $5/2$ covariance.</i>\n",
    "\n",
    "You can learn about how to implement [new kernel objects in GPy\n",
    "here](https://gpy.readthedocs.io/en/latest/tuto_creating_new_kernels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('-sY8zW3Om1Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Designing the covariance function for your Gaussian process\n",
    "is a key place in which you introduce your understanding of the data\n",
    "problem. To learn more about the design of covariance functions, see\n",
    "this talk from Nicolas Durrande at GPSS in 2016.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian Process Regression Model\n",
    "-----------------------------------\n",
    "\n",
    "We will now combine the Gaussian process prior with some data to form a\n",
    "GP regression model with GPy. We will generate data from the function $$\n",
    "f( x) = − \\cos(\\pi x) + \\sin(4\\pi x)\n",
    "$$ over the domain $[0, 1]$, adding some noise to gives $$\n",
    "y(x) = f(x) + \\epsilon,\n",
    "$$ with the noise being Gaussian distributed,\n",
    "$\\epsilon\\sim \\mathcal{N}\\left(0,0.01\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0.05,0.95,10)[:,np.newaxis]\n",
    "Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(X,Y,'kx',mew=1.5, linewidth=2)\n",
    "\n",
    "mlai.write_figure('noisy-sine.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/noisy-sine.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Data from the noisy sine wave for fitting with a GPy\n",
    "model.</i>\n",
    "\n",
    "A GP regression model based on an exponentiated quadratic covariance\n",
    "function can be defined by first defining a covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then combining it with the data to form a Gaussian process model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X,Y,kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for the covariance function object, we can find out about the\n",
    "model using the command `display(model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default the model includes some observation noise with\n",
    "variance 1. We can see the posterior mean prediction and visualize the\n",
    "marginal posterior variances using `model.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "model.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('noisy-sine-gp-fit.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/noisy-sine-gp-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian process fit to the noisy sine data. Here the\n",
    "parameters of the process and the covariance function haven’t yet been\n",
    "optimized.</i>\n",
    "\n",
    "You can also look directly at the predictions for the model using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstar = np.linspace(0, 10, 100)[:, np.newaxis]\n",
    "Ystar, Vstar = model.predict(Xstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives you the mean (`Ystar`), the variance (`Vstar`) at the\n",
    "locations given by `Xstar`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Function Parameter Estimation\n",
    "----------------------------------------\n",
    "\n",
    "As we have seen during the lectures, the parameters values can be\n",
    "estimated by maximizing the likelihood of the observations. Since we\n",
    "don’t want one of the variance to become negative during the\n",
    "optimization, we can constrain all parameters to be positive before\n",
    "running the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.constrain_positive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings are because the parameters are already constrained by\n",
    "default, the software is warning us that they are being reconstrained.\n",
    "\n",
    "Now we can optimize the model using the `model.optimize()` method. Here\n",
    "we switch messages on, which allows us to see the progession of the\n",
    "optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the optimization is using a limited memory BFGS optimizer\n",
    "(Byrd et al., 1995).\n",
    "\n",
    "Once again we can display the model, now to see how the parameters have\n",
    "changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengthscale is much smaller, as well as the noise level. The\n",
    "variance of the exponentiated quadratic has also reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "model.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('noisy-sine-gp-optimized-fit.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/noisy-sine-gp-optimized-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian process fit to the noisy sine data with parameters\n",
    "optimized.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Software\n",
    "--------------\n",
    "\n",
    "GPy has inspired other software solutions, first of all\n",
    "[GPflow](https://github.com/GPflow/GPflow), which uses Tensor Flow’s\n",
    "automatic differentiation engine to allow rapid prototyping of new\n",
    "covariance functions and algorithms. More recently,\n",
    "[GPyTorch](https://github.com/cornellius-gp/gpytorch) uses PyTorch for\n",
    "the same purpose.\n",
    "\n",
    "The Probabilistic programming language [pyro](https://pyro.ai/) also has\n",
    "GP support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "---------------\n",
    "\n",
    "-   Chapter 2 of Neal (1994)\n",
    "\n",
    "-   Rest of Neal (1994)\n",
    "\n",
    "-   All of MacKay (1992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.\n",
    "Consistent mapping of government malaria records across a changing\n",
    "territory delimitation. Malaria Journal 13.\n",
    "<https://doi.org/10.1186/1475-2875-13-S1-P5>\n",
    "\n",
    "Byrd, R.H., Lu, P., Nocedal, J., 1995. A limited memory algorithm for\n",
    "bound constrained optimization. SIAM Journal on Scientific and\n",
    "Statistical Computing 16, 1190–1208.\n",
    "\n",
    "Cho, Y., Saul, L.K., 2009. Kernel methods for deep learning, in: Bengio,\n",
    "Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A.\n",
    "(Eds.), Advances in Neural Information Processing Systems 22. Curran\n",
    "Associates, Inc., pp. 342–350.\n",
    "\n",
    "Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,\n",
    "Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data\n",
    "from health management information systems in Africa using space–time\n",
    "geostatistics. PLoS Medicine 3.\n",
    "<https://doi.org/10.1371/journal.pmed.0030271>\n",
    "\n",
    "Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep\n",
    "network training by reducing internal covariate shift, in: Bach, F.,\n",
    "Blei, D. (Eds.), Proceedings of the 32nd International Conference on\n",
    "Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille,\n",
    "France, pp. 448–456.\n",
    "\n",
    "Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.\n",
    "Courcier, Paris.\n",
    "\n",
    "MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).\n",
    "California Institute of Technology.\n",
    "\n",
    "Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,\n",
    "N.D., 2014. Malaria surveillance with multiple data sources using\n",
    "Gaussian process models, in: 1st International Conference on the Use of\n",
    "Mobile ICT in Africa.\n",
    "\n",
    "Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).\n",
    "Dept. of Computer Science, University of Toronto.\n",
    "\n",
    "Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine\n",
    "learning. mit, Cambridge, MA.\n",
    "\n",
    "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
    "Press.\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component\n",
    "analysis. Journal of the Royal Statistical Society, B 6, 611–622.\n",
    "<https://doi.org/doi:10.1111/1467-9868.00196>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
